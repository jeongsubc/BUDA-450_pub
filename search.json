[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Data Mining and Visualization",
    "section": "",
    "text": "Welcome!\nThis website is for the lecture notes of { BUDA-450 Business Data Mining and Visualization }, and it will be updated with the development of the course.\nThe materials aim to introduce to Data Mining and Visualization with Python Programming basics. The content of the course will be given with the following software and brief introductions.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Business Data Mining and Visualization",
    "section": "About this Book",
    "text": "About this Book\n\nThis book provides an introduction to the skill of computer programming (“programming 101”) in the Python language, with an emphasis on skills useful for information and data sciences. It will introduce programming concepts such as syntax (how to “speak” Python), data organization (how computers represent information), control flow (how to have the computer do lots of things in a row), and debugging (how to fix broken programs). The goal is to develop skills in formal language semantics, algorithmic thinking, abstraction, and debugging—being able to “think like the computer” to understand how it works. In short: the goal is to get you familiar with programming concepts and abstract thinking skills used in automatically managing and analyzing information. It assumes no previous programming background.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome Page</span>"
    ]
  },
  {
    "objectID": "index.html#conventions",
    "href": "index.html#conventions",
    "title": "Business Data Mining and Visualization",
    "section": "Conventions",
    "text": "Conventions\nThe lecture notes are compiled using Quarto.\n\nItalic, Bold, and ItalicBold texts are used to highlight concepts or items being discussed.\ntypewriter font is used for function, inline code, and file names, pertaining to software use.\nThe following code blocks are used to show codes (and results) in Python:\n\n\na = 2\nb = 5\nprint(\"a plus b is equal to\", a+b)\n\na plus b is equal to 7\n\n\n\nSome blocks will show the code (without the result for exercise) when you click:\n\n\n\nShow the code for 3 to the 4th\nc = 3\nd = 4\nprint(\"3 to the 4th is equal to\", c ** d)\n\n\n\nThe following blocks are used to draw extra attention based on the five types below:\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor additional or extra topics.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor practical issues or warnings.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFor key information and takeaways.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor useful or extra techniques.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFor practice or exercise problems.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Business Data Mining and Visualization",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe main references of this book are:\n\nData Analysis and Visualization in Python for Ecologists developed by The Carpentries\nIntroduction to R & Data developed by Research Data Management Support at Utrecht University.\n\nThese materials were developed for the IMT/LIS 511: Introduction to Programming for Data Science and Visualization courses taught at the University of Washington Information School; (The “INFX” in the URL title was the original prefix for these courses). However, this book has been structured to be an online resource for anyone who wishes to learn programming. Some chapters of this book have been developed in conjunction with Technical Foundations of Informatics, by Freeman and Ross.\n\n\nsite\n\ntarget format/structure of book: https://www.albany.edu/spatial/GIS4SS/lecture/",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction to Courses",
    "section": "",
    "text": "1.1 Overview\nThe materials in this website are for more than one course. The following list is the recommended path for each course.\nWe begin our journey into data mining with a brief introduction—what it is, why it matters, and how it’s applied in practice. Throughout this course, we’ll learn both the theory and the practical tools to uncover patterns and insights from data. My goal is to help you build both conceptual understanding and hands-on skills that you can use in real business and analytical contexts.\nHere’s our roadmap for this introduction:\nThink of this as a high-level overview of the field before we dive into deeper technical details in later classes.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Courses</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Course Summary",
    "section": "",
    "text": "3.1 Summary\nBUDA 450\nRecommended Path - Getting Started - Basics in Programming with Python - Basics in Data Science - Data Visualization - Supervised Data Mining - Unsupervised Data Mining",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Summary</span>"
    ]
  },
  {
    "objectID": "GS/GS-Python.html",
    "href": "GS/GS-Python.html",
    "title": "1  Python Preliminaries",
    "section": "",
    "text": "1.1 Overview\nThis section aims to provide a basic idea about this course’s main tool (Python and its packages) with guide about installing and accessing it.\nHere’s what to expect in this section:\nFollowing these instructions is strongly recommended, as it will ensure that you can run code successfully.\nBy the end of this chapter, you’ll be ready to write and run your own code!",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Preliminaries</span>"
    ]
  },
  {
    "objectID": "GS/GS-Python.html#overview",
    "href": "GS/GS-Python.html#overview",
    "title": "1  Python Preliminaries",
    "section": "",
    "text": "We’ll begin with a brief introduction to key coding concepts.\nThen, you’ll have two options to start coding:\n\nSet up Python on your own computer, or\nUse a popular online cloud-based coding environment.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Preliminaries</span>"
    ]
  },
  {
    "objectID": "GS/GS-Python.html#preliminaries",
    "href": "GS/GS-Python.html#preliminaries",
    "title": "1  Python Preliminaries",
    "section": "1.2 Preliminaries",
    "text": "1.2 Preliminaries\n\n1.2.1 Why Python?\nPython is one of the most popular programming languages.\nIt’s one of the easiest to learn. Further. Also, it can be used for a wide range of tasks as a general-purpose language. This combination is why people often say Python has a “low floor and high ceiling.”\nFurther, it’s also very versatile as it can cover various tasks (e.g., basic calculations to breakthrough innovations like large language models) in many domains, including machine learning, data science, web development, automation, engineering, etc., and accordingly across the industry, academia, and the public sector.\nIts simplicity and readability make it a common first language for students, while its deep ecosystem of libraries makes it suitable for advanced tasks.\n\n\n\nZDNet’s programming language popularity index in 2025 (Image source link).\n\n\nFor business analytics, learning Python is especially valuable. Once you become familiar with Python (or any language), learning other (more specialized) languages like R or C++ becomes much easier. Most of the programming concepts, which you’ll see in this course such as variables, loops, and data structures, similarly apply across most languages.\nBesides, Python also shines in cloud computing environments, thanks to strong support from cloud providers and its rich ecosystem of open-source packages.\n\nIn summary, Python is an outstanding language for analytics, data science, and various applications in business. It’s no surprise that it consistently ranks among the most popular programming languages in the world!\n\n\n1.2.2 Concepts\nBefore diving into coding, it’s important to understand a computational environment—the setup that allows you to write and run Python code. In this section, let’s briefly understand the key elements of a computational environment for coding.\nA computational environment typically includes elements that together define where and how your code will run:\n\nThe operating system you’re working on (e.g., macOS Catalina)\nThe version of the programming language (e.g., Python 3.10)\nInstalled packages or libraries (e.g., pandas 2.1.0)\n\nTo set up a working Python environment, you’ll need:\n\nA computer with an operating system (e.g., Windows, macOS, Linux, or a cloud-based platform)\nAn installation of Python, the programming language that enables the computer to interpret and execute the code\nVarious packages that extend Python’s capabilities for various tasks\nAnd, an Integrated Development Environment (IDE) that is a tool to write and execute code (in a convient way)\n\nLet’s walk through each of these in more detail.\n\n1.2.2.1 Operating System\nAlmost all the code you’ll see can be run on all three of the major operating systems: Windows, MacOS, and Linux, so it doesn’t matter much what operating system you’re using. Almost all cloud services use Linux.\nIf you haven’t yet decided which operating system to use, this book recommends either Linux or MacOS because, in a very small number of cases, you’ll find it easier to run the most advanced code on them rather than on Windows. Don’t panic if you have Windows already though—most things will work just fine and it tends to be power users who run into this problem. (If you’re not familiar with Linux, it’s a free operating system that is also widely used for cloud services and, while it used to have a reputation as being fearsomely difficult for beginners, some modern Linux distributions, such as Ubuntu, are pretty user-friendly.)\nIf you have Windows and you want to use Linux or Mac but don’t want to shell out for a new computer, there are a couple of options. One is to use the Windows Subsystem for Linux. It’s essentially a Linux operating system that installs alongside and integrates with your existing Windows operating system. This allows you to run code as if you were using Linux. You can get WSL for free from Microsoft. Another option (easier but can be more expensive) is to use an online cloud service, an option we’ll return to shortly.\n\n\n1.2.2.2 Python interpreter\nPython is both a programming language that humans can read, and a language that computers can read, interpret, and then carry out instructions based on. For a computer to be able to read and execute Python code, it needs to have a Python interpreter installed. There are lots of ways to install a Python “interpreter” on your own computer, this book recommends the uv distribution of Python for its flexibility, simplicity, and features. Cloud services often come with a Python interpreter installed, and we’ll see shortly how to install one on your own computer.\nIn the box below, you can see which version of Python was used to compile this book:\n\n\nCompiled with Python version: 3.11.11 (main, Dec 11 2024, 10:25:04) [Clang 14.0.6 ]\n\n\n\n\n1.2.2.3 Integrated Development Environment (IDE)\nAn integrated development environment (IDE) is a software application that provides a few tools to make coding easier. The most important of these is a way to write the code itself! IDEs are not the only way to programme, but they are perhaps the most useful.\nIf you have used Stata or Matlab, you may not have realised it, but these analytical tools bundle the interpreter and the IDE together. But they are separate things: the interpreter is a way of processing your instructions, the IDE is where you write those instructions.\nThere are a lot of integrated development environments (IDEs) out there, including PyCham, IDLE, Visual and Studio Code. In this course, Jupiter will be used as it works on all major operating systems and is one of the most popular.\n\n\n1.2.2.4 Packages\nA Python package is a collection of functions, data, and documentation that extends the capabilities of an installed version of Python. Using packages is key to most data science because most of the functionality we’ll need comes from extra packages. You’ll see statements like import numpy as np at the start of many Python code scripts—these are instructions to use an installed package (here one called numpy) and to give it a shortened name (np, for convenience) in the rest of the script. The functions in the numpy package are then accessed through syntax like np.; for example, you can take logs with np.log(x) where x is a variable containing a number. You need only install packages once, but you must import them into each script you need to use them in. We’ll see more on how to both install and use packages in subsequent chapters.\n\n\n1.2.2.5 Typical workflow\nThe typical workflow for analysis with code might be something like this:\n\nOpen up your integrated development environment (IDE)\nWrite some code in a script (a text file with code in) in your IDE\nIf necessary for the analysis that you’re doing, install any extra packages\nUse the IDE to send bits of code from the script, or the entire script, to be executed by Python and add-on packages, and to display results\n(once the project is complete) ensure the script can be run from top to bottom to re-produce your analysis\n\nWe’ll see two ways to achieve this workflow:\n\nInstalling an IDE, a Python interpreter, and any extra Python packages on your own computer\nUsing a computer in the cloud that you access through your internet browser. Cloud computers often have an IDE and Python built-in, and you can easily install extra packages in them too. However, you should be aware that the cloud service we recommend has a 60 hours / month free tier. Beyond this you’ll need to pay for extra hours.\n\nYou should pick whichever you’re more comfortable with! Eventually, you’ll probably try both.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Preliminaries</span>"
    ]
  },
  {
    "objectID": "GS/GS-Python.html#writing-codes",
    "href": "GS/GS-Python.html#writing-codes",
    "title": "1  “Python Preliminaries”",
    "section": "1.4 Writing Codes",
    "text": "1.4 Writing Codes\nIn this chapter, we’ll look at the different ways you can both write and run code. This is something that can be very confusing if you’re just getting into programming.\nThere are different ways to write and run code that suit different needs. For example, for creating a reproducible pipeline of tasks or writing production-grade software, you might opt for a script——a file that is mostly code. And you might even bundle that up in an installable package. But for sending instructions to a colleague or exploring a narrative, you might choose to write your code in a notebook because it can present text and code together more naturally than a script can.\nWe already met some ways to write and run code in previous chapters. Here, we’ll be a bit more systematic so that, by the end of the chapter, you’ll be comfortable writing code in both scripts (the most popular way) and notebooks. We’ll also look ahead to writing executable code chunks in markdown documents, which has some real strengths for communication to people who don’t need to see the code.\nLet’s start with some definitions.\n\nIDE, or integrated development environment: this is the application that you write code of all different kinds in (scripts, notebooks, markdown). This book recommends Visual Studio Code as an IDE. It’s got tons of helpful features, including support for many languages. Making an analogy with writing documents, VS Code is to programming languages what a word processor is to actual languages. JupyterLab is another IDE, but one which is geared towards the use of notebooks.\nthe interpreter: this is the programming language (eg Python) that has to be installed separately onto your computer. It is what takes your written commands and turns them into actions. VS Code and other IDEs will use whatever interpreters they can find on your computer and use them to execute the code you’ve written.\nscripts: these are files that almost exclusively contain code. They can be edited and run in an IDE or in the terminal. Python scripts always have the file extension .py.\nnotebooks: aka Jupyter Notebooks, these are files that can contain code and text in different blocks called “cells”. The code appears in code cells, while the text appears in markdown cells. You can have any number, order, or type of cells you want. The code parts can be run in an IDE either all at once or however you like. Jupyter Notebooks always have the file extension .ipynb. Notebooks can be exported to other formats, like word documents, HTML pages, PDFs, and even slides! The content for this page is a\nthe terminal: this is the text interface that you use to send instructions to your computer’s operating system. It’s typically what you use to install new packages, for example with commands like pip install packagename. Although your computer will come with a separate terminal application too, you can open a terminal in VS Code by clicking on Terminal &gt; New Terminal at the top of your VS Code window (there’s a keyboard shortcut too, but it varies across systems).\nmarkdown: this is a lightweight language that turns simple text commands into professional looking documents. It’s widely used by people who code. It’s also what’s used for the text cells in Jupyter Notebooks. When not in a notebook, files containing markdown always have the extension .md. With the Visual Studio Code markdown extensions installed, you can right-click within a Markdown file in VS Code and then select Markdown Preview Enhanced to see how the rendered document will look. The difference between HTML code and the same website viewed in a browser like Chrome is a good analogy for the difference between what you see in a .md file and what you see in the preview of that markdown file.\nquarto markdown: this is a special variant of markdown, with file extension .qmd, that can be used to combine text and code that gets executed so that code outputs are inserted into final outputs such as html or pdf documents.\n\nLet’s now turn to all of the different ways you can write code in a fully-featured integrated development environment like Visual Studio Code. They each have pros and cons, and you’re likely to want to use them at different times. The table below sets out all of the different ways you can write, and execute, code.\nIf you’re looking for a typical workflow, this book recommends working with scripts (files that end in .py) and the VS Code interactive window. Remember, if you’re working with a .py file, you can always open the Visual Studio Code interactive window by right-clicking somewhere within the script and selecting ‘Run in interactive window’.\n\n\n\nWhat\nHow to use\nPrerequisites\nPros\nCons\n\n\n\n\nScript, eg script.py\n‘Run in interactive window’ in an integrated development environment (IDE)\nPython installation + an IDE with Python support, eg Visual Studio Code.\nCan be run all-in-one or step-by-step as needed. Very powerful tools available to aid coding in scripts. De facto standard for production-quality code. Can be imported by other scripts. Version control friendly.\nNot very good if you want to have lots of text alongside code.\n\n\nJupyter Notebook, eg notebook.ipynb\nOpen the file with Visual Studio Code.\nUse Visual Studio Code and the VS Code Jupyter extension.\nCode and text can alternate in the same document. Rich outputs of code can be integrated into document. Can export to PDF, HTML, and more, with control over whether code inputs/outputs are shown, and either exported directly or via Quarto. Can be run all-in-one or step-by-step as needed.\nFussy to use with version control. Code and text cannot be mixed in same ‘cell’. Not easy to import in other code files.\n\n\nMarkdown with executable code chunks using Quarto, eg markdown_script.qmd\nTo produce output, write in a mix of markdown and code blocks and then export with commands like quarto render markdown_script.qmd --to html on the command line or using the Visual Studio Code extension. Other output types available.\nInstallations of Python and Quarto, plus their dependencies.\nAllows for true mixing of text and code. Can export to wide variety of other formats, such as PDF and HTML, with control over whether code inputs/outputs are shown. Version control friendly.\nCannot be imported by other code files.\n\n\n\nSome of the options above make use of the command line, a way to issue text-based instructions to your computer. Remember, the command line (aka the terminal) can be accessed via the Terminal app on Mac, the Command Prompt app on Windows, or ctrl + alt + t on Linux. To open up the command line within Visual Studio Code, you can use the keyboard shortcut ⌃ + ` (on Mac) or ctrl + ` (Windows/Linux), or click “View &gt; Terminal”.\nNow let’s look at each of these ways to run code in more detail using a common example: Hello World!\n\n1.4.1 Scripts\nMost code is written in scripts and they should be your go-to.\nWe already met scripts, but let’s have a recap. Create a new file in Visual Studio Code called hello_world.py. In the Visual Studio Code editor, add a single line to the file:\nprint('Hello World!')\nSave the file. Right-click and, to run the script, you can either use ‘Run current file in interactive window’, or ‘Run current file in terminal’, or ‘Run selection/line in interactive window’. These are two different methods of running the script: in the IDE (VS Code in this case) or in the command line.\nA typical workflow would be selecting some lines within a script, and then hitting ‘Run selection/line in interactive window’ or using the keyboard shortcut of shift + enter.\nAs an alternative for the latter, you can open up the command line yourself and run\npython hello_world.py\nwhich will execute the script.\n\n\n1.4.2 Jupyter Notebooks\nJupyter Notebooks are another popular way to write code, in addition to scripts (.py files). Notebooks mix code and text by having a series of “cells” that are either code or text. Jupyter Notebooks are for experimentation, tinkering, and keeping text and code together. They are the lab books of the coding world. This book is mostly written in Jupyter Notebooks, including this chapter! You can download the notebooks that make up most chapters of this book and run them on your own computer: look for the download symbol at the top of each page; “.ipynb” means ipython notebook.\nThe name, ‘Jupyter’, is a reference to the three original languages supported by Jupyter, which are Julia, Python, and R, and to Galileo’s notebooks recording the discovery of the moons of Jupiter. Jupyter notebooks now support a vast number of languages beyond the original three, including Ruby, Haskell, Go, Scala, Octave, Java, and more.\n\n1.4.2.1 Writing Your First Notebook\n(Alternatively to the instructions here, you can use Google Colab to try a kind of notebook with no setup at all.)\nTo get started with Jupyter Notebooks, you’ll need to have a Python installation and to have run pip install jupyterlab on the command line (to install the packages needed for Jupyter Notebooks). Then, in Visual Studio Code, creating a new notebook is as easy as File -&gt; New File -&gt; Jupyter Notebook. Save your new notebook file as hello_world.ipynb. (You can just open any new file and name it with a .ipynb extension, but you’ll need to close the file and re-open it for VS Code to recognise that it’s a notebook.)\nThe notebook interface should automatically load and you’ll see options to create cells with plus signs labelled ‘Code’ and ‘Markdown’. A cell is an independent chunk of either code or text. Text cells use markdown, a lightweight language for creating text outputs that you will find out more about in {ref}wrkflow-markdown.\nTry adding print(\"hello world!\") to the first (code) cell and hitting the play symbol on the left-hand side of the cell. You will be prompted to select a “kernel”, a version of Python on your system. For this, it doesn’t matter which kernel (Python interpreter) you use. In future, you may want to set the kernel using the “Select Kernel” option at the top right-hand side of the screen to tell Visual Studio Code what specific version of Python you want to use to execute any code.\nNow add a markdown cell (“+ Markdown”) and enter:\n# This is a title\n\n## This is a subtitle\n\nThis notebook demonstrates printing 'hello world!' to screen.\nClick the tick that appears at the top of this cell.\nNow, for the next cell, choose code and write:\nprint('another code cell')\nTo run the notebook, you can choose to run all cells (usually a double play button at the top of the notebook page) or just each cell at a time (a play button beside a cell). ‘Running’ a markdown cell will render the markdown in display mode; running a code cell will execute it and insert the output below. When you play the code cell, you should see the ‘hello world!’ message appear.\nNote that you can use the keyboard short-cut Shift+Enter to execute cells one-by-one instead of hitting the play button.\nJupyter Notebooks are versatile and popular for early exploration of ideas, especially in fields like data science. Jupyter Notebooks can easily be run in the cloud using a browser too (via Binder or Google Colab) without any prior installation. Although it’s not got any executable code in, the page you’re reading now can be loaded into Google Colab as a Jupyter Notebook by clicking ‘Colab’ under the rocket icon at the top of the page.\nozqpomkuhazz Exercise What happens when you press \"clear outputs\"? What about \"run all?\".\nOne really nice feature of Jupyter Notebooks is that you can use them as the input files for Quarto instead of using .qmd files, and this opens up many export options and possibilities (like hiding some code inputs). You can find more information here (look for the guidance on Jupyter Notebooks aka .ipynb files) or look ahead to the chapters on {ref}wrkflow-markdown and {ref}quarto.\nYou can try a Jupyter Notebook without installing anything online at https://jupyter.org/try. Click on Try Classic Notebook for a tutorial. If you get stuck with getting started with notebooks, there’s a more in-depth VS Code and Jupyter tutorial available here.\n\n\n1.4.2.2 Tips when using Jupyter Notebooks\n\nVersion control: if you are using version control, be wary of saving the outputs of Jupyter Notebooks when you only want to save code. Most IDEs that support Jupyter Notebooks have a clear outputs option. You can also automate this as a pre-commit git hook (if you don’t know what that is, don’t worry). You could also pair your notebook to a script or markdown file (covered in the next section). Outputs or not, Jupyter Notebooks will render on GitHub, the popular remote repository for source code.\nTerminal commands: these can be run from inside a Jupyter Notebook by placing a ! in front of the command and executing the cell. For example, !ls gives the directory the notebook is in. You can also !pip install and !conda install in this way.\nMagic commands: statements that begin with % are magic commands. %whos displays information about defined variables. %run script.py runs a script called script.py. %timeit times how long the cell takes to execute. Finally, you can see many more magic commands using %quickref.\nNotebook cells can be executed in any sequence you choose. But if you’re planning to share your notebook or use it again for yourself, it’s good practice to check that its cells do what you want when run in sequence, from top to bottom.\nThere are tons of extensions to Jupyter Notebooks; you can find a list here. Of particular note is ipywidgets, which adds interactivity.\nGet help info on a command by running it but with ? appended to end.\n\n\n\n\n1.4.3 Markdown with Executable Code Chunks\nThis is by far the least common way of coding, though it has gained popularity in recent years and it’s great if you’re going to ultimately export to other formats like slides, documents, or even a website!\nWhen you have much more text combined code, even using Jupyter Notebooks can feel a bit onerous and, historically, editing the text in a notebook was a bit tedious - especially if you wanted to move cells around a lot. Markdown makes for a much more pleasant writing experience. But markdown on its own cannot execute code—but imagine you want to combine reproducibility, text, and code + code outputs: however, there is a tool called Quarto that allows you to do this by adding executable code chunks to markdown.\nAs this is a bit more of an advanced topic, and as much about communication as it is about writing code, we’ll come back to how to do it in {ref}quarto.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Preliminaries</span>"
    ]
  },
  {
    "objectID": "GS/GS-Python.html#related-readingreference",
    "href": "GS/GS-Python.html#related-readingreference",
    "title": "1  “Python Preliminaries”",
    "section": "1.5 Related Reading/Reference",
    "text": "1.5 Related Reading/Reference\n\nSections 2 & 5 of An Introduction to R: An introduction to R as one of the official manual documents by the R Development Core Team The R Manuals\nTextbook websites for R basics\n\nSection 5 of Hands-On Programming with R\nSection 2 of An Introduction to R\n\n\n(code-where)= # Writing Code\nIn this chapter, we’ll look at the different ways you can both write and run code. This is something that can be very confusing if you’re just getting into programming.\nThere are different ways to write and run code that suit different needs. For example, for creating a reproducible pipeline of tasks or writing production-grade software, you might opt for a script——a file that is mostly code. And you might even bundle that up in an installable package. But for sending instructions to a colleague or exploring a narrative, you might choose to write your code in a notebook because it can present text and code together more naturally than a script can.\nWe already met some ways to write and run code in previous chapters. Here, we’ll be a bit more systematic so that, by the end of the chapter, you’ll be comfortable writing code in both scripts (the most popular way) and notebooks. We’ll also look ahead to writing executable code chunks in markdown documents, which has some real strengths for communication to people who don’t need to see the code.\nLet’s start with some definitions.\n\nIDE, or integrated development environment: this is the application that you write code of all different kinds in (scripts, notebooks, markdown). This book recommends Visual Studio Code as an IDE. It’s got tons of helpful features, including support for many languages. Making an analogy with writing documents, VS Code is to programming languages what a word processor is to actual languages. JupyterLab is another IDE, but one which is geared towards the use of notebooks.\nthe interpreter: this is the programming language (eg Python) that has to be installed separately onto your computer. It is what takes your written commands and turns them into actions. VS Code and other IDEs will use whatever interpreters they can find on your computer and use them to execute the code you’ve written.\nscripts: these are files that almost exclusively contain code. They can be edited and run in an IDE or in the terminal. Python scripts always have the file extension .py.\nnotebooks: aka Jupyter Notebooks, these are files that can contain code and text in different blocks called “cells”. The code appears in code cells, while the text appears in markdown cells. You can have any number, order, or type of cells you want. The code parts can be run in an IDE either all at once or however you like. Jupyter Notebooks always have the file extension .ipynb. Notebooks can be exported to other formats, like word documents, HTML pages, PDFs, and even slides! The content for this page is a\nthe terminal: this is the text interface that you use to send instructions to your computer’s operating system. It’s typically what you use to install new packages, for example with commands like pip install packagename. Although your computer will come with a separate terminal application too, you can open a terminal in VS Code by clicking on Terminal &gt; New Terminal at the top of your VS Code window (there’s a keyboard shortcut too, but it varies across systems).\nmarkdown: this is a lightweight language that turns simple text commands into professional looking documents. It’s widely used by people who code. It’s also what’s used for the text cells in Jupyter Notebooks. When not in a notebook, files containing markdown always have the extension .md. With the Visual Studio Code markdown extensions installed, you can right-click within a Markdown file in VS Code and then select Markdown Preview Enhanced to see how the rendered document will look. The difference between HTML code and the same website viewed in a browser like Chrome is a good analogy for the difference between what you see in a .md file and what you see in the preview of that markdown file.\nquarto markdown: this is a special variant of markdown, with file extension .qmd, that can be used to combine text and code that gets executed so that code outputs are inserted into final outputs such as html or pdf documents.\n\nLet’s now turn to all of the different ways you can write code in a fully-featured integrated development environment like Visual Studio Code. They each have pros and cons, and you’re likely to want to use them at different times. The table below sets out all of the different ways you can write, and execute, code.\nIf you’re looking for a typical workflow, this book recommends working with scripts (files that end in .py) and the VS Code interactive window. Remember, if you’re working with a .py file, you can always open the Visual Studio Code interactive window by right-clicking somewhere within the script and selecting ‘Run in interactive window’.\n\n\n\nWhat\nHow to use\nPrerequisites\nPros\nCons\n\n\n\n\nScript, eg script.py\n‘Run in interactive window’ in an integrated development environment (IDE)\nPython installation + an IDE with Python support, eg Visual Studio Code.\nCan be run all-in-one or step-by-step as needed. Very powerful tools available to aid coding in scripts. De facto standard for production-quality code. Can be imported by other scripts. Version control friendly.\nNot very good if you want to have lots of text alongside code.\n\n\nJupyter Notebook, eg notebook.ipynb\nOpen the file with Visual Studio Code.\nUse Visual Studio Code and the VS Code Jupyter extension.\nCode and text can alternate in the same document. Rich outputs of code can be integrated into document. Can export to PDF, HTML, and more, with control over whether code inputs/outputs are shown, and either exported directly or via Quarto. Can be run all-in-one or step-by-step as needed.\nFussy to use with version control. Code and text cannot be mixed in same ‘cell’. Not easy to import in other code files.\n\n\nMarkdown with executable code chunks using Quarto, eg markdown_script.qmd\nTo produce output, write in a mix of markdown and code blocks and then export with commands like quarto render markdown_script.qmd --to html on the command line or using the Visual Studio Code extension. Other output types available.\nInstallations of Python and Quarto, plus their dependencies.\nAllows for true mixing of text and code. Can export to wide variety of other formats, such as PDF and HTML, with control over whether code inputs/outputs are shown. Version control friendly.\nCannot be imported by other code files.\n\n\n\nSome of the options above make use of the command line, a way to issue text-based instructions to your computer. Remember, the command line (aka the terminal) can be accessed via the Terminal app on Mac, the Command Prompt app on Windows, or ctrl + alt + t on Linux. To open up the command line within Visual Studio Code, you can use the keyboard shortcut ⌃ + ` (on Mac) or ctrl + ` (Windows/Linux), or click “View &gt; Terminal”.\nNow let’s look at each of these ways to run code in more detail using a common example: Hello World!",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Preliminaries</span>"
    ]
  },
  {
    "objectID": "GS/GS-Python.html#scripts-1",
    "href": "GS/GS-Python.html#scripts-1",
    "title": "1  “Python Preliminaries”",
    "section": "1.6 Scripts",
    "text": "1.6 Scripts\nMost code is written in scripts and they should be your go-to.\nWe already met scripts, but let’s have a recap. Create a new file in Visual Studio Code called hello_world.py. In the Visual Studio Code editor, add a single line to the file:\nprint('Hello World!')\nSave the file. Right-click and, to run the script, you can either use ‘Run current file in interactive window’, or ‘Run current file in terminal’, or ‘Run selection/line in interactive window’. These are two different methods of running the script: in the IDE (VS Code in this case) or in the command line.\nA typical workflow would be selecting some lines within a script, and then hitting ‘Run selection/line in interactive window’ or using the keyboard shortcut of shift + enter.\nAs an alternative for the latter, you can open up the command line yourself and run\npython hello_world.py\nwhich will execute the script.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Preliminaries</span>"
    ]
  },
  {
    "objectID": "GS/GS-Python.html#jupyter-notebooks-1",
    "href": "GS/GS-Python.html#jupyter-notebooks-1",
    "title": "1  “Python Preliminaries”",
    "section": "1.7 Jupyter Notebooks",
    "text": "1.7 Jupyter Notebooks\nJupyter Notebooks are another popular way to write code, in addition to scripts (.py files). Notebooks mix code and text by having a series of “cells” that are either code or text. Jupyter Notebooks are for experimentation, tinkering, and keeping text and code together. They are the lab books of the coding world. This book is mostly written in Jupyter Notebooks, including this chapter! You can download the notebooks that make up most chapters of this book and run them on your own computer: look for the download symbol at the top of each page; “.ipynb” means ipython notebook.\nThe name, ‘Jupyter’, is a reference to the three original languages supported by Jupyter, which are Julia, Python, and R, and to Galileo’s notebooks recording the discovery of the moons of Jupiter. Jupyter notebooks now support a vast number of languages beyond the original three, including Ruby, Haskell, Go, Scala, Octave, Java, and more.\n\n1.7.1 Writing Your First Notebook\n(Alternatively to the instructions here, you can use Google Colab to try a kind of notebook with no setup at all.)\nTo get started with Jupyter Notebooks, you’ll need to have a Python installation and to have run pip install jupyterlab on the command line (to install the packages needed for Jupyter Notebooks). Then, in Visual Studio Code, creating a new notebook is as easy as File -&gt; New File -&gt; Jupyter Notebook. Save your new notebook file as hello_world.ipynb. (You can just open any new file and name it with a .ipynb extension, but you’ll need to close the file and re-open it for VS Code to recognise that it’s a notebook.)\nThe notebook interface should automatically load and you’ll see options to create cells with plus signs labelled ‘Code’ and ‘Markdown’. A cell is an independent chunk of either code or text. Text cells use markdown, a lightweight language for creating text outputs that you will find out more about in {ref}wrkflow-markdown.\nTry adding print(\"hello world!\") to the first (code) cell and hitting the play symbol on the left-hand side of the cell. You will be prompted to select a “kernel”, a version of Python on your system. For this, it doesn’t matter which kernel (Python interpreter) you use. In future, you may want to set the kernel using the “Select Kernel” option at the top right-hand side of the screen to tell Visual Studio Code what specific version of Python you want to use to execute any code.\nNow add a markdown cell (“+ Markdown”) and enter:\n# This is a title\n\n## This is a subtitle\n\nThis notebook demonstrates printing 'hello world!' to screen.\nClick the tick that appears at the top of this cell.\nNow, for the next cell, choose code and write:\nprint('another code cell')\nTo run the notebook, you can choose to run all cells (usually a double play button at the top of the notebook page) or just each cell at a time (a play button beside a cell). ‘Running’ a markdown cell will render the markdown in display mode; running a code cell will execute it and insert the output below. When you play the code cell, you should see the ‘hello world!’ message appear.\nNote that you can use the keyboard short-cut Shift+Enter to execute cells one-by-one instead of hitting the play button.\nJupyter Notebooks are versatile and popular for early exploration of ideas, especially in fields like data science. Jupyter Notebooks can easily be run in the cloud using a browser too (via Binder or Google Colab) without any prior installation. Although it’s not got any executable code in, the page you’re reading now can be loaded into Google Colab as a Jupyter Notebook by clicking ‘Colab’ under the rocket icon at the top of the page.\nozqpomkuhazz Exercise What happens when you press \"clear outputs\"? What about \"run all?\".\nOne really nice feature of Jupyter Notebooks is that you can use them as the input files for Quarto instead of using .qmd files, and this opens up many export options and possibilities (like hiding some code inputs). You can find more information here (look for the guidance on Jupyter Notebooks aka .ipynb files) or look ahead to the chapters on {ref}wrkflow-markdown and {ref}quarto.\nYou can try a Jupyter Notebook without installing anything online at https://jupyter.org/try. Click on Try Classic Notebook for a tutorial. If you get stuck with getting started with notebooks, there’s a more in-depth VS Code and Jupyter tutorial available here.\n\n\n1.7.2 Tips when using Jupyter Notebooks\n\nVersion control: if you are using version control, be wary of saving the outputs of Jupyter Notebooks when you only want to save code. Most IDEs that support Jupyter Notebooks have a clear outputs option. You can also automate this as a pre-commit git hook (if you don’t know what that is, don’t worry). You could also pair your notebook to a script or markdown file (covered in the next section). Outputs or not, Jupyter Notebooks will render on GitHub, the popular remote repository for source code.\nTerminal commands: these can be run from inside a Jupyter Notebook by placing a ! in front of the command and executing the cell. For example, !ls gives the directory the notebook is in. You can also !pip install and !conda install in this way.\nMagic commands: statements that begin with % are magic commands. %whos displays information about defined variables. %run script.py runs a script called script.py. %timeit times how long the cell takes to execute. Finally, you can see many more magic commands using %quickref.\nNotebook cells can be executed in any sequence you choose. But if you’re planning to share your notebook or use it again for yourself, it’s good practice to check that its cells do what you want when run in sequence, from top to bottom.\nThere are tons of extensions to Jupyter Notebooks; you can find a list here. Of particular note is ipywidgets, which adds interactivity.\nGet help info on a command by running it but with ? appended to end.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Preliminaries</span>"
    ]
  },
  {
    "objectID": "GS/GS-Python.html#markdown-with-executable-code-chunks-1",
    "href": "GS/GS-Python.html#markdown-with-executable-code-chunks-1",
    "title": "3  Python Preliminaries",
    "section": "3.7 Markdown with Executable Code Chunks",
    "text": "3.7 Markdown with Executable Code Chunks\nThis is by far the least common way of coding, though it has gained popularity in recent years and it’s great if you’re going to ultimately export to other formats like slides, documents, or even a website!\nWhen you have much more text combined code, even using Jupyter Notebooks can feel a bit onerous and, historically, editing the text in a notebook was a bit tedious - especially if you wanted to move cells around a lot. Markdown makes for a much more pleasant writing experience. But markdown on its own cannot execute code—but imagine you want to combine reproducibility, text, and code + code outputs: however, there is a tool called Quarto that allows you to do this by adding executable code chunks to markdown.\nAs this is a bit more of an advanced topic, and as much about communication as it is about writing code, we’ll come back to how to do it in {ref}quarto.",
    "crumbs": [
      "**Getting Started**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Preliminaries</span>"
    ]
  },
  {
    "objectID": "BP/BP-IntroCoding.html",
    "href": "BP/BP-IntroCoding.html",
    "title": "3  Introduction to Coding",
    "section": "",
    "text": "3.1 Overview\nThis section introduces three fundamental concepts:",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Coding</span>"
    ]
  },
  {
    "objectID": "BP/BP-IntroCoding.html#coding-basics",
    "href": "BP/BP-IntroCoding.html#coding-basics",
    "title": "3  Introduction to Coding",
    "section": "3.2 Coding Basics",
    "text": "3.2 Coding Basics\nLet’s review some basics in the interests of getting you up to speed as quickly as possible. You can use Python as a calculator:\n\nprint(1 + 2 * 3)\nprint((4 + 5) / 6)\n\n7\n1.5\n\n\nYou can create new objects to keep such calculation results with the assignment operator =. Think of this assignment as copying the value(s) from the operation(s) on the right-hand side into the variable on the left-hand side.\n\nobj = 7 * 8\nprint(obj)\n\n56\n\n\nSimilarly, another object can be created from an object by using the assignment statement:\n\nother_obj = obj\nprint(other_obj)\n\n56\n\n\n\n3.2.1 Keeping Track of Variables\nYou can always inspect an already-created object by typing its name into the console or the interactive window:\n\nobj\n\n56\n\n\nIf you want to know what type of object it is, use type(object) in the interactive window like this:\n\ntype(obj)\n\nint",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Coding</span>"
    ]
  },
  {
    "objectID": "BP/BP-IntroCoding.html#coding-basics-1",
    "href": "BP/BP-IntroCoding.html#coding-basics-1",
    "title": "3  Introduction",
    "section": "3.3 Coding Basics",
    "text": "3.3 Coding Basics\nLet’s review some basics in the interests of getting you up to speed as quickly as possible. You can use Python as a calculator:\n\nprint(1 / 200 * 30)\nprint((59 + 73 + 2) / 3)\n\n0.15\n44.666666666666664\n\n\nThe extra package numpy contains many of the additional mathematical operators that you might need. If you don’t already have numpy installed, open up the terminal in Visual Studio Code (go to “Terminal -&gt; New Terminal” and then type pip install numpy into the terminal then hit return). Once you have numpy installed, you can import it and use it like this:\n\nimport numpy as np\nprint(np.sin(np.pi / 2))\n\n1.0\n\n\nYou can create new objects with the assignment operator =. You should think of this as copying the value of whatever is on the right-hand side into the variable on the left-hand side.\n\nx = 3 * 4\nprint(x)\n\n12\n\n\nThere are several structures in Python that capture multiple objects simultaneously but perhaps the most common is the list, which is designated by square brackets.\n\nprimes = [1, 2, 3, 5, 7, 11, 13]\nprint(primes)\n\n[1, 2, 3, 5, 7, 11, 13]\n\n\nAll Python statements where you create objects (known as assignment statements) have the same form:\n\nobject_name = value\n\nWhen reading that code, say “object name gets value” in your head.",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Coding</span>"
    ]
  },
  {
    "objectID": "BP/BP-IntroCoding.html#comments",
    "href": "BP/BP-IntroCoding.html#comments",
    "title": "3  Introduction to Coding",
    "section": "3.3 Comments",
    "text": "3.3 Comments\nNot all the codes are for operation, but communication.\nComments are texts that are ignored by Python but can be read by other humans. To include a comment, you can type any text after # on a line, and the following operation can be reinstated on the next line.\n\n# no. days in months\ndays_oddmonth = 31*6                 # days of odd-number months in a year\ndays_evenmonth = 365 - days_oddmonth # the rest days in a year\n\nprint(\"No. days in odd months:\", days_oddmonth, \"\\nNo. days in even months:\", days_evenmonth)\n\nNo. days in odd months: 186 \nNo. days in even months: 179\n\n\nIncluding comments in a script helps explain what’s happening with the code and (briefly) describing what the subsequent code does.\n\n\n\n\n\n\nTip\n\n\n\nIt is not necessary to leave a command for every single line of code. You should try to use informative names wherever you can because these help readers of your code (including youself) understand what is going on.",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Coding</span>"
    ]
  },
  {
    "objectID": "BP/BP-IntroCoding.html#keeping-track-of-variables",
    "href": "BP/BP-IntroCoding.html#keeping-track-of-variables",
    "title": "3  Introduction",
    "section": "3.5 Keeping Track of Variables",
    "text": "3.5 Keeping Track of Variables\nYou can always inspect an already-created object by typing its name into the interactive window:\n\nprimes\n\n[1, 2, 3, 5, 7, 11, 13]\n\n\nIf you want to know what type of object it is, use type(object) in the interactive window like this:\n\ntype(primes)\n\nlist\n\n\nVisual Studio Code has some powerful features to help you keep track of objects:\n\nAt the top of your interactive window, you should see a ‘Variables’ button. Click it to see a panel appear with all variables that you’ve defined.\nHover your mouse over variables you’ve previously entered into the interactive window; you will see a pop-up that tells you what type of object it is.\nIf you start typing a variable name into the interactive window, Visual Studio Code will try to auto-complete the name for you. Press the ‘tab’ key on your keyboard to accept the top option.",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Coding</span>"
    ]
  },
  {
    "objectID": "BP/BP-IntroCoding.html#values-variables-and-types",
    "href": "BP/BP-IntroCoding.html#values-variables-and-types",
    "title": "3  Introduction to Coding",
    "section": "3.4 Values, variables, and types",
    "text": "3.4 Values, variables, and types\nA value is datum such as a number or text. There are different types of values: 352.3 is known as a float or double, 22 is an integer, and “Hello World!” is a string. A variable is a name that refers to a value: you can think of a variable as a box that has a value, or multiple values, packed inside it.\nAlmost any word can be a variable name as long as it starts with a letter or an underscore, although there are some special keywords that can’t be used because they already have a role in the Python language: these include if, while, class, and lambda.\nCreating a variable in Python is achieved via an assignment (putting a value in the box), and this assignment is done via the = operator. The box, or variable, goes on the left while the value we wish to store appears on the right. It’s simpler than it sounds:\n\na = 10\nprint(a)\n\n10\n\n\nThis creates a variable a, assigns the value 10 to it, and prints it. Sometimes you will hear variables referred to as objects. Everything that is not a literal value, such as 10, is an object. In the above example, a is an object that has been assigned the value 10.\nHow about this:\n\nb = \"This is a string\"\nprint(b)\n\nThis is a string\n\n\nIt’s the same thing but with a different type of data, a string instead of an integer. Python is dynamically typed, which means it will guess what type of variable you’re creating as you create it. This has pros and cons, with the main pro being that it makes for more concise code.\n\nEverything is an object, and every object has a type.\n\nThe most basic built-in data types that you’ll need to know about are: integers 10, floats 1.23, strings like this, booleans True, and nothing None. Python also has a built-in type called a list [10, 15, 20] that can contain anything, even different types. So\n\nlist_example = [10, 1.23, \"like this\", True, None]\nprint(list_example)\n\n[10, 1.23, 'like this', True, None]\n\n\nis completely valid code. None is a special type of nothingness, and represents an object with no value. It has type NoneType and is more useful than you might think!\nAs well as the built-in types, packages can define their own custom types. If you ever want to check the type of a Python variable, you can call the type() function on it like so:\n\ntype(list_example)\n\nlist\n\n\nThis is especially useful for debugging ValueError messages.\nBelow is a table of common data types in Python:\n\n\n\n\n\n\n\n\n\n\nName\nType name\nType Category\nDescription\nExample\n\n\n\n\ninteger\nint\nNumeric Type\npositive/negative whole numbers\n22\n\n\nfloating point number\nfloat\nNumeric Type\nreal number in decimal form\n3.14159\n\n\nboolean\nbool\nBoolean Values\ntrue or false\nTrue\n\n\nstring\nstr\nSequence Type\ntext\n\"Hello World!\"\n\n\nlist\nlist\nSequence Type\na collection of objects - mutable & ordered\n['text entry', True, 16]\n\n\ntuple\ntuple\nSequence Type\na collection of objects - immutable & ordered\n(51.02, -0.98)\n\n\ndictionary\ndict\nMapping Type\nmapping of key-value pairs\n{'name':'Ada', 'subject':'computer science'}\n\n\nnone\nNoneType\nNull Object\nrepresents no value\nNone\n\n\nfunction\nfunction\nFunction\nRepresents a function\ndef add_one(x): return x+1\n\n\n\n::: {callout-note title=“Exercise”} What type is this Python object?\n\ncities_to_temps = {\"Paris\": 32, \"London\": 22, \"Seville\": 36, \"Wellesley\": 29}\n\nWhat type is the first key (hint: comma separated entries form key-value pairs)? :::\n\n3.4.1 Brackets\nYou may notice that there are several kinds of brackets that appear in the code we’ve seen so far, including [], {}, and (). These can play different roles depending on the context, but the most common uses are:\n\n[] is used to denote a list, eg ['a', 'b'], or to signify accessing a position using an index, eg vector[0] to get the first entry of a variable called vector.\n{} is used to denote a set, eg {'a', 'b'}, or a dictionary (with pairs of terms), eg {'first_letter': 'a', 'second_letter': 'b'}.\n() is used to denote a tuple, eg ('a', 'b'), or the arguments to a function, eg function(x) where x is the input passed to the function, or to indicate the order operations are carried out.",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Coding</span>"
    ]
  },
  {
    "objectID": "BP/BP-IntroCoding.html#operators",
    "href": "BP/BP-IntroCoding.html#operators",
    "title": "3  Introduction to Coding",
    "section": "3.5 Operators",
    "text": "3.5 Operators\nAll of the basic operators you see in mathematics are available to use: + for addition, - for subtraction, * for multiplication, ** for powers, / for division, and % for modulo. These work as you’d expect on numbers.\nBelow is a table of the basic arithmetic operations.\n\nBasic operators for numeric types\n\n\nOperator\nDescription\n\n\n\n\n+\naddition\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n**\nexponentiation\n\n\n//\ninteger division / floor division\n\n\n%\nmodulo\n\n\n@\nmatrix multiplication\n\n\n\nAs well as the usual operators, Python supports assignment operators. An example of one is x+=3, which is equivalent to running x = x + 3. Pretty much all of the operators can be used in this way.\n\n\n\n\n\n\nExercise\n\n\n\nUsing Python operations only, calculate the following:\n\\[\n\\frac{2^5}{7 \\cdot (4 - 2^3)}\n\\]\n\n\nShow the code\n(2 ** 5) / ( 7 * ( 4 - 2 ** 3) )\n\n\n-1.1428571428571428\n\n\n\n\nBesides, these operators are sometimes defined for operations of other data types.\nFor example, we can sum (actually, concatenate) strings:\n\nstring_one = \"Data Mining \"\nstring_two = \"and Visualization\"\nstring_summed = string_one + string_two\nprint(string_summed)\n\nData Mining and Visualization\n\n\nWe can use the sum to connect two lists:\n\nlist_one = [\"Accounting\", \"Management\", \"Marketing\"]\nlist_two = [\"Information System\", \"Supply Chain\"]\nlist_summed = list_one + list_two\nprint(list_summed)\n\n['Accounting', 'Management', 'Marketing', 'Information System', 'Supply Chain']\n\n\nSurprisingly, you can multiply those to repeat the elements:\n\nunit_string = \"ABC \"\nprint(unit_string * 3)\n\nunit_list = [\"abc\", \"efg\"]\nprint(unit_list * 3)\n\nABC ABC ABC \n['abc', 'efg', 'abc', 'efg', 'abc', 'efg']",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Coding</span>"
    ]
  },
  {
    "objectID": "BP/BP-IntroCoding.html#indentation",
    "href": "BP/BP-IntroCoding.html#indentation",
    "title": "3  Introduction to Coding",
    "section": "3.6 Indentation",
    "text": "3.6 Indentation\nSome empty spaces in Python may have some meaning. Specifically, the indentation in a code is a way to tell Python how to execute part of a function, a conditional clause, or loop.\nHere’s a simple example of indentation for part of an if loop. The print() statement that is indented only executes if the condition evaluates to true.\n\nx = 9\n\n# if loop\nif x &gt; 2:\n    print(\"x is greater than 2\")\nelse:\n    print(\"x is not greater than 2\")\n\nx is greater than 2\n\n\nFor the code that contains a mix of functions, conditional clauses, or loops, the indentation may need to be placed at multiple levels.\n\nx = 10\n\nif x &gt; 2:                                    # The outer conditional clause\n    # the 1st level indentation for the outer clause\n    print(\"x is greater than 2\")\n    if x == 10:                              # The inner conditional clause\n        # the 2nd level for the inner clause\n        print(\"x is equal to 10\")\n        # the 2nd level end \n    else:\n        # the 2nd level for the inner clause\n        print(\"x is not equal to 10\")\n        # the 2nd level end \n    # the 1st level end\nelse:\n    print(\"x is not greater than 2\")\n\nx is greater than 2\nx is equal to 10\n\n\n\n\n\n\n\n\nHow many spaces are required to indent?\n\n\n\nIn Python, while it’s a strong convention to use 4 spaces per indentation level for code blocks, Python allows for any number of spaces or tabs for indentation.\nHere is the same example, but with the 2-space indentation.\n\n# if loop\nif x &gt; 2:\n  print(\"x is greater than 2\")\nelse:\n  print(\"x is not greater than 2\")\n\nx is greater than 2\n\n\nHowever, note that consistency is key; you should use the same number of spaces throughout your code for readability and to avoid errors.\nYou can take a look of the discussion about the indentation in python link.\n\n\n\n::: {callout-note title=“Exercise”} Try writing a code snippet that reaches the triple level of indentation. :::",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Coding</span>"
    ]
  },
  {
    "objectID": "BP/BP-IntroCoding.html#related-readingreference",
    "href": "BP/BP-IntroCoding.html#related-readingreference",
    "title": "3  Introduction to Coding",
    "section": "3.8 Related Reading/Reference",
    "text": "3.8 Related Reading/Reference\n\nTextbook Websites about getting data in and out of R\n\nSection 5, 6 of R Programming for Data Science\nSections 3.3, 3.6 of An Introduction to R\nsection 7 of R for Data Science (2e)\n\nTextbook Websites about tidyverse of R\n\nSection 1.4 of An R Companion for Introduction to Data Mining\nSection 10 of Beginning Computer Science with R",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Coding</span>"
    ]
  },
  {
    "objectID": "BD/BD-IntroDS.html",
    "href": "BD/BD-IntroDS.html",
    "title": "5  Data Science with Python",
    "section": "",
    "text": "5.1 Overview\nOften, data cannot be used directly for your analysis. For accurate, meaningful outcomes, data have to be prepared, prior to the application of data mining tools.\n“This material outlines basic methods for preparing data in R, which is essential for analysis. It covers inspecting data, organizing data by subsetting and sorting, integrating multiple datasets, cleaning data with missing elements, and transforming data by applying mathematical operations, binning, and combining categories.”",
    "crumbs": [
      "Data with Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Science with Python</span>"
    ]
  },
  {
    "objectID": "BD/BD-IntroDS.html#overview",
    "href": "BD/BD-IntroDS.html#overview",
    "title": "5  Data Science with Python",
    "section": "",
    "text": "5.1.0.1 Related Reading/Reference\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Data with Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Science with Python</span>"
    ]
  },
  {
    "objectID": "DV/DV-IntroVisual.html",
    "href": "DV/DV-IntroVisual.html",
    "title": "9  Introduction to Data Visualization",
    "section": "",
    "text": "9.1 Overview\nThis section provides a background on data visualization and points you toward both further chapters in this book and external visualization resources.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "DV/DV-IntroVisual.html#overview",
    "href": "DV/DV-IntroVisual.html#overview",
    "title": "9  Introduction to Data Visualization",
    "section": "",
    "text": "9.1.1 Preliminaries\n\n9.1.1.1 Packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "DV/DV-IntroVisual.html#introduction",
    "href": "DV/DV-IntroVisual.html#introduction",
    "title": "9  Introduction to Data Visualization",
    "section": "9.2 Introduction",
    "text": "9.2 Introduction\nIn this chapter, you’ll get a bit of background on data visualisation and lots of pointers to both further chapters and other visualisation resources.\nThere are a plethora of options (and packages) for data visualisation using code. First, though a note about the different philosophies of data visualisation. There are broadly two categories of approach to using code to create data visualisations: imperative, where you build what you want, and declarative, where you say what you want. Choosing which to use involves a trade-off: imperative libraries offer you flexibility but at the cost of some verbosity; declarative libraries offer you a quick way to plot your data, but only if it’s in the right format to begin with, and customisation may be more difficult.\nThere are also different purposes of data visualisation. It can be useful to bear in mind the three broad categories of visualisation that are out there:\n\nexploratory\nscientific\nnarrative\n\nPython excels at exploratory and scientific visualisation. The tools for narrative visualisation are not as good as they could be for making common chart types efficiently, but did receive a big boost with the release of the lets-plot library, which is covered in Chapter {ref}vis-letsplot. And Python is blessed with perhaps the most powerfully flexible data visualisation package anywhere, matplotlib, in which you can always get the effect you want (with some work). matplotlib is covered in Chapter {ref}vis-matplotlib.\n\n9.2.1 Exploratory Visualisation\nThe first of the three kinds of vis, exploratory visualisation, is the kind that you do when you’re looking and data and trying to understand it. Just plotting the data is a really good strategy for getting a feel for any issues there might be. This is perhaps most famously demonstrated by Anscombe’s quartet: four different datasets with the same mean, standard deviation, and correlation but very different data distributions.\n(First let’s import the packages we’ll need:)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Set seed for random numbers\nseed_for_prng = 78557\nprng = np.random.default_rng(\n    seed_for_prng\n)  # prng=probabilistic random number generator\n\nAnscombe’s quartet:\n\nx = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\ndatasets = {\"I\": (x, y1), \"II\": (x, y2), \"III\": (x, y3), \"IV\": (x4, y4)}\n\nfig, axs = plt.subplots(\n    2,\n    2,\n    sharex=True,\n    sharey=True,\n    figsize=(10, 6),\n    gridspec_kw={\"wspace\": 0.08, \"hspace\": 0.08},\n)\naxs[0, 0].set(xlim=(0, 20), ylim=(2, 14))\naxs[0, 0].set(xticks=(0, 10, 20), yticks=(4, 8, 12))\n\nfor ax, (label, (x, y)) in zip(axs.flat, datasets.items()):\n    ax.text(0.1, 0.9, label, fontsize=20, transform=ax.transAxes, va=\"top\")\n    ax.tick_params(direction=\"in\", top=True, right=True)\n    ax.plot(x, y, \"o\")\n\n    # linear regression\n    p1, p0 = np.polyfit(x, y, deg=1)  # slope, intercept\n    ax.axline(xy1=(0, p0), slope=p1, color=\"r\", lw=2)\n\n    # add text box for the statistics\n    stats = (\n        f\"$\\\\mu$ = {np.mean(y):.2f}\\n\"\n        f\"$\\\\sigma$ = {np.std(y):.2f}\\n\"\n        f\"$r$ = {np.corrcoef(x, y)[0][1]:.2f}\"\n    )\n    bbox = dict(boxstyle=\"round\", fc=\"blanchedalmond\", ec=\"orange\", alpha=0.5)\n    ax.text(\n        0.95,\n        0.07,\n        stats,\n        fontsize=9,\n        bbox=bbox,\n        transform=ax.transAxes,\n        horizontalalignment=\"right\",\n    )\n\nplt.suptitle(\"Anscombe's Quartet\")\nplt.show()\n\n\n\n\n\n\n\n\nExploratory visualisation is usually quick and dirty, and flexible too. Some exploratory data viz can be automated, and some of the packages you’ll see later in the chapter on {ref}data-exploratory-analysis can do this. Beyond you and perhaps your co-authors/collaborators, not many other people should be seeing your exploratory visualisation.\n\n\n9.2.2 Scientific Visualisation\nThe second kind, scientific visualisation, is the prime cut of your exploratory visualisation. It’s the kind of plot you might include in a more technical paper, the picture that says a thousand words. I often think of the first image of a black hole by {cite:t}akiyama2019first as a prime example of this (made with matplotlib!) You can get away with having a high density of information in a scientific plot and, in short format journals, you may need to. The journal Physical Review Letters, which has an 8 page limit, has a classic of this genre in more or less every issue. Ensuring that important values can be accurately read from the plot is especially important in these kinds of charts. But they can also be the kind of plot that presents the killer results in a study; they might not be exciting to people who don’t look at charts for a living, but they might be exciting and, just as importantly, understandable by your peers.\n\n\n9.2.3 Narrative Visualisation\nThe third and final kind is narrative visualisation. This is the one that requires the most thought in the step where you go from the first view to the end product. It’s a visualisation that doesn’t just show a picture, but gives an insight. These are the kind of visualisations that you might see in the Financial Times, The Economist, or on the BBC News website. They come with aids that help the viewer focus on the aspects that the creator wanted them to (you can think of these aids or focuses as doing for visualisation what bold font does for text). They’re well worth using in your work, especially if you’re trying to communicate a particular narrative, and especially if the people you’re communicating with don’t have deep knowledge of the topic. You might use them in a paper that you hope will have a wide readership, in a blog post summarising your work, or in a report intended for a policymaker.\nYou can find more information on the topic in the {ref}vis-narrative chapter.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "DV/DV-IntroVisual.html#quick-guide-to-data-visualisation",
    "href": "DV/DV-IntroVisual.html#quick-guide-to-data-visualisation",
    "title": "9  Introduction to Data Visualization",
    "section": "9.3 Quick guide to data visualisation",
    "text": "9.3 Quick guide to data visualisation\nAddressing data visualisation, a huge topic in itself, is definitely out of scope for this book! But it’s worth discussing a few general pointers at the outset that will serve you very well if you follow them.\nA picture may tell a 1000 words, but you’ve got to be a bit careful about what those words are. The first question you should ask yourself when it comes to data visualisation is ‘what does this plot tell the viewer?’, ie what do you want people to take away from your chart. That nugget of information should be as apparent as possible from the plot. Then you want to ensure that people do take away what you meant them to; the viewer should be left in little doubt about what you are saying.\nAnother factor to bear in mind is that papers typically don’t have more than, say, ten plots in them–and frequently fewer than that. So each one must count and advance the narrative of your work somehow. (Easier to say, hard to do in practice.) As an example, if you have data that are normally distributed, and you want to show this, it’s probably not worth showing it on a plot. But if you had two distributions whose differences were important for the overall story you were telling, that might be something to highlight.\nThen there are more practical matters: is this plot better done as a scatter plot or a line? Should I stack my bar chart or split out the contributions? Those questions address the type of plot you’re creating. For example, if you have observations that are independent from one another, with no auto-correlation along the x-axis, a scatter plot is more appropriate than a line chart. However, for time series, which tend to exhibit a high degree of auto-correlation, a line chart could be just the thing. As well as the overall type, for example scatter plot, you can think about adding more information through the use of colours, shapes, sizes, and so on. But my advice is always to be sparing with extra dimensions of information as it very quickly becomes difficult to read. In most cases, an x-axis, a y-axis, and, usually, one other dimension (eg colour) will be the best option.\nOnce you’ve decided on the type of chart, you can then think about smaller details. Unfortunately, lack of labels is endemic in economics (“percent of what!?”, I cry at least three times a day). Always make what you’re plotting clear and, if it has units, express them (eg “Salary (2015 USD)”). Think carefully about the tick labels to use too; you’ll want something quite different for linear versus log plots. Titles can be helpful too, if the axes labels and the chart by themselves don’t tell the whole story.\nThen, if there are very specific features you’d like to draw attention to, you can achieve this with text labels (as used liberally in the data visualisations you’ll see in newspapers like the Financial Times), greying out all but the most interesting data point, etc.; anything that singles out one part of the chart as the interesting one. A common trick is to plot the less important features with greater transparency and the important line/point/bar with solid colour. These all have the effect of drawing the eye straight to where it should spend the most time.\nThis is just the briefest of brief overviews of what you should bear in mind for good visualisation; I highly recommend the short and delightful Fundamentals of Data visualisation if you’d like to know more.\nIn terms of further resources to help you choose the right plot for the occassion, you can’t go too far wrong than the Financial Times Visual Vocabulary of charts. And, please, please use vector graphics whenever you can!\n\n9.3.1 Colour\nThis section has benefitted from this blog piece on visualisation and colour, and you can find more information there.\nColours often make a chart come alive, but, when encoding differences with colour, think carefully about what would serve your audience and message best. It’s best not to use colour randomly, but to choose colours that either add information to the chart or get out of the way of the message. Often, you’ll want to draw your colours from a ‘colour palette’, a collection of different colours that work together to create a particular effect. The best colour palettes take into account that colour blindness is a problem for many people, and they remain informative even when converted to greyscale.\nOne of the most popular Python visualisation libraries, matplotlib, comes with a wide range of colour palettes available here and you can find another good package for colour palettes here.\n\n9.3.1.1 Categorical Data\nFor (unordered) categorical data, visually distinct colour palettes (also known as qualitative palettes) are better. The basic rule is that you should use distinct hues when your values don’t have an inherent order or range. Note that this does not include Likert scales (“strongly agree, agree, neutral, disagree, strongly disagree”), because even though there are distinct categories, there is an order to the possible responses.\nHere are some examples of the qualitative hues available in the visualisation package matplotlib.\n\n# remove-input\ncmaps = [\n    (\n        \"Perceptually Uniform Sequential\",\n        [\"viridis\", \"plasma\", \"inferno\", \"magma\", \"cividis\"],\n    ),\n    (\n        \"Sequential\",\n        [\n            \"Greys\",\n            \"Purples\",\n            \"Blues\",\n            \"Greens\",\n            \"Oranges\",\n            \"Reds\",\n            \"YlOrBr\",\n            \"YlOrRd\",\n            \"OrRd\",\n            \"PuRd\",\n            \"RdPu\",\n            \"BuPu\",\n            \"GnBu\",\n            \"PuBu\",\n            \"YlGnBu\",\n            \"PuBuGn\",\n            \"BuGn\",\n            \"YlGn\",\n        ],\n    ),\n    (\n        \"Sequential (2)\",\n        [\n            \"binary\",\n            \"gist_yarg\",\n            \"gist_gray\",\n            \"gray\",\n            \"bone\",\n            \"pink\",\n            \"spring\",\n            \"summer\",\n            \"autumn\",\n            \"winter\",\n            \"cool\",\n            \"Wistia\",\n            \"hot\",\n            \"afmhot\",\n            \"gist_heat\",\n            \"copper\",\n        ],\n    ),\n    (\n        \"Diverging\",\n        [\n            \"PiYG\",\n            \"PRGn\",\n            \"BrBG\",\n            \"PuOr\",\n            \"RdGy\",\n            \"RdBu\",\n            \"RdYlBu\",\n            \"RdYlGn\",\n            \"Spectral\",\n            \"coolwarm\",\n            \"bwr\",\n            \"seismic\",\n        ],\n    ),\n    (\"Cyclic\", [\"twilight\", \"twilight_shifted\", \"hsv\"]),\n    (\n        \"Qualitative\",\n        [\n            \"Pastel1\",\n            \"Pastel2\",\n            \"Paired\",\n            \"Accent\",\n            \"Dark2\",\n            \"Set1\",\n            \"Set2\",\n            \"Set3\",\n            \"tab10\",\n            \"tab20\",\n            \"tab20b\",\n            \"tab20c\",\n        ],\n    ),\n    (\n        \"Miscellaneous\",\n        [\n            \"flag\",\n            \"prism\",\n            \"ocean\",\n            \"gist_earth\",\n            \"terrain\",\n            \"gist_stern\",\n            \"gnuplot\",\n            \"gnuplot2\",\n            \"CMRmap\",\n            \"cubehelix\",\n            \"brg\",\n            \"gist_rainbow\",\n            \"rainbow\",\n            \"jet\",\n            \"turbo\",\n            \"nipy_spectral\",\n            \"gist_ncar\",\n        ],\n    ),\n]\n\n\ngradient = np.linspace(0, 1, 256)\ngradient = np.vstack((gradient, gradient))\n\n\ndef plot_color_gradients(cmap_category, cmap_list):\n    # Create figure and adjust figure height to number of colormaps\n    with plt.style.context(\"default\"):\n        nrows = len(cmap_list)\n        figh = 0.35 + 0.15 + (nrows + (nrows - 1) * 0.1) * 0.22\n        fig, axs = plt.subplots(nrows=nrows, figsize=(6.4, figh))\n        fig.subplots_adjust(\n            top=1 - 0.35 / figh, bottom=0.15 / figh, left=0.2, right=0.99\n        )\n\n        axs[0].set_title(cmap_category + \" colormaps\", fontsize=14)\n\n        for ax, name in zip(axs, cmap_list):\n            ax.imshow(gradient, aspect=\"auto\", cmap=plt.get_cmap(name))\n            ax.text(\n                -0.01,\n                0.5,\n                name,\n                va=\"center\",\n                ha=\"right\",\n                fontsize=10,\n                transform=ax.transAxes,\n            )\n\n        # Turn off *all* ticks & spines, not just the ones with colormaps.\n        for ax in axs:\n            ax.set_axis_off()\n        plt.show()\n\n\nfor cmap_category, cmap_list in cmaps[5:6]:\n    plot_color_gradients(cmap_category, cmap_list)\n\n\n\n\n\n\n\n\n\n\n9.3.1.2 Continuous Colour Scales\nContinuously varying data need a sequential colour scale, but there are two types: sequential and diverging\nFor data that vary from low to high, you can use a sequential colourmap. Best practice is to use a sequential colourmap that is perceptually uniform. The authors of the Python package matplotlib developed several perceptually uniform colourmaps that are now widely used, not just in Python, but in other languages and contexts too {cite:ps}nunez2018optimizing. These are the ones built-in to matplotlib:\n\n# remove input\nfor cmap_category, cmap_list in cmaps[:1]:\n    plot_color_gradients(cmap_category, cmap_list)\n\n\n\n\n\n\n\n\nDo not use the JET colourmap: it is very much not perceptually uniform. If you do want a rainbow-like sequential and perceptually uniform colourmap, then turbo, developed by Google, is as good a choice as you’re going to find. You can find turbo within matplotlib.\n\n# remove-input\ndef show_one_colourmap(cmap):\n    fig, ax = plt.subplots(figsize=(6.4, 0.3))\n    ax.text(\n        -0.01, 0.5, cmap, va=\"center\", ha=\"right\", fontsize=10, transform=ax.transAxes\n    )\n    ax.imshow(gradient, aspect=\"auto\", cmap=plt.get_cmap(cmap))\n    ax.set_axis_off()\n    plt.show()\n\n\nshow_one_colourmap(\"turbo\")\n\n\n\n\n\n\n\n\nSometimes a diverging colourmap will be more appropriate for your data. These are also called bipolar or double-ended color scales and, instead of just going from low to high, they tend to have a default middle value (often brighter) with values either side that are darker in different hues. Diverging color scales are often used to visualise negative and positive differences relative to zero, election results, or Likert scales (for example, “strongly agree, agree, neutral, disagree, strongly disagree”).\nThese are the built-in ones in matplotlib:\n\n# remove input\nfor cmap_category, cmap_list in cmaps[3:4]:\n    plot_color_gradients(cmap_category, cmap_list)\n\n\n\n\n\n\n\n\nSo how do you choose between a diverging or sequential colour scale? Divering colour scales work better when i) there’s a meaningful middle point, ii) there are extremes that you want to emphasise, iii) when differences are more of the story than levels, and iv) when you don’t mind that people will have to put in a bit of extra effort to understand the chart relative to the typically more intuitive sequential colour scale.\nFinally, this book uses a colour-blind friendly qualitative scheme (you can find the list of colours in this file).\n\n# remove input\nfrom matplotlib.colors import LinearSegmentedColormap\n\ncolours = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ncmap = LinearSegmentedColormap.from_list(\n    \"coding for economists: qualitative colourmap\", colours, N=len(colours)\n)\n\nfig, ax = plt.subplots(figsize=(6.4, 0.3))\nax.imshow(gradient, aspect=\"auto\", cmap=plt.get_cmap(cmap))\nax.text(\n    -0.01,\n    0.5,\n    \"Coding for Economists\",\n    va=\"center\",\n    ha=\"right\",\n    fontsize=10,\n    transform=ax.transAxes,\n)\nax.set_axis_off()\nplt.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "DV/DV-IntroVisual.html#libraries-for-data-visualisation",
    "href": "DV/DV-IntroVisual.html#libraries-for-data-visualisation",
    "title": "9  Introduction to Data Visualization",
    "section": "9.4 Libraries for Data Visualisation",
    "text": "9.4 Libraries for Data Visualisation\nIn subsequent chapters, we’ll take a look at making visualisations with several of these libraries. Our overall advice is to use lets-plot for declarative plotting and matplotlib for imperative plotting due to their high quality, flexibility, and likelihood of long term support. You can find lots of examples of the kinds of visualisations these packages produce side-by-side over in {ref}vis-common-plots-one.\nThe most important and widely used data visualisation library in Python is matplotlib. It was used to make the first image of a black hole by {cite:t}akiyama2019first and to image the first empirical evidence of gravitational waves by {cite:t}abbott2016observation. matplotlib is an imperative visualisation library: you specify each part of what you want individually to build up an entire picture. It’s perhaps the easiest to get started with and the most difficult to master. As well as making plots, it can also be used to make diagrams, animations, and 3D visualisations (which you should use sparingly, if at all). Reach for it if you like to have very fine-grained control over your plots, for example, you’re making a bespoke or unusual chart, you need to control the look of a chart very carefully, you’re making something that requires more graphical design, or you prefer to build up your plots from elemental pieces. You can explore it further in {ref}vis-matplotlib.\nlets-plot is a fantastic and extremely high-quality declarative library that works especially well with data that are in a tidy format (one row per observation, one column per variable). It adopts a grammar of graphics approach. What this means is that all visualisations begin with the same command, ggplot, and are combinations of layers that address different aspects of a plot, for example points or lines, scale, labels, and so on. Use it if you want one of a long list of standard charts produced quickly and to a high standard, and if your data are in a tidy format. You can find out more about it in {ref}vis-letsplot.\nplotly express is another declarative-leaning library that’s suited to web apps and dashboards.\nseaborn is a popular declarative library that builds on maplotlib and works especially well with data that are in a tidy format (one row per observation, one column per variable). seaborn is still a work in progress, but you can find information on it in {ref}vis-seaborn.\nplotnine is another declarative plotting library that adopts a grammar of graphics approach and uses a ggplot command, but it’s not quite as full featured as lets-plot. You can read more about it in {ref}vis-plotnine.\naltair is yet another declarative plotting library for Python! It’s most suited to interactive graphics on the web, and produces really beautiful charts. Under the hood, it calls a javascript library named Vega-Lite that’s the sort of thing newspaper data visualisation units might use to make their infographics.\npandas also has built-in plotting functions that you will have seen in the data analysis part of this book. They are of the form df.plot.* where * could be, for example, scatter. These are convenience functions for making a quick plot of your data and they actually use matplotlib; we won’t see much of these here but you can find them in the data analysis chapter.\nWe’re going to start this chapter by finding out a little bit more about each of these data visualisation libraries before looking at some examples of how to make specific plots with all the main libraries. We’ll end by looking at some more interesting and advanced plots.\n\n9.4.1 Other Data Visualisation Tools\nThere are tons of data visualisation libraries in Python, so many that most cannot be featured in great detail. Here are a few more that may be worth looking into depending on what you need.\nHere’s a quick run down of the other libraries that are available:\n\nproplot aims to be “A lightweight matplotlib wrapper for making beautiful, publication-quality graphics”, though the style is more similar to how people might make plots in the hard sciences rather than the social sciences. The point of this library is to take some of the verbosity out of matplotlib.\nif you’re using very big data in machine learning models, it might be worth looking at Facebook’s hiplot\nSeaborn image does for image data what seaborn does for numerical and categorical data\nLit provides an open-source platform for visualization and understanding of NLP models (very impressive)\nWordcloud does exactly what you’d expect (but use word clouds very sparingly!)\nVisPy for very large datasets plotted with WebGL and GPU acceleration.\nPyQtGraph, a pure-Python graphics library for PyQt5/PySide2 and intended for use in (heavy) mathematics / scientific / engineering applications (not very user friendly).\nbokeh offers interactive web plotting in Python.\nHoloViews, a library dsigned to make data analysis and visualization seamless and simple with very concise commands (builds on bokeh and matplotlib).\nYellowBrick for visualisations of machine learning models and metrics.\nfacets for quickly visualising machine learning features (aka regressors). Also useful for exploratory data analysis.\nchartify, Spotify’s quick plotting library for data scientists.\nscienceplots provides scientific plotting styles–some associated with specific journals–for Matplotlib.\ncolour provides professional level colour tools for Python.\npalettable has extra colour palettes that work well with Matplotlib.\ncolorcet is a collection of perceptually uniform colourmaps.\nmissingno for visualization of missing data.\n\nYou can see an overview of all Python plotting tools at PyViz.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "DV/DV-IntroVisual.html#review",
    "href": "DV/DV-IntroVisual.html#review",
    "title": "9  Introduction to Data Visualization",
    "section": "9.5 Review",
    "text": "9.5 Review\nIf you know:\n\n✅ a little bit about how to use data visualisation; and\n✅ what some of the most popular libraries for data vis are in Python\n\nthen you are well on your way to being a whizz with data vis!\nhttps://datavizproject.com/ https://policyviz.com/wp-content/uploads/2021/02/DataCatalogs.pdf\ntechnical https://www.labri.fr/perso/nrougier/python-opengl/#raw-lines",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "MD/MD-TimeSeries.html",
    "href": "MD/MD-TimeSeries.html",
    "title": "7  Time Series Mining",
    "section": "",
    "text": "8 Overview\nPrincipal Component Analysis (PCA) is an unsupervised technique for dimensionality reduction with linear transformation.\n“This section introduces a feature extraction technique, that is principal component analysis (PCA). The outcomes from PCA are discussed and a visualization with biplot is applied to a PCA result. The procedure for the reduced dimensional data after transformation is presented.”\n\n8.0.0.1 Related Reading/Reference\n\nChapter 2.3 in Introduction to Data Mining, 2nd ed. (Tan et al., 2019)\n\n\n\n\n9 Data Preparation\nLet’s load the USArrests data of four attributes for 50 states, which is available in the R base package.\n\ndata('USArrests')\nhead(USArrests)\n\n           Murder Assault UrbanPop Rape\nAlabama      13.2     236       58 21.2\nAlaska       10.0     263       48 44.5\nArizona       8.1     294       80 31.0\nArkansas      8.8     190       50 19.5\nCalifornia    9.0     276       91 40.6\nColorado      7.9     204       78 38.7\n\n\nThis data contains attributes different centers and variations at different scales, which can cause potential issues in terms of measuring and interpreting the contribution of each attribute if PCA is conducted on the raw data.\nTherefore, for fair comparison of features and ease of interpretation, it is recommended to center the attribute at zero and scale the variation of each attribute. In this example, the standardization of the attributes can handle this task.\n\ndat &lt;- apply(USArrests, 2, scale)\napply(dat, 2, mean) # mean \n\n       Murder       Assault      UrbanPop          Rape \n-7.663087e-17  1.112408e-16 -4.332808e-16  8.942391e-17 \n\napply(dat, 2, sd)   # standard deviation\n\n  Murder  Assault UrbanPop     Rape \n       1        1        1        1",
    "crumbs": [
      "Mining with Dependent Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Time Series Mining</span>"
    ]
  },
  {
    "objectID": "what-next.html",
    "href": "what-next.html",
    "title": "What Next?",
    "section": "",
    "text": "Libraries:\nThe aim of this workshop is to demonstrate the power of Python and to provide you with a small foundation to be able to start using Python in your own project. We hopefully inspire you to learn more. Starting to use Python in your own project will not be easy, and you will need to invest more time to be able to use it effectively. The best way to learn is to start using it in your own project and explore the Python libraries that are relevant. You probably will get stuck, but we are always happy to help you to get back on track during our weekly walk-in hours. We are also happy to think along and suggest additional (online) courses to improve programming skills that are relevant for your project.\nThis is a short list of popular Python libraries that might be useful for your next steps:\nThe documentation sites of these libraries often contain many examples and tutorials to get you started.\nMany more libraries for working with various data types (text, images, audio, video, etc.) and a wide variety of other tasks/applications (web scraping, parallel computing, large language models, image classification, speech recognition, etc.) are available.",
    "crumbs": [
      "What Next?"
    ]
  },
  {
    "objectID": "what-next.html#libraries",
    "href": "what-next.html#libraries",
    "title": "What Next?",
    "section": "",
    "text": "numpy for numerical computing\npandas for data analysis\nscipy and statsmodels for statistics\nscikit-learn for machine learning\ngeopandas and rasterio for geospatial data\nmatplotlib and seaborn for plotting",
    "crumbs": [
      "What Next?"
    ]
  },
  {
    "objectID": "what-next.html#courses",
    "href": "what-next.html#courses",
    "title": "What Next?",
    "section": "Courses:",
    "text": "Courses:\n\nBest practices for writing reproducible code by UU RDM support\nVarious intermediate and advanced programming courses by the eScience Center\nSoftware Carpentries\nPython for Data Science and Data Wrangling (online book)\nPython Data Science Handbook (online book)",
    "crumbs": [
      "What Next?"
    ]
  },
  {
    "objectID": "what-next.html#find-us",
    "href": "what-next.html#find-us",
    "title": "What Next?",
    "section": "Find us:",
    "text": "Find us:\nWe are happy to help you in your journey to master Python and use it in your own projects. You can find us at the following places:\n\nWalk-In Hours, come with your questions!\nProgramming Cafe, informal meetup about programming. Bring your laptop, work on your project and get help when you need it!\nUU Research Engineers\nUU RDM consultants",
    "crumbs": [
      "What Next?"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "App/App_RndNumG.html",
    "href": "App/App_RndNumG.html",
    "title": "Appendix A — Random Number Generation",
    "section": "",
    "text": "A.1 Overview\nRandom Number Generation is a process by which, often by means of a random number generator (RNG), a sequence of numbers or symbols is generated that cannot be reasonably predicted better than by random chance.\nThis session is designed to introduce you to the basic concept of random number generation that is used for (stochastic) simulation.\nThis session provides an overview of random number generation (RNG). It explains random numbers generated through algorithms, and their desired properties — independence and uniformity. Also, it discusses the importance of random number generators in simulations and outlines key properties. Algorithms for generation of pseudo-random numbers, including the Linear Congruential Generator (LCG), are introduced.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "App/App_RndNumG.html#overview",
    "href": "App/App_RndNumG.html#overview",
    "title": "Appendix A — Random Number Generation",
    "section": "",
    "text": "Related Reading/Reference\n\nChapter 15.3 in Borshchev, A. and Grigoryev, I. The big book of simulation modeling: multimethod modeling with AnyLogic 8. https://www.anylogic.com/resources/books/big-book-of-simulation-modeling/\nChapter 1 in Robinson, S. (2014). Simulation: the practice of model development and use. Bloomsbury Publishing.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "App/App_RndNumG.html#randomness-in-simulation",
    "href": "App/App_RndNumG.html#randomness-in-simulation",
    "title": "Appendix A — Random Number Generation",
    "section": "A.2 Randomness in Simulation",
    "text": "A.2 Randomness in Simulation\nBefore diving into the details of random number generation, let’s first review why randomness is important in simulation modeling.\n\nA.2.1 Why Do We Use Randomness in Simulations?\nMany real-world systems involve uncertainty, meaning we cannot predict their outcomes with absolute certainty. Examples include:\n\nFinance: Stock price movements are unpredictable.\nSupply Chain*: Customer demand fluctuates daily.\nHealthcare: Patient wait times vary based on arrival patterns.\nManufacturing: Machines may experience random failures.\n\nTo model and analyze these uncertainties from many possible inputs and the outcomes therefrom, we use stochastic methods, including Monte Carlo simulations, which rely on random numbers to mimic real-world randomness.\nSince these models depend on random inputs, generating high-quality random numbers is crucial to ensure accurate and reliable results by analyzing situations.\n\n\nA.2.2 How Are Random Numbers Used in Simulations?\nRandom numbers allow us to sample from probability distributions.\n\nIn many applications, we don’t just need random numbers between 0 and 1 —– we need them to follow specific distributions, such as normal, exponential, or Poisson distributions.\nRandom numbers are first generated in the (0,1) range, then transformed to fit the required distribution for the simulation.\n\n\n\nA.2.3 Are These Numbers Truly Random?\nComputers are deterministic—they follow instructions precisely. This means they cannot generate truly random numbers. Instead, they generate pseudo-random numbers, which are:\n\nNot truly random, but appear random.\nGenerated using mathematical formulas and initial seed values.\nReplicable, meaning the same seed will always produce the same sequence of numbers.\n\n\n“Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin.” — John von Neumann\n\nThis means that using a deterministic process to generate randomness is fundamentally flawed, yet necessary for computational applications.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "App/App_RndNumG.html#random-numbers",
    "href": "App/App_RndNumG.html#random-numbers",
    "title": "Appendix A — Random Number Generation",
    "section": "A.3 Random Numbers",
    "text": "A.3 Random Numbers\n\nA.3.1 What Are Random Numbers?\nLet’s consider a sequence of \\(N\\) random numbers as a set of numbers, denoted as:\n\\[ R_1, R_2, R_3, ..., R_N. \\]\nTo be useful (considered as random numbers) in simulations, each number in the sequence is expected to exhibit certain properties.\n\n\nA.3.2 Key Properties of Random Numbers\nTwo fundamental properties of a good random number sequence are indenpendence and uniformity:\n\nIndependence – Each number in the sequence must be statistically independent from the previous numbers\n\nThe value of one number should not affect the next number. This means that knowing RiR_iRi gives no information about Ri+1R_{i+1}Ri+1.\nExample: If we generate the number 0.32, the next number should be completely unrelated, not something like 0.33 or 0.31.\n\nUniformity – The numbers should be uniformly distributed over the interval \\((0,1)\\).\n\nThe numbers should be evenly distributed between 0 and 1. This means that every subinterval of equal length should have approximately the same count of numbers.\n\nOver many trials, each sub-range within (0,1) should contain roughly the same proportion of numbers.\n\nExample: If we generate 1,000 random numbers and divide the range into 10 equal bins, each bin should contain approximately 100 numbers.\nA truly uniform distribution ensures that the generated numbers cover the entire range without clustering or gaps.\n\n✔ Uniform Distribution: Every interval gets an equal share of numbers.\n✖ Non-Uniform Distribution: Some ranges get more numbers than others, causing bias.\n\n\n\n\nA.3.2.1 What Happens If These Properties Are Violated?\nIf a random number generator does not maintain these properties:\n\nLack of Independence: The numbers follow a pattern, making results predictable\n\nIf the numbers are not independent, patterns may emerge, leading to biased results in simulations.\n\nLack of Uniformity: Some numbers occur more frequently than others, biasing the simulation.\n\n\n\nA.3.2.2 Mathematical Representation\nMathematically, we say that a sequence of random numbers follows a \\(\\mathrm{Uniform}(0,1)\\) distribution:\n\\[\nR_i ~ \\sim ~ \\mathrm{Uniform}(0,1) ~~~ \\mathrm{for} ~ i=1,2,...,N\n\\]\nThis notation means that each number \\(R_i\\) is drawn independently from a uniform distribution between 0 and 1, and\n\n\nA.3.2.3 Why Are These Properties Important?\n\nMany simulations assume independent inputs--—if they are correlated, results may be inaccurate.\nTransforming uniformly distributed numbers into other distributions (e.g., normal, exponential) requires them to be evenly spread over (0,1).\nIf random numbers fail to meet these properties, simulations may not reflect real-world randomness properly.\n\n\n\nA.3.2.4 Example\nHere are two visualizations of 1000 random numbers generated by a library (numpy) in a popular programming language (python)1:\nScatter Plot (Independence Check) – This plot shows pairs of consecutive random numbers (\\(R_i\\), \\(R_{i+1}\\)). If the numbers are truly independent, there should be no visible pattern in the scatter plot, just a random spread of points.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate pseudo-random numbers to test independence and uniformity\nnum_samples = 1000\nrandom_numbers = np.random.rand(num_samples)\n\n# Plot 1: Scatter plot (Independence check)\nplt.figure(figsize=(6, 5))\nplt.scatter(random_numbers[:-1], random_numbers[1:], alpha=0.5, color='blue')\nplt.xlabel(\"Random Number R_i\")\nplt.ylabel(\"Random Number R_(i+1)\")\nplt.title(\"Independence Check: Scatter Plot\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nHistogram (Uniformity Check) – This histogram displays the frequency distribution of the generated random numbers. A well-behaved uniform distribution should result in a flat histogram, meaning the numbers are spread evenly across the interval \\((0,1)\\).\n\n# Plot 2: Histogram (Uniformity check)\nplt.figure(figsize=(6, 5))\nplt.hist(random_numbers, bins=20, edgecolor='black', alpha=0.7)\nplt.xlabel(\"Random Number Value\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Uniformity Check: Histogram\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThese visual checks help confirm that the generated numbers maintain independence and uniformity, two crucial properties for simulation modeling.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "App/App_RndNumG.html#random-number-generators",
    "href": "App/App_RndNumG.html#random-number-generators",
    "title": "Appendix A — Random Number Generation",
    "section": "A.4 Random Number Generators",
    "text": "A.4 Random Number Generators\nLet’s explore one of the the most well-known and widely-used pseudo-random number generators (PRNGs): the Linear Congruential Generator (LCG).\n\nA.4.1 What is an LCG?\nThe Linear Congruential Generator (LCG) is a RNG for generating pseudo-random numbers using a mathematical recurrence relation.\nThis method is simple yet effective, which follows this recursive formula:\n\\[\nX_i ~ =~(aX_{i−1}+c) ~ \\mathrm{mod} ~ m\n\\] where:\n\n\\(X_i\\) is the next number in the sequence.\n\\(X_{i−1}\\) is the previous number (starting with a seed \\(X_0\\)).\n\\(a\\) is the multiplier (controls randomness spread, how numbers are spread).\n\\(c\\) is the increment (optional, often \\(0\\)).\n\\(m\\) is the modulus (determines the upper bound, the range of generated numbers).\n\nThe output random number (the final pseudo-random number) is then calculated as: \\[\nU_i ~ = ~ \\frac{X_i}{m}\n\\] where \\(U_i\\) is a uniform random number in the range \\((0,1)\\).\n\n\nA.4.2 How Does It Work?\nLCG can produce pseudo-random numbers through the following procedure and iteration:\n\nStep 1. Choose initial parameters: \\(a\\), \\(c\\), \\(m\\), and the seed \\(X_0\\).\nStep 2. Generate the next value \\(X_i\\) using the recursive formula.\nStep 3. Scale the output by dividing by \\(m\\) to obtain a number \\(U_i\\) in the range (0,1).\nStep 4. Repeat Steps 2 and 3, producing a sequence of pseudo-random numbers.\n\nIt’s simple and easy to implement with only a few arithmetic operations, and so it’s very fast for generating sequences with reproducibility (i.e., given the same parameters and seed, the same sequence can be generated where the same calculations are performed).\n\nA.4.2.1 Example of Step-by-Step Calculation\nLet’s consider an LCG with the following parameters:\n\n\\(a ~ = ~ 123\\)\n\\(c ~ = ~ 4\\)\n\\(m ~ = ~ 56789\\)\n\\(X_0 ~ = ~ 1\\) (seed).\n\nUsing the LCG formula: \\[\n\\begin{split}\n&X_1 ~ = ~ (123 \\times 1 + 4) ~ \\mathrm{mod} ~ 56789 = ~ 127 ~ \\mathrm{mod} ~ 56789 = 127 \\\\\n&U_1 = \\frac{127}{56789} = 0.002236 \\\\\n&X_2 ~ = ~ (123 \\times 127 + 4) ~ \\mathrm{mod} ~ 56789 = 15625 ~ \\mathrm{mod} ~ 56789 = 15625 \\\\\n&U_2 = \\frac{15625}{56789} = 0.275141 \\\\\n&\\vdots\n\\end{split}\n\\]\nWe repeat this process to generate pseudo-random numbers in python.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters for the LCG\na = 123  # Multiplier\nc = 4  # Increment\nm = 5678  # Modulus\nX0 = 1  # Seed\nnum_samples = 50  # Number of random numbers to generate\n\n# Generate the LCG sequence\nX = [X0]\nfor _ in range(num_samples - 1):\n    X.append((a * X[-1] + c) % m)\n\n# Normalize to get U values in (0,1)\nU = [x / m for x in X]\n\n# Plot the generated numbers\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, num_samples + 1), U, marker='o', linestyle='-', color='b')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Generated Random Number (U)\")\nplt.title(\"Linear Congruential Generator (LCG) Output\")\nplt.grid(True)\nplt.ylim(0, 1)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nA.4.2.2 Example in Excel\nExcel provides built-in random number functions, but we can manually implement an LCG using simple formulas. For the following example’s implementation on Excel, please check this file - Mod_2-4_RNG_Ex.xlsx.\n\nChoose LCG parameters:\n\nLet’s consider predetermined parameters:\n\na = 123\nc = 4\nm = 56789\nX0 = 1\n\n\nSet up columns in Excel:\n\nColumn F: Iteration number i\nColumn G: the linear term = a * X(i-1) + c\nColumn H: Xi (integer sequence) = Q mod m\nColumn I: Ui (normalized random numbers) = X(i) / m\n\nDrag the formulas down to generate multiple random numbers.\n\n\n\n\nA.4.3 Key Parameters in LCGs\nTo ensure the quality of the generated pseudo-random numbers, the following parameters of a LCG must be carefully selected:\n\nMultiplier \\(a\\)\n\nMust be chosen to ensure a long cycle length.\nA poor choice can result in short cycles or patterns in the output.\n\nIncrement \\(c\\)\n\nIf \\(c=0\\), the generator is called a multiplicative LCG (not recommended in some cases).\nIf \\(c\\neq0\\), the generator is an additive LCG, which can avoid some weaknesses of the multiplicative version.\n\nModulus \\(m\\)\n\nDetermines the maximum range of numbers before repetition.\nCommonly chosen as a large prime number or a power of 2 (e.g., \\(2^32\\) or \\(2^64\\), considering the sake of processing in a 32-bit or 64-bit computer).\n\nSeed \\(X_0\\)\n\nThe starting value—choosing a good seed ensures different runs generate different sequences.\nIf the same seed is used, the same sequence will be generated (which is useful for reproducibility).\n\n\n\nA.4.3.1 Cycle Length and Periodicity\nThe period of an LCG is the number of numbers it generates before repeating.\n\nA full-period LCG generates all numbers from \\(0\\) to \\(m - 1\\) before repeating.\nA poorly chosen \\(a\\), \\(c\\), or \\(m\\) can result in very short cycles, reducing randomness quality.\n\nFor example, the numbers generated in the example above with \\(m = 9\\) shows repetition after just a few iterations instead of spanning the full range.\n\n\n\n\n\n\nHow to Choose Good Parameters?\n\n\n\nA good LCG follows the Hull-Dobell Theorem, which states that for an LCG to have a full period:\n\n\\(m\\) and \\(c\\) must be relatively prime to (i.e., they have no common factors).\n\\(a − 1\\) must be a multiple of all prime factors of \\(m\\).\n\\(a − 1\\) must be a multiple of \\(4\\) if \\(m\\) is a power of \\(2\\).\n\nThere have been many studies on good values for LCG parameters, and many modern systems use large prime values for \\(m\\) to ensure long periods. A well organized list is available in Parameters in common use.\n\n\n\n\n\nA.4.4 Why Test an LCG?\nTo check the quality of an LCG (of course, other PRNGs too), we can apply some tests, because not all LCGs produce good random numbers. Some issues include:\n\nShort cycles – Numbers repeat too soon.\nLack of uniformity – Some values occur more often than others.\nCorrelation – Consecutive numbers may be related, reducing randomness.\n\nAs defined earlier, statistical properties are those that can readily be tested to analyze the numbers.\n\nA.4.4.1 Minimal Statistical Test for LCG: Uniformity\nOne simple way to test an LCG is to compute the sample mean of a set of generated numbers and compare it to the expected mean of a uniform distribution.\nThat is, we generate the random numbers to follow that fall into the range between 0 and 1, the theoretical mean of the uniform distribution is 0.5. If our LCG is good, the sample mean of the generated numbers should be close to 0.5.\nAlso, if the LCG is good, such sample means are expected to show stable around 0.5 for increasing sample sizes.\nSimilarly, the empirical distribution of generated random numbers should be uniformed distributed.\n\nExample: Testing an LCG in Excel\nYou can perform this test in the Excel file2 by:\n\nGenerating 1000 random numbers using an LCG.\nUsing the formula =AVERAGE(I3:I1003) to compute the sample mean.\nChecking if the value is close to 0.5.\n\n\n\n\n\n\n\nNote\n\n\n\nFor more advanced testing beyond uniformity, we can apply additional statistical randomness tests like:\n\nChi-Square Goodness-of-Fit\nKolmogorov-Smirnov (KS) Test\nAutocorrelation Test\n\n\n\n\n\n\n\nA.4.5 General Framework for Random Number Generation\nBeyond LCGs, we can define a general framework for generating random numbers:\n\nA.4.5.1 General Procedure for Random Number Generation\nMost random number generation methods follow a three-step process:\n\nStep 0: Select User-Specified Parameters\n\nChoose parameters such as seed values, modulus, lag sizes, etc.\nExample: In LCGs, we select a,c,m,X0a, c, m, X_0a,c,m,X0.\n\nStep 1: Generate a Sequence of Values\n\nThis step follows a recurrence relation or transformation function.\nExample: LCGs use: \\(X_i = (aX_{i-1} + c) ~ \\mathrm{mod} ~ m\\)\n\nStep 2: Transform to Desired Distribution and Return values based on an output function.\n\nMost applications require numbers from specific probability distributions (e.g., normal, exponential, Poisson).\nWe apply a function \\(g(X)\\) to transform uniform \\((0,1)\\) random numbers into the desired distribution.\nExample: In LCGs, we use \\(g(X) = \\frac{X_i}{m}\\)\n\n\n\nMathematical Representation\nThe general random number generation framework can be expressed as: \\[\n\\begin{split}\n&X_i = f_i(X_{i-r}, X_{i-r+1}, ..., X_{i-1}) \\quad \\text{for} \\quad r \\geq 1 \\\\\n&U_i = g(X_i)\n\\end{split}\n\\]\nwhere: * \\(f_i\\) is the function generating the sequence. * \\(g(X)\\) transforms the sequence into a desired distribution.\n\n\n\n\n\nA.4.6 Other RNGs\nWe have primarily focused on LCGs, but there are many other methods for generating pseudo-random numbers. Let’s explore a few of them.\n\nFibonacci Generators, \\(X_i = (X_{i-p} + X_{i-q}) \\mod m\\)\n\nSum of previous two numbers, which was inspired by the Fibonacci sequence.\nBetter randomness and longer periods than basic LCGs, but still has periodic issues.\nStill not as good as modern generators.\nUsed for older simulations where longer cycles were needed.\n\nRecursive Generators, \\(X_i = a_1 X_{i-1} + a_2 X_{i-2} + ... + a_r X_{i-r} \\mod m\\)\n\nDepend on prior values\nUses multiple previous values to compute the next random number:\nHelps reduce correlation compared to simple LCGs.\nUsed in early numerical simulations.\nMore complex but provides better randomness than standard LCGs.\n\nNonlinear Generators, e.g., \\(X_i = aX_{i-1}^2 + bX_{i-1} + c \\mod m\\)\n\nMore complex transformations\nUnlike LCGs, these avoid linear relationships\nBetter randomness properties.\nHarder to implement and requires more computational power.\nUseful for applications needing higher quality randomness, such as security.\nHelps remove linear correlations.\n\nCombined Generators\n\nMix different methods for better randomness\nMixes multiple RNGs to produce better sequences.\nExample: Combining two LCGs to eliminate periodicity.\nUsed in modern high-quality random number generation.\nProvides longer periods and better independence.\n\nMersenne Twister (MT19937) – The Industry Standard\n\nOne of the most widely used modern PRNGs.\nFeatures:\n\nExtremely long period, \\(2^{19937}-1\\) (hence the name).\nFast generation of random numbers.\nBetter randomness properties than LCGs.\n\nUsed in many software, including Python’s random module, Excel’s RAND() function, and many statistical simulations\nThe preferred choice for most non-cryptographic applications.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "App/App_RndNumG.html#footnotes",
    "href": "App/App_RndNumG.html#footnotes",
    "title": "Appendix A — Random Number Generation",
    "section": "",
    "text": "In some of the following examples, python was used just for the sake of documentation, although we don’t use it in the main materials of our course.↩︎\nYou may try it with different parameters.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "App/App_RndSample.html",
    "href": "App/App_RndSample.html",
    "title": "Appendix B — Random Sample",
    "section": "",
    "text": "B.1 Overview\nSampling is the selection of a subset or a statistical sample of individuals from within a statistical population to estimate characteristics of the whole population.\nThis session provides an introduction to random sampling. It explains the concepts of population and sample, and it differentiates between probability and non-probability sampling. The session also discusses probability distributions in sampling. Additionally, it introduces inverse transform sampling, explaining how random numbers from a uniform distribution can be used to generate random samples from any target probability distribution, with examples for both discrete and continuous random variables.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random Sample</span>"
    ]
  },
  {
    "objectID": "App/App_RndSample.html#overview",
    "href": "App/App_RndSample.html#overview",
    "title": "Appendix B — Random Sample",
    "section": "",
    "text": "Related Reading/Reference\n\nChapter 1 in Borshchev, A. and Grigoryev, I. The big book of simulation modeling: multimethod modeling with AnyLogic 8. https://www.anylogic.com/resources/books/big-book-of-simulation-modeling/\nChapter 1 in Robinson, S. (2014). Simulation: the practice of model development and use. Bloomsbury Publishing.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random Sample</span>"
    ]
  },
  {
    "objectID": "App/App_RndSample.html#sampling",
    "href": "App/App_RndSample.html#sampling",
    "title": "Appendix B — Random Sample",
    "section": "B.2 Sampling",
    "text": "B.2 Sampling\n\nB.2.1 Population vs. Sample\nIn any statistical analysis, including Monte Carlo simulations, we must understand the concepts of population and sample. These play a crucial role in ensuring our simulation results are meaningful and applicable to real-world scenarios.\nPopulation: The complete set of all possible observations or outcomes in a study.\n\nExample: Every single week of PC sales the store has ever experienced the past during 100 weeks.\n\n\n\n\nTable B.1: Full Population Data (100 Weeks)\n\n\n\n\n\nWeek\nDemand\n\n\n\n\n1\n1 PC\n\n\n2\n0 PCs\n\n\n3\n2 PCs\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n100\n3 PCs\n\n\n\n\n\n\nSample: A subset of the population used to analyze and infer conclusions about the larger group (i.e., the population).\n\nExample: The store’s sales data from the past 100 weeks.\n\n\n\n\nTable B.2: Sample of 5 Weeks (Chosen Randomly)\n\n\n\n\n\nWeek\nDemand\n\n\n\n\n5\n1 PC\n\n\n18\n2 PCs\n\n\n37\n0 PCs\n\n\n45\n\n\n\n72\n1 PC\n\n\n\n\n\n\nIn many practices, we often obtain an estimate (e.g., the expected demand and revenue) using a sample instead of the full population.\n\n\n\n\n\n\nWhy not use the entire population?\n\n\n\n\nCollecting all data may be impractical or impossible.\nIn simulations, we generate random samples to model uncertainty.\n\n\n\n\n\nB.2.2 Types of samples\nSampling methods are broadly categorized into probability sampling and non-probability sampling techniques.\nProbability Sampling (Random Sampling) is an approach to sampling that ensures that every member of the population has a known, non-zero chance of being selected. There are many methods for the probability sampling, including:\n\nSimple Random Sampling\n\nEvery individual has an equal chance of being selected.\nExample: Drawing names from a hat or using a random number generator.\n\nSystematic Sampling\n\nEvery k-th individual from a list after a random starting point is selected.\nExample: Choosing every 5th student from a school roster.\n\nStratified Sampling\n\nThe population is divided into subgroups (strata) based on specific characteristics (e.g., age, income level), and samples are randomly taken from each group.\nRepresentation of the overall population from all key subgroups is ensured.\n\nCluster Sampling\n\nThe population is divided into clusters (e.g., cities, schools), and entire clusters are randomly selected.\nUsed when populations are geographically spread out.\n\n\nNon-Probability Sampling is another approach to sampling that do not give every individual a known or equal chance of being selected, often leading to bias. The following are some well-known methods for the non-probability sampling:\n\nConvenience Sampling\n\nSelecting individuals who are easy to reach.\nExample: Surveying people at a shopping mall.\n\nJudgment (Purposive) Sampling\n\nThe researcher selects individuals based on expertise or specific criteria.\nExample: Interviewing only experienced professionals in an industry.\n\nQuota Sampling\n\nEnsuring that specific groups (e.g., gender, age) are represented, but selection within the group is non-random.\nExample: Interviewing 50 men and 50 women without randomization.\n\nSnowball Sampling\n\nExisting participants recruit new participants from their network.\nUsed in hard-to-reach populations (e.g., drug users, undocumented workers).\n\n\n\n\n\n\n\n\nKey Differences\n\n\n\nA good sample should be random, unbiased, and large enough to reflect the population accurately.\nProbability Sampling focuses on more representative values, and so is considered, if available, in scientific approaches.\nNon-Probability Sampling is often employed due to practical issues, where it’s easier and cheaper to collect the data but more prone to bias than the probability sampling.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random Sample</span>"
    ]
  },
  {
    "objectID": "App/App_RndSample.html#random-sampling",
    "href": "App/App_RndSample.html#random-sampling",
    "title": "Appendix B — Random Sample",
    "section": "B.3 Random Sampling",
    "text": "B.3 Random Sampling\nTo understand how Monte Carlo simulations work, we need to define the concept of random sampling.\n\nB.3.1 What is a Random Sample?\nA (simple) random sample is a subset of a population where every element has an equal chance of selection. This ensures that the sample accurately represents the population, reducing bias in statistical analysis and simulations.\n\n\n\n\n\n\nHow do we ensure a good sample for simulations?\n\n\n\nMany stochastic simulations, including Monte Carlo simulations, employ random sampling to multiple samples that mimic real-world uncertainty, which allow us to estimate probabilities and expected values. In there, random sampling plays a critical role on:\n\nEnsuring that simulation results are statistically valid.\nReducing bias, making the simulation accurate and reliable.\nForming the basis for probability estimation, leading to more realistic business decisions.\n\n\n\nFor the analysis with valid statistical properties in the result, the values in a random sample should be independent and identically distributed (i.i.d.):\n\nIndependent: No one value should influence another.\nIdentically Distributed: Each value follows the same probability distribution as the population.\n\nExample: A random sample from a set of five independent weekly demands selected randomly, assuming that each follows the same probability distribution as the overall demand.\n\n\nB.3.2 Random Sampling with Probability Distribution\nTo sample random numbers fairly, a common option is a probability distribution-based approach that selects a value based on probabilities of the possible values.\nImagine a roulette wheel divided into segments proportional to demand probabilities. The value of a higher probability is assigned a larger segment on the wheel, and the value of a lower probability is assigned a smaller segment. Using such wheel, we take the value to which the wheel arrow directs as the result from spinning the wheel.\n\nSimilarly, the key idea of random sampling is to take the realization of a random variable, which is determined at random (e.g., the result from the wheel). Then, how can we make the realization at random? One idea is to label segments proportionally assigned to possible values for the realization. For example, the following table shows the list of possible values for the weekly demand and the segments assigned, when 100 segments correspond to all possible events (i.e., 100% coverage).\n\n\n\nTable B.3: Distribution and Assigned Segments of Weekly PC Demands\n\n\n\n\n\n\n\n\n\n\nDemand per week, \\(x_i\\)\nProbability, \\(P(X = x_i)\\)\nRange of Assigned Segments\n\n\n\n\n0 PCs\n0.20 (20%)\n0 ~ 19\n\n\n1 PC\n0.40 (40%)\n20 ~ 59\n\n\n2 PCs\n0.20 (20%)\n60 ~ 79\n\n\n3 PCs\n0.10 (10%)\n80 ~ 89\n\n\n4 PCs\n0.10 (10%)\n90 ~ 99\n\n\n\n\n\n\nBy mapping the segments to the demand values, we can randomly select a demand as long as a number for one of the 100 segments is randomly generated to select the corresponding segment. In other words, we can fairly simulate the demands by using fairly generated random numbers.\nFor example, we select demands, mapping the random numbers to the demands in Table B.3, as\n\n1 PC from random number 33\n1 PC from random number 56\n2 PCs from random number 70.\n\n\n\n\n\n\n\nNote\n\n\n\nThen, one following question is — how can we generate such random numbers? As a fundamental tool for many modern techniques, most software from general purpose spreadsheets (e.g., rand() in Microsoft Excel) to packages in programming languages (e.g., numpy.random in python) can generate random numbers based on certain algorithms. For now, let’s assume random numbers can be generated. Regarding the random number generation, we will discuss more in the section Random Number Generation.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random Sample</span>"
    ]
  },
  {
    "objectID": "App/App_RndSample.html#inverse-transform-sampling",
    "href": "App/App_RndSample.html#inverse-transform-sampling",
    "title": "Appendix B — Random Sample",
    "section": "B.4 Inverse Transform Sampling",
    "text": "B.4 Inverse Transform Sampling\nMonte Carlo simulations often require generating random samples from a given probability distribution. Inverse Transform Sampling is a basic yet efficient method to achieve this.\n\nB.4.1 What is Inverse Transform Sampling?\nInverse Transform Sampling is a method to generate random numbers from a specific probability distribution using uniformly distributed random numbers.\nGiven a cumulative distribution function (CDF) \\(F_{X}(x)\\), the method finds values of \\(X\\) such that:\n\\[\nF_{X}(x) = r\n\\]\nwhere \\(r\\) is a uniform random number between \\(0\\) and \\(1\\) as \\(r \\sim \\mathrm{Uniform(0,1)}\\).\nHere, the key idea is matching the range of the CDF’s possible values, \\(0 \\leq F_{X}(x) \\leq 1\\)) and the range of \\(r\\)’s possible values ( \\(0 \\leq r \\leq 1\\) ). That is, the random number of \\(X\\) is generated by matching its value in the CDF with a randomly generated value from \\(r\\).\n\nB.4.1.1 Steps to Apply Inverse Transform Sampling\n\nGenerate a uniform random number\n\nSelect a value \\(r \\sim \\mathrm{Uniform(0,1)}\\) that is a random number between 0 and 1.\n\nCompute the inverse CDF\n\nFind the value of \\(X\\) such that: \\[\nX=F_{X}^{-1}(r)\n\\] This step converts the uniform random number \\(r\\) into a value from the target distribution.\n\nReturn the generated sample\n\nThe computed \\(X\\) follows the desired probability distribution.\n\n\nThis technique works for both discrete and continuous distributions.\n\n\n\nB.4.2 Example of Inverse Transform Sampling with Discrete RV\nInverse Transform Sampling can be applied to discrete random variables (RVs) by mapping uniform random numbers to predefined probability mass functions (PMFs).\n\nB.4.2.1 Step 1: Defining a Discrete Random Variable\nConsider a discrete RV with the following PMF \\(p_{X}(x)\\):\n\n\n\nTable B.4: The probability mass function of discrete random variable \\(X\\)\n\n\n\n\n\n\\(x_i\\)\n\\(p_X(x_i)\\)\n\n\n\n\n1\n0.2\n\n\n2\n0.3\n\n\n3\n0.5\n\n\n\n\n\n\nThe corresponding CDF of \\(X\\) is:\n\n\n\nTable B.5: The cumulative distribution function of discrete random variable \\(X\\)\n\n\n\n\n\nRange\n\\(F_X(x_i)\\)\n\n\n\n\n\\(x &lt; 1\\)\n0.0\n\n\n\\(1 \\leq x &lt; 2\\)\n0.2\n\n\n\\(2 \\leq x &lt; 3\\)\n0.5\n\n\n\\(3 \\leq x\\)\n1.0\n\n\n\n\n\n\n\n\nB.4.2.2 Step 2: Applying Inverse Transform Sampling\n\nGenerate a uniform random number \\(r \\sim \\mathrm{Uniform(0,1)}\\)\nCompute the inverse CDF\n\nFind the smallest \\(X\\) where $ F_{X}(x)$.\nExample:\n\nIf \\(r=0.15\\), we select \\(X=1\\) because \\(0.15&lt;0.2\\).\nIf \\(r=0.80\\), we select \\(X=3\\) because \\(0.5 \\leq 0.80\\).\nIf \\(r=0.35\\), we select \\(X=2\\) because \\(0.2 \\leq 0.35 &lt; 0.5\\).\n\n\nReturn the generated sample\n\nTake the selected values for \\(X\\) as the generated sample that follows the desired probability distribution.\nExample:\n\nThe sampled \\(X\\) values are \\(1\\) ,\\(3\\), and \\(2\\).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo generate random numbers in a specific format, some additional steps (e.g., transformations) can be consider in the procedure above.\nFor example, the random whole numbers between 0 and 99 can be obtained by\n\ngenerating the random number between 0 and 1 (e.g., 0.1575),\nmultiplying it by 100 (e.g., 15.75),\nand rounding off the decimal places (e.g., 15).\n\n\n\n\n\n\nB.4.3 Example of Inverse Transform Sampling with Continuous RV\nInverse Transform Sampling is also applicable to continuous RVs. Instead of mapping discrete values, we use the inverse cumulative distribution function (CDF) to generate continuous random samples.\n\nB.4.3.1 Step 1: Defining a Continuous Random Variable\nConsider a continuous RV with a probability density function (PDF) \\(f_{X}(x)\\):\n\\[\nf_{X}(x)=\n\\begin{cases}\n  \\frac{x}{8}, & 0 \\leq x &lt; 2 \\\\\n  0, & \\mathrm{otherwise}\n\\end{cases}      \n\\]\nThe corresponding cumulative distribution function (CDF) is:\n\\[\nF_{X}(x)=\n\\begin{cases}\n  0, & x &lt; 0 \\\\\n  \\frac{x^2}{16} & 0 \\leq x &lt; 4 \\\\\n  1 & x \\geq 4\n\\end{cases}      \n\\]\n\n\n\nB.4.4 Step 2: Applying Inverse Transform Sampling\n\nGenerate a uniform random number \\(r \\sim \\mathrm{Uniform(0,1)}\\)\nCompute the inverse CDF\n\nSolve for \\(X\\) using the inverse CDF \\[\nX = F_{X}^{-1}(r) = \\sqrt{16r}.\n\\]\nExample:\n\nIf \\(r=0.10\\), we select \\(X=1.26\\) because \\(0.15&lt;0.2\\).\nIf \\(r=0.90\\), we select \\(X=3.79\\) because \\(0.5 \\leq 0.80\\).\nIf \\(r=0.50\\), we select \\(X=2.83\\) because \\(0.2 \\leq 0.35 &lt; 0.5\\).\n\n\nReturn the generated sample\n\nTake the selected values for \\(X\\) as the generated sample that follows the desired probability distribution.\nExample:\n\nThe sampled \\(X\\) values are \\(1.26\\) ,\\(3.79\\), and \\(2.83\\).\n\n\n\nThis random sample is used to estimate the true demand distribution, forming the basis for the Monte Carlo Simulation process.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random Sample</span>"
    ]
  },
  {
    "objectID": "App/App_ProbDist.html",
    "href": "App/App_ProbDist.html",
    "title": "Appendix C — Probability Distribution",
    "section": "",
    "text": "D Overview\nThis section is designed to introduce you to the fundamental concepts of modeling and simulation for solving a real-world problem.\nSimulation is an imitative representation of a process or system that could exist in the real world.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "App/App_ProbDist.html#what-is-system",
    "href": "App/App_ProbDist.html#what-is-system",
    "title": "Appendix C — Probability Distribution",
    "section": "E.1 What is System?",
    "text": "E.1 What is System?\nA system is a group of interacting or interrelated elements that act together as a unified whole. Here, a system operates under a set of rules, meaning that its components follow specific interactions and behaviors.\nIn the real world, systems are all around us, and they exist in various forms:\n\nNatural systems: like the ecosystem, where plants, animals, and climate interact to maintain balance.\nTechnical systems: such as an automobile, which consists of an engine, transmission, and braking system working together.\nBusiness systems: like a supply chain, where suppliers, manufacturers, warehouses, and logistics collaborate to deliver goods to customers.\n\n\n\n\n\n\n\nHow a system can be defined?\n\n\n\nEach system is defined by having its own:\n\nPurpose – the objective or function of the system.\nBoundaries – defining what is inside and what is outside the system.\nStructure – specifying how components are arranged and interact (i.e., how the processes or operations in the system are performed with the components).\n\nFor example, a supply chain system includes multiple stakeholders—raw material suppliers, factories, warehouses, transportation services, and retailers—working together to ensure products reach customers efficiently. External factors such as market demand, government regulations, and technological advancements influence how the system operates.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "App/App_ProbDist.html#system-analysis-in-real-world",
    "href": "App/App_ProbDist.html#system-analysis-in-real-world",
    "title": "Appendix C — Probability Distribution",
    "section": "E.2 System Analysis in Real world",
    "text": "E.2 System Analysis in Real world\nWhen we want to analyze a system, our primary goal is usually to measure, improve, design, or control its characteristics and behaviors. The most direct way to do this is by studying the real system itself.\nHowever, in many cases, working directly with the actual system is either impractical or impossible. There are reasons why directly studying a real system can be challenging:\n\nDisruptive or Expensive: Intervening in a working system could cause major disruptions.\n\nExample: testing different traffic management strategies on a city’s roads could lead to congestion and inconvenience for commuters.\n\nDangerous: Experimenting with a real system could pose safety risks.\n\nExample: a nuclear power plant’s cooling system in real life could pose safety risks. Similarly, testing new medical treatments directly on patients without prior simulation could have serious health consequences.\n\nNonexistent Systems: In some cases, the system we want to study doesn’t yet exist.\n\nExample: If a company wants to build a new airport, there is no real system to test before construction. Instead, simulations are used to predict how passenger flows, security checks, and baggage handling will function.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Super/SC-DecisionTree.html",
    "href": "Super/SC-DecisionTree.html",
    "title": "7  Decision Tree for Classification",
    "section": "",
    "text": "8 Overview\nTree-based methods partition the feature (e.g., input) space into a series of decision segments, which has the added benefit of being easy to understand. Think of it as a flow chart for making decisions. The viewer of the chart is presented with a diagram that offers outcomes in response to (yes / no) questions (decisions) about important predictors found in the data set.\nThis chapter covers decision trees for classification and regression and discusses how models are built and evaluated.\n“This session provides a guide for classification with decision trees in R. Using R packages, decision trees are built from a baseline to other models, exploring model parameters and hyperparameters, and different impurity measures for splitting rules. This session explains how to grow a full decision tree, prune it, and predict unseen data using the trained model.”",
    "crumbs": [
      "Supervised - Classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Decision Tree for Classification</span>"
    ]
  },
  {
    "objectID": "Super/SC-DecisionTree.html#simple-model-building",
    "href": "Super/SC-DecisionTree.html#simple-model-building",
    "title": "7  Decision Tree for Classification",
    "section": "10.1 Simple Model building",
    "text": "10.1 Simple Model building\nLet’s create a baseline decision tree (DT) model for classification, using DecisionTreeClassifier() function in rpart package with class for method and default settings for the rest[^det2]:\n\nfrom sklearn import tree\n# Train baseline decision tree\nclf_base = tree.DecisionTreeClassifier(\n    max_depth=2, \n    random_state=1\n)\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\nThe displayed information from the object base_DT of class rpart shows a simple version of the model training result. For more detailed summary of the training result, you can use summary() function:\n\nclf_base.fit(X, y)\n\nDecisionTreeClassifier(max_depth=2, random_state=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=2, random_state=1) \n\n\nBy using attbiutes() function on the resulted tree, a list of available data attributed to the tree[^det5] can be called. To access/retrieve such attributed result, $ operator can be used.\n\nclf_base_rules = tree.export_text(clf_base)\nprint(clf_base_rules)\n\n|--- feature_0 &lt;= 5.50\n|   |--- feature_2 &lt;= 84.50\n|   |   |--- class: High\n|   |--- feature_2 &gt;  84.50\n|   |   |--- class: Low\n|--- feature_0 &gt;  5.50\n|   |--- feature_5 &lt;= 79.50\n|   |   |--- class: Low\n|   |--- feature_5 &gt;  79.50\n|   |   |--- class: Low\n\n\n\nThe preceding commands will extract the predictor (X) and target class (Y) attributes from the vertebrate dataset and create a decision tree classifier object using entropy as its impurity measure for splitting criterion. The decision tree class in Python sklearn library also supports using ‘gini’ as impurity measure. The classifier above is also constrained to generate trees with a maximum depth equals to 3. Next, the classifier is trained on the labeled data using the fit() function.\nThere is a useful function to visualize the built DT, rpart.plot() function in rpart.plot package1. Using different values for arguments type and extra, a different plot can created for the trained DT.\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\n\n# Visualize the tree\nplt.figure(figsize=(12, 6))\nplot_tree(\n    clf_base, \n    feature_names=X.columns, \n    class_names=y['mpg'].cat.categories, # class_names=label_encoders['mpg'].classes_, \n    proportion=False,\n    filled=True,\n    fontsize=8\n) \nplt.title(\"Baseline Decision Tree\")\nplt.show()",
    "crumbs": [
      "Supervised - Classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Decision Tree for Classification</span>"
    ]
  },
  {
    "objectID": "Super/SC-DecisionTree.html#full-tree-and-pruning",
    "href": "Super/SC-DecisionTree.html#full-tree-and-pruning",
    "title": "7  Decision Tree for Classification",
    "section": "10.2 Full tree and pruning",
    "text": "10.2 Full tree and pruning\nNow, let’s build another DT, allowing the model to be fully grown, with additional arguments for rpart().\n\ncp is a tree complexity parameter.\n\nWith cp=1, a tree will always have no split.\nWith cp=0, a tree will fully grow until no more split needs\n\nminsplit is the minimum number of observations (i.e., records) in a node to be attempted to be split\nminbucket is the minimum number of observations that any leaf node should have at least\n\n\n# Train full tree with no pruning (min_samples_split=2, min_samples_leaf=1)\nclf_full = tree.DecisionTreeClassifier(\n    min_samples_split=2, \n    min_samples_leaf=1, \n    ccp_alpha=0,\n    random_state=1, \n)\nclf_full.fit(X, y)\n\nDecisionTreeClassifier(ccp_alpha=0, random_state=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(ccp_alpha=0, random_state=1) \n\n\n\n# Visualize the tree\nplt.figure(figsize=(12, 6))\nplot_tree(\n    clf_full, \n    feature_names=X.columns, \n    class_names=y['mpg'].cat.categories, # class_names=label_encoders['mpg'].classes_, \n    proportion=False,\n    filled=True,\n    fontsize=1\n) \nplt.title(\"Full Decision Tree\")\nplt.show()\n\n\n\n\n\n\n\n\n\n10.2.0.1 Impurity measure for spliting rules\nSo far we have built DTs, minimizing impurity based on “Gini index” as the defaults in rpart(). The other measure “information” that is based on entropy and so information gain can be used in rpart(). Let’s train two models that use a different splitting criterion (“gini” and “information”) and then see the resulted tree test. For this, an additional argument parms is needed with a named list that contains values of different parameters to influence how the model is trained. As shown below, the two models resulted in different rules.\n\n# Comparison of split criteria: gini\nclf_gini = tree.DecisionTreeClassifier(\n    criterion=\"gini\", \n    min_samples_split=32, \n    min_samples_leaf=8, \n)\nclf_gini.fit(X, y)\n\nplt.figure(figsize=(12, 6))\nplot_tree(\n    clf_gini, \n    feature_names=X.columns, \n    class_names=y['mpg'].cat.categories, # class_names=label_encoders['mpg'].classes_, \n    proportion=False,\n    filled=True,\n    fontsize=4\n) \nplt.title(\"Decision Tree - Gini Index\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Comparison of split criteria: entropy\nclf_entropy = tree.DecisionTreeClassifier(\n    criterion=\"entropy\", \n    min_samples_split=32, \n    min_samples_leaf=8, \n)\nclf_entropy.fit(X, y)\n\nplt.figure(figsize=(12, 6))\n\nplot_tree(\n    clf_entropy, \n    feature_names=X.columns, \n    class_names=y['mpg'].cat.categories, # class_names=label_encoders['mpg'].classes_, \n    proportion=False,\n    filled=True,\n    fontsize=4\n) \nplt.title(\"Decision Tree - Entropy\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Parameters vs Hyperparameters\n\n\n\nThere are two types of parameters in many (nowadays most of ) data mining models.\n\nModel parameters (often called just parameters) are the things/values/rules that are internally determined by the learning algorithm through the induction (model training) process. For example, slope and intercept coefficients in a linear model, the numbers of splits and leaf nodes in a decision tree, etc.\nHyper parameters (sometimes called meta parameters) are the things/values/configurations that control the induction (model training) process, which need to be externally determined by additional procedures, some heuristics, or manually analysts before getting a result from the model. For example, the arguments of rpart() such as cp, minsplit, minbucket for a DT model building process, as well as the splitting criterion.\n\nTypically, most modeling functions (not only in R but also in many others) have some predetermined defaults for such hyperparameters because some effort is required to set such values. Of course, this does not imply that the defaults are appropriate for any cases.",
    "crumbs": [
      "Supervised - Classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Decision Tree for Classification</span>"
    ]
  },
  {
    "objectID": "Super/SC-DecisionTree.html#prediction-with-a-trained-model",
    "href": "Super/SC-DecisionTree.html#prediction-with-a-trained-model",
    "title": "7  Decision Tree for Classification",
    "section": "10.3 Prediction with a trained model",
    "text": "10.3 Prediction with a trained model\nLet’s create three data object in a data.frame X_te as a test data set whose Mileage will be predicted.\n\n# Create test data\nX_te = pd.DataFrame(\n    {\n        'cylinders': [4, 8, 8],\n        'displacement': [160, 170, 300],\n        'horsepower': [180, 210, 200],\n        'weight': [3650, 3800, 3900],\n        'acceleration': [12, 12, 12],\n        'model_year': [90, 95, 98],\n        'origin': [1, 0, 1],\n        \n     \n    }\n)\nX_te.columns\nX.columns\n\n# Encode test data using same label encoders\n#for col in X_te.columns:\n#    if col in label_encoders:\n#        X_te[col] = label_encoders[col].transform(X_te[col])\n\nIndex(['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration',\n       'model_year', 'origin'],\n      dtype='object')\n\n\nA trained DT model can be used to predict unseen target class of data objects in a test data set, using predict() function that takes arguments for a model, a data set to be predicted, and some options.\n\n# Predict using pruned or full model (here we use clf_full)\ny_te_pred = clf_entropy.predict(X_te)\ny_te_pred\n\narray(['Low', 'Low', 'Low'], dtype=object)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure that the data for prediction() contains all the input attributes (even in the same order) that were used to build the model.",
    "crumbs": [
      "Supervised - Classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Decision Tree for Classification</span>"
    ]
  },
  {
    "objectID": "Super/SC-DecisionTree.html#related-readingreference",
    "href": "Super/SC-DecisionTree.html#related-readingreference",
    "title": "7  Decision Tree for Classification",
    "section": "10.4 Related Reading/Reference",
    "text": "10.4 Related Reading/Reference\n\nChapter 13.1, 13.2 in Business Analytics: communicating with Numbers, 2nd ed. (Jaggia et al., 2023)\nChapter 3.1, 3.2, 3.3 in Introduction to Data Mining, 2nd ed. (Tan et al., 2019)",
    "crumbs": [
      "Supervised - Classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Decision Tree for Classification</span>"
    ]
  },
  {
    "objectID": "Super/SC-DecisionTree.html#footnotes",
    "href": "Super/SC-DecisionTree.html#footnotes",
    "title": "7  Decision Tree for Classification",
    "section": "",
    "text": "rpart.plot is a separate package to plot the tree created by using rpart package.↩︎",
    "crumbs": [
      "Supervised - Classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Decision Tree for Classification</span>"
    ]
  },
  {
    "objectID": "MD/MD-Front.html",
    "href": "MD/MD-Front.html",
    "title": "Mining with Dependent Data",
    "section": "",
    "text": "Overview\nThis chapter explores data mining methods for dependent data",
    "crumbs": [
      "Mining with Dependent Data"
    ]
  },
  {
    "objectID": "MD/MD-Front.html#overview",
    "href": "MD/MD-Front.html#overview",
    "title": "Mining with Dependent Data",
    "section": "",
    "text": "Graph Mining\nText mining\nImage Mining",
    "crumbs": [
      "Mining with Dependent Data"
    ]
  },
  {
    "objectID": "App/App-RndNumG.html",
    "href": "App/App-RndNumG.html",
    "title": "Appendix B — Random Number Generation",
    "section": "",
    "text": "B.1 Overview\nRandom Number Generation is a process by which, often by means of a random number generator (RNG), a sequence of numbers or symbols is generated that cannot be reasonably predicted better than by random chance.\nThis session is designed to introduce you to the basic concept of random number generation that is used for (stochastic) simulation.\nThis session provides an overview of random number generation (RNG). It explains random numbers generated through algorithms, and their desired properties — independence and uniformity. Also, it discusses the importance of random number generators in simulations and outlines key properties. Algorithms for generation of pseudo-random numbers, including the Linear Congruential Generator (LCG), are introduced.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "App/App-RndNumG.html#overview",
    "href": "App/App-RndNumG.html#overview",
    "title": "Appendix B — Random Number Generation",
    "section": "",
    "text": "Related Reading/Reference\n\nChapter 15.3 in Borshchev, A. and Grigoryev, I. The big book of simulation modeling: multimethod modeling with AnyLogic 8. https://www.anylogic.com/resources/books/big-book-of-simulation-modeling/\nChapter 1 in Robinson, S. (2014). Simulation: the practice of model development and use. Bloomsbury Publishing.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "App/App-RndNumG.html#randomness-in-simulation",
    "href": "App/App-RndNumG.html#randomness-in-simulation",
    "title": "Appendix B — Random Number Generation",
    "section": "B.2 Randomness in Simulation",
    "text": "B.2 Randomness in Simulation\nBefore diving into the details of random number generation, let’s first review why randomness is important in simulation modeling.\n\nB.2.1 Why Do We Use Randomness in Simulations?\nMany real-world systems involve uncertainty, meaning we cannot predict their outcomes with absolute certainty. Examples include:\n\nFinance: Stock price movements are unpredictable.\nSupply Chain*: Customer demand fluctuates daily.\nHealthcare: Patient wait times vary based on arrival patterns.\nManufacturing: Machines may experience random failures.\n\nTo model and analyze these uncertainties from many possible inputs and the outcomes therefrom, we use stochastic methods, including Monte Carlo simulations, which rely on random numbers to mimic real-world randomness.\nSince these models depend on random inputs, generating high-quality random numbers is crucial to ensure accurate and reliable results by analyzing situations.\n\n\nB.2.2 How Are Random Numbers Used in Simulations?\nRandom numbers allow us to sample from probability distributions.\n\nIn many applications, we don’t just need random numbers between 0 and 1 —– we need them to follow specific distributions, such as normal, exponential, or Poisson distributions.\nRandom numbers are first generated in the (0,1) range, then transformed to fit the required distribution for the simulation.\n\n\n\nB.2.3 Are These Numbers Truly Random?\nComputers are deterministic—they follow instructions precisely. This means they cannot generate truly random numbers. Instead, they generate pseudo-random numbers, which are:\n\nNot truly random, but appear random.\nGenerated using mathematical formulas and initial seed values.\nReplicable, meaning the same seed will always produce the same sequence of numbers.\n\n\n“Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin.” — John von Neumann\n\nThis means that using a deterministic process to generate randomness is fundamentally flawed, yet necessary for computational applications.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "App/App-RndNumG.html#random-numbers",
    "href": "App/App-RndNumG.html#random-numbers",
    "title": "Appendix B — Random Number Generation",
    "section": "B.3 Random Numbers",
    "text": "B.3 Random Numbers\n\nB.3.1 What Are Random Numbers?\nLet’s consider a sequence of \\(N\\) random numbers as a set of numbers, denoted as:\n\\[ R_1, R_2, R_3, ..., R_N. \\]\nTo be useful (considered as random numbers) in simulations, each number in the sequence is expected to exhibit certain properties.\n\n\nB.3.2 Key Properties of Random Numbers\nTwo fundamental properties of a good random number sequence are indenpendence and uniformity:\n\nIndependence – Each number in the sequence must be statistically independent from the previous numbers\n\nThe value of one number should not affect the next number. This means that knowing RiR_iRi gives no information about Ri+1R_{i+1}Ri+1.\nExample: If we generate the number 0.32, the next number should be completely unrelated, not something like 0.33 or 0.31.\n\nUniformity – The numbers should be uniformly distributed over the interval \\((0,1)\\).\n\nThe numbers should be evenly distributed between 0 and 1. This means that every subinterval of equal length should have approximately the same count of numbers.\n\nOver many trials, each sub-range within (0,1) should contain roughly the same proportion of numbers.\n\nExample: If we generate 1,000 random numbers and divide the range into 10 equal bins, each bin should contain approximately 100 numbers.\nA truly uniform distribution ensures that the generated numbers cover the entire range without clustering or gaps.\n\n✔ Uniform Distribution: Every interval gets an equal share of numbers.\n✖ Non-Uniform Distribution: Some ranges get more numbers than others, causing bias.\n\n\n\n\nB.3.2.1 What Happens If These Properties Are Violated?\nIf a random number generator does not maintain these properties:\n\nLack of Independence: The numbers follow a pattern, making results predictable\n\nIf the numbers are not independent, patterns may emerge, leading to biased results in simulations.\n\nLack of Uniformity: Some numbers occur more frequently than others, biasing the simulation.\n\n\n\nB.3.2.2 Mathematical Representation\nMathematically, we say that a sequence of random numbers follows a \\(\\mathrm{Uniform}(0,1)\\) distribution:\n\\[\nR_i ~ \\sim ~ \\mathrm{Uniform}(0,1) ~~~ \\mathrm{for} ~ i=1,2,...,N\n\\]\nThis notation means that each number \\(R_i\\) is drawn independently from a uniform distribution between 0 and 1, and\n\n\nB.3.2.3 Why Are These Properties Important?\n\nMany simulations assume independent inputs--—if they are correlated, results may be inaccurate.\nTransforming uniformly distributed numbers into other distributions (e.g., normal, exponential) requires them to be evenly spread over (0,1).\nIf random numbers fail to meet these properties, simulations may not reflect real-world randomness properly.\n\n\n\nB.3.2.4 Example\nHere are two visualizations of 1000 random numbers generated by a library (numpy) in a popular programming language (python)1:\nScatter Plot (Independence Check) – This plot shows pairs of consecutive random numbers (\\(R_i\\), \\(R_{i+1}\\)). If the numbers are truly independent, there should be no visible pattern in the scatter plot, just a random spread of points.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate pseudo-random numbers to test independence and uniformity\nnum_samples = 1000\nrandom_numbers = np.random.rand(num_samples)\n\n# Plot 1: Scatter plot (Independence check)\nplt.figure(figsize=(6, 5))\nplt.scatter(random_numbers[:-1], random_numbers[1:], alpha=0.5, color='blue')\nplt.xlabel(\"Random Number R_i\")\nplt.ylabel(\"Random Number R_(i+1)\")\nplt.title(\"Independence Check: Scatter Plot\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nHistogram (Uniformity Check) – This histogram displays the frequency distribution of the generated random numbers. A well-behaved uniform distribution should result in a flat histogram, meaning the numbers are spread evenly across the interval \\((0,1)\\).\n\n# Plot 2: Histogram (Uniformity check)\nplt.figure(figsize=(6, 5))\nplt.hist(random_numbers, bins=20, edgecolor='black', alpha=0.7)\nplt.xlabel(\"Random Number Value\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Uniformity Check: Histogram\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThese visual checks help confirm that the generated numbers maintain independence and uniformity, two crucial properties for simulation modeling.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "App/App-RndNumG.html#random-number-generators",
    "href": "App/App-RndNumG.html#random-number-generators",
    "title": "Appendix B — Random Number Generation",
    "section": "B.4 Random Number Generators",
    "text": "B.4 Random Number Generators\nLet’s explore one of the the most well-known and widely-used pseudo-random number generators (PRNGs): the Linear Congruential Generator (LCG).\n\nB.4.1 What is an LCG?\nThe Linear Congruential Generator (LCG) is a RNG for generating pseudo-random numbers using a mathematical recurrence relation.\nThis method is simple yet effective, which follows this recursive formula:\n\\[\nX_i ~ =~(aX_{i−1}+c) ~ \\mathrm{mod} ~ m\n\\] where:\n\n\\(X_i\\) is the next number in the sequence.\n\\(X_{i−1}\\) is the previous number (starting with a seed \\(X_0\\)).\n\\(a\\) is the multiplier (controls randomness spread, how numbers are spread).\n\\(c\\) is the increment (optional, often \\(0\\)).\n\\(m\\) is the modulus (determines the upper bound, the range of generated numbers).\n\nThe output random number (the final pseudo-random number) is then calculated as: \\[\nU_i ~ = ~ \\frac{X_i}{m}\n\\] where \\(U_i\\) is a uniform random number in the range \\((0,1)\\).\n\n\nB.4.2 How Does It Work?\nLCG can produce pseudo-random numbers through the following procedure and iteration:\n\nStep 1. Choose initial parameters: \\(a\\), \\(c\\), \\(m\\), and the seed \\(X_0\\).\nStep 2. Generate the next value \\(X_i\\) using the recursive formula.\nStep 3. Scale the output by dividing by \\(m\\) to obtain a number \\(U_i\\) in the range (0,1).\nStep 4. Repeat Steps 2 and 3, producing a sequence of pseudo-random numbers.\n\nIt’s simple and easy to implement with only a few arithmetic operations, and so it’s very fast for generating sequences with reproducibility (i.e., given the same parameters and seed, the same sequence can be generated where the same calculations are performed).\n\nB.4.2.1 Example of Step-by-Step Calculation\nLet’s consider an LCG with the following parameters:\n\n\\(a ~ = ~ 123\\)\n\\(c ~ = ~ 4\\)\n\\(m ~ = ~ 56789\\)\n\\(X_0 ~ = ~ 1\\) (seed).\n\nUsing the LCG formula: \\[\n\\begin{split}\n&X_1 ~ = ~ (123 \\times 1 + 4) ~ \\mathrm{mod} ~ 56789 = ~ 127 ~ \\mathrm{mod} ~ 56789 = 127 \\\\\n&U_1 = \\frac{127}{56789} = 0.002236 \\\\\n&X_2 ~ = ~ (123 \\times 127 + 4) ~ \\mathrm{mod} ~ 56789 = 15625 ~ \\mathrm{mod} ~ 56789 = 15625 \\\\\n&U_2 = \\frac{15625}{56789} = 0.275141 \\\\\n&\\vdots\n\\end{split}\n\\]\nWe repeat this process to generate pseudo-random numbers in python.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters for the LCG\na = 123  # Multiplier\nc = 4  # Increment\nm = 5678  # Modulus\nX0 = 1  # Seed\nnum_samples = 50  # Number of random numbers to generate\n\n# Generate the LCG sequence\nX = [X0]\nfor _ in range(num_samples - 1):\n    X.append((a * X[-1] + c) % m)\n\n# Normalize to get U values in (0,1)\nU = [x / m for x in X]\n\n# Plot the generated numbers\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, num_samples + 1), U, marker='o', linestyle='-', color='b')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Generated Random Number (U)\")\nplt.title(\"Linear Congruential Generator (LCG) Output\")\nplt.grid(True)\nplt.ylim(0, 1)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nB.4.2.2 Example in Excel\nExcel provides built-in random number functions, but we can manually implement an LCG using simple formulas. For the following example’s implementation on Excel, please check this file - Mod_2-4_RNG_Ex.xlsx.\n\nChoose LCG parameters:\n\nLet’s consider predetermined parameters:\n\na = 123\nc = 4\nm = 56789\nX0 = 1\n\n\nSet up columns in Excel:\n\nColumn F: Iteration number i\nColumn G: the linear term = a * X(i-1) + c\nColumn H: Xi (integer sequence) = Q mod m\nColumn I: Ui (normalized random numbers) = X(i) / m\n\nDrag the formulas down to generate multiple random numbers.\n\n\n\n\nB.4.3 Key Parameters in LCGs\nTo ensure the quality of the generated pseudo-random numbers, the following parameters of a LCG must be carefully selected:\n\nMultiplier \\(a\\)\n\nMust be chosen to ensure a long cycle length.\nA poor choice can result in short cycles or patterns in the output.\n\nIncrement \\(c\\)\n\nIf \\(c=0\\), the generator is called a multiplicative LCG (not recommended in some cases).\nIf \\(c\\neq0\\), the generator is an additive LCG, which can avoid some weaknesses of the multiplicative version.\n\nModulus \\(m\\)\n\nDetermines the maximum range of numbers before repetition.\nCommonly chosen as a large prime number or a power of 2 (e.g., \\(2^32\\) or \\(2^64\\), considering the sake of processing in a 32-bit or 64-bit computer).\n\nSeed \\(X_0\\)\n\nThe starting value—choosing a good seed ensures different runs generate different sequences.\nIf the same seed is used, the same sequence will be generated (which is useful for reproducibility).\n\n\n\nB.4.3.1 Cycle Length and Periodicity\nThe period of an LCG is the number of numbers it generates before repeating.\n\nA full-period LCG generates all numbers from \\(0\\) to \\(m - 1\\) before repeating.\nA poorly chosen \\(a\\), \\(c\\), or \\(m\\) can result in very short cycles, reducing randomness quality.\n\nFor example, the numbers generated in the example above with \\(m = 9\\) shows repetition after just a few iterations instead of spanning the full range.\n\n\n\n\n\n\nHow to Choose Good Parameters?\n\n\n\nA good LCG follows the Hull-Dobell Theorem, which states that for an LCG to have a full period:\n\n\\(m\\) and \\(c\\) must be relatively prime to (i.e., they have no common factors).\n\\(a − 1\\) must be a multiple of all prime factors of \\(m\\).\n\\(a − 1\\) must be a multiple of \\(4\\) if \\(m\\) is a power of \\(2\\).\n\nThere have been many studies on good values for LCG parameters, and many modern systems use large prime values for \\(m\\) to ensure long periods. A well organized list is available in Parameters in common use.\n\n\n\n\n\nB.4.4 Why Test an LCG?\nTo check the quality of an LCG (of course, other PRNGs too), we can apply some tests, because not all LCGs produce good random numbers. Some issues include:\n\nShort cycles – Numbers repeat too soon.\nLack of uniformity – Some values occur more often than others.\nCorrelation – Consecutive numbers may be related, reducing randomness.\n\nAs defined earlier, statistical properties are those that can readily be tested to analyze the numbers.\n\nB.4.4.1 Minimal Statistical Test for LCG: Uniformity\nOne simple way to test an LCG is to compute the sample mean of a set of generated numbers and compare it to the expected mean of a uniform distribution.\nThat is, we generate the random numbers to follow that fall into the range between 0 and 1, the theoretical mean of the uniform distribution is 0.5. If our LCG is good, the sample mean of the generated numbers should be close to 0.5.\nAlso, if the LCG is good, such sample means are expected to show stable around 0.5 for increasing sample sizes.\nSimilarly, the empirical distribution of generated random numbers should be uniformed distributed.\n\nExample: Testing an LCG in Excel\nYou can perform this test in the Excel file2 by:\n\nGenerating 1000 random numbers using an LCG.\nUsing the formula =AVERAGE(I3:I1003) to compute the sample mean.\nChecking if the value is close to 0.5.\n\n\n\n\n\n\n\nNote\n\n\n\nFor more advanced testing beyond uniformity, we can apply additional statistical randomness tests like:\n\nChi-Square Goodness-of-Fit\nKolmogorov-Smirnov (KS) Test\nAutocorrelation Test\n\n\n\n\n\n\n\nB.4.5 General Framework for Random Number Generation\nBeyond LCGs, we can define a general framework for generating random numbers:\n\nB.4.5.1 General Procedure for Random Number Generation\nMost random number generation methods follow a three-step process:\n\nStep 0: Select User-Specified Parameters\n\nChoose parameters such as seed values, modulus, lag sizes, etc.\nExample: In LCGs, we select a,c,m,X0a, c, m, X_0a,c,m,X0.\n\nStep 1: Generate a Sequence of Values\n\nThis step follows a recurrence relation or transformation function.\nExample: LCGs use: \\(X_i = (aX_{i-1} + c) ~ \\mathrm{mod} ~ m\\)\n\nStep 2: Transform to Desired Distribution and Return values based on an output function.\n\nMost applications require numbers from specific probability distributions (e.g., normal, exponential, Poisson).\nWe apply a function \\(g(X)\\) to transform uniform \\((0,1)\\) random numbers into the desired distribution.\nExample: In LCGs, we use \\(g(X) = \\frac{X_i}{m}\\)\n\n\n\nMathematical Representation\nThe general random number generation framework can be expressed as: \\[\n\\begin{split}\n&X_i = f_i(X_{i-r}, X_{i-r+1}, ..., X_{i-1}) \\quad \\text{for} \\quad r \\geq 1 \\\\\n&U_i = g(X_i)\n\\end{split}\n\\]\nwhere: * \\(f_i\\) is the function generating the sequence. * \\(g(X)\\) transforms the sequence into a desired distribution.\n\n\n\n\n\nB.4.6 Other RNGs\nWe have primarily focused on LCGs, but there are many other methods for generating pseudo-random numbers. Let’s explore a few of them.\n\nFibonacci Generators, \\(X_i = (X_{i-p} + X_{i-q}) \\mod m\\)\n\nSum of previous two numbers, which was inspired by the Fibonacci sequence.\nBetter randomness and longer periods than basic LCGs, but still has periodic issues.\nStill not as good as modern generators.\nUsed for older simulations where longer cycles were needed.\n\nRecursive Generators, \\(X_i = a_1 X_{i-1} + a_2 X_{i-2} + ... + a_r X_{i-r} \\mod m\\)\n\nDepend on prior values\nUses multiple previous values to compute the next random number:\nHelps reduce correlation compared to simple LCGs.\nUsed in early numerical simulations.\nMore complex but provides better randomness than standard LCGs.\n\nNonlinear Generators, e.g., \\(X_i = aX_{i-1}^2 + bX_{i-1} + c \\mod m\\)\n\nMore complex transformations\nUnlike LCGs, these avoid linear relationships\nBetter randomness properties.\nHarder to implement and requires more computational power.\nUseful for applications needing higher quality randomness, such as security.\nHelps remove linear correlations.\n\nCombined Generators\n\nMix different methods for better randomness\nMixes multiple RNGs to produce better sequences.\nExample: Combining two LCGs to eliminate periodicity.\nUsed in modern high-quality random number generation.\nProvides longer periods and better independence.\n\nMersenne Twister (MT19937) – The Industry Standard\n\nOne of the most widely used modern PRNGs.\nFeatures:\n\nExtremely long period, \\(2^{19937}-1\\) (hence the name).\nFast generation of random numbers.\nBetter randomness properties than LCGs.\n\nUsed in many software, including Python’s random module, Excel’s RAND() function, and many statistical simulations\nThe preferred choice for most non-cryptographic applications.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "App/App-RndNumG.html#footnotes",
    "href": "App/App-RndNumG.html#footnotes",
    "title": "Appendix B — Random Number Generation",
    "section": "",
    "text": "In some of the following examples, python was used just for the sake of documentation, although we don’t use it in the main materials of our course.↩︎\nYou may try it with different parameters.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "App/App-RndSample.html",
    "href": "App/App-RndSample.html",
    "title": "Appendix C — Random Sample",
    "section": "",
    "text": "C.1 Overview\nSampling is the selection of a subset or a statistical sample of individuals from within a statistical population to estimate characteristics of the whole population.\nThis session provides an introduction to random sampling. It explains the concepts of population and sample, and it differentiates between probability and non-probability sampling. The session also discusses probability distributions in sampling. Additionally, it introduces inverse transform sampling, explaining how random numbers from a uniform distribution can be used to generate random samples from any target probability distribution, with examples for both discrete and continuous random variables.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Random Sample</span>"
    ]
  },
  {
    "objectID": "App/App-RndSample.html#overview",
    "href": "App/App-RndSample.html#overview",
    "title": "Appendix C — Random Sample",
    "section": "",
    "text": "Related Reading/Reference\n\nChapter 1 in Borshchev, A. and Grigoryev, I. The big book of simulation modeling: multimethod modeling with AnyLogic 8. https://www.anylogic.com/resources/books/big-book-of-simulation-modeling/\nChapter 1 in Robinson, S. (2014). Simulation: the practice of model development and use. Bloomsbury Publishing.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Random Sample</span>"
    ]
  },
  {
    "objectID": "App/App-RndSample.html#sampling",
    "href": "App/App-RndSample.html#sampling",
    "title": "Appendix C — Random Sample",
    "section": "C.2 Sampling",
    "text": "C.2 Sampling\n\nC.2.1 Population vs. Sample\nIn any statistical analysis, including Monte Carlo simulations, we must understand the concepts of population and sample. These play a crucial role in ensuring our simulation results are meaningful and applicable to real-world scenarios.\nPopulation: The complete set of all possible observations or outcomes in a study.\n\nExample: Every single week of PC sales the store has ever experienced the past during 100 weeks.\n\n\n\n\nTable C.1: Full Population Data (100 Weeks)\n\n\n\n\n\nWeek\nDemand\n\n\n\n\n1\n1 PC\n\n\n2\n0 PCs\n\n\n3\n2 PCs\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n100\n3 PCs\n\n\n\n\n\n\nSample: A subset of the population used to analyze and infer conclusions about the larger group (i.e., the population).\n\nExample: The store’s sales data from the past 100 weeks.\n\n\n\n\nTable C.2: Sample of 5 Weeks (Chosen Randomly)\n\n\n\n\n\nWeek\nDemand\n\n\n\n\n5\n1 PC\n\n\n18\n2 PCs\n\n\n37\n0 PCs\n\n\n45\n\n\n\n72\n1 PC\n\n\n\n\n\n\nIn many practices, we often obtain an estimate (e.g., the expected demand and revenue) using a sample instead of the full population.\n\n\n\n\n\n\nWhy not use the entire population?\n\n\n\n\nCollecting all data may be impractical or impossible.\nIn simulations, we generate random samples to model uncertainty.\n\n\n\n\n\nC.2.2 Types of samples\nSampling methods are broadly categorized into probability sampling and non-probability sampling techniques.\nProbability Sampling (Random Sampling) is an approach to sampling that ensures that every member of the population has a known, non-zero chance of being selected. There are many methods for the probability sampling, including:\n\nSimple Random Sampling\n\nEvery individual has an equal chance of being selected.\nExample: Drawing names from a hat or using a random number generator.\n\nSystematic Sampling\n\nEvery k-th individual from a list after a random starting point is selected.\nExample: Choosing every 5th student from a school roster.\n\nStratified Sampling\n\nThe population is divided into subgroups (strata) based on specific characteristics (e.g., age, income level), and samples are randomly taken from each group.\nRepresentation of the overall population from all key subgroups is ensured.\n\nCluster Sampling\n\nThe population is divided into clusters (e.g., cities, schools), and entire clusters are randomly selected.\nUsed when populations are geographically spread out.\n\n\nNon-Probability Sampling is another approach to sampling that do not give every individual a known or equal chance of being selected, often leading to bias. The following are some well-known methods for the non-probability sampling:\n\nConvenience Sampling\n\nSelecting individuals who are easy to reach.\nExample: Surveying people at a shopping mall.\n\nJudgment (Purposive) Sampling\n\nThe researcher selects individuals based on expertise or specific criteria.\nExample: Interviewing only experienced professionals in an industry.\n\nQuota Sampling\n\nEnsuring that specific groups (e.g., gender, age) are represented, but selection within the group is non-random.\nExample: Interviewing 50 men and 50 women without randomization.\n\nSnowball Sampling\n\nExisting participants recruit new participants from their network.\nUsed in hard-to-reach populations (e.g., drug users, undocumented workers).\n\n\n\n\n\n\n\n\nKey Differences\n\n\n\nA good sample should be random, unbiased, and large enough to reflect the population accurately.\nProbability Sampling focuses on more representative values, and so is considered, if available, in scientific approaches.\nNon-Probability Sampling is often employed due to practical issues, where it’s easier and cheaper to collect the data but more prone to bias than the probability sampling.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Random Sample</span>"
    ]
  },
  {
    "objectID": "App/App-RndSample.html#random-sampling",
    "href": "App/App-RndSample.html#random-sampling",
    "title": "Appendix C — Random Sample",
    "section": "C.3 Random Sampling",
    "text": "C.3 Random Sampling\nTo understand how Monte Carlo simulations work, we need to define the concept of random sampling.\n\nC.3.1 What is a Random Sample?\nA (simple) random sample is a subset of a population where every element has an equal chance of selection. This ensures that the sample accurately represents the population, reducing bias in statistical analysis and simulations.\n\n\n\n\n\n\nHow do we ensure a good sample for simulations?\n\n\n\nMany stochastic simulations, including Monte Carlo simulations, employ random sampling to multiple samples that mimic real-world uncertainty, which allow us to estimate probabilities and expected values. In there, random sampling plays a critical role on:\n\nEnsuring that simulation results are statistically valid.\nReducing bias, making the simulation accurate and reliable.\nForming the basis for probability estimation, leading to more realistic business decisions.\n\n\n\nFor the analysis with valid statistical properties in the result, the values in a random sample should be independent and identically distributed (i.i.d.):\n\nIndependent: No one value should influence another.\nIdentically Distributed: Each value follows the same probability distribution as the population.\n\nExample: A random sample from a set of five independent weekly demands selected randomly, assuming that each follows the same probability distribution as the overall demand.\n\n\nC.3.2 Random Sampling with Probability Distribution\nTo sample random numbers fairly, a common option is a probability distribution-based approach that selects a value based on probabilities of the possible values.\nImagine a roulette wheel divided into segments proportional to demand probabilities. The value of a higher probability is assigned a larger segment on the wheel, and the value of a lower probability is assigned a smaller segment. Using such wheel, we take the value to which the wheel arrow directs as the result from spinning the wheel.\n\nSimilarly, the key idea of random sampling is to take the realization of a random variable, which is determined at random (e.g., the result from the wheel). Then, how can we make the realization at random? One idea is to label segments proportionally assigned to possible values for the realization. For example, the following table shows the list of possible values for the weekly demand and the segments assigned, when 100 segments correspond to all possible events (i.e., 100% coverage).\n\n\n\nTable C.3: Distribution and Assigned Segments of Weekly PC Demands\n\n\n\n\n\n\n\n\n\n\nDemand per week, \\(x_i\\)\nProbability, \\(P(X = x_i)\\)\nRange of Assigned Segments\n\n\n\n\n0 PCs\n0.20 (20%)\n0 ~ 19\n\n\n1 PC\n0.40 (40%)\n20 ~ 59\n\n\n2 PCs\n0.20 (20%)\n60 ~ 79\n\n\n3 PCs\n0.10 (10%)\n80 ~ 89\n\n\n4 PCs\n0.10 (10%)\n90 ~ 99\n\n\n\n\n\n\nBy mapping the segments to the demand values, we can randomly select a demand as long as a number for one of the 100 segments is randomly generated to select the corresponding segment. In other words, we can fairly simulate the demands by using fairly generated random numbers.\nFor example, we select demands, mapping the random numbers to the demands in Table C.3, as\n\n1 PC from random number 33\n1 PC from random number 56\n2 PCs from random number 70.\n\n\n\n\n\n\n\nNote\n\n\n\nThen, one following question is — how can we generate such random numbers? As a fundamental tool for many modern techniques, most software from general purpose spreadsheets (e.g., rand() in Microsoft Excel) to packages in programming languages (e.g., numpy.random in python) can generate random numbers based on certain algorithms. For now, let’s assume random numbers can be generated. Regarding the random number generation, we will discuss more in the section Random Number Generation.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Random Sample</span>"
    ]
  },
  {
    "objectID": "App/App-RndSample.html#inverse-transform-sampling",
    "href": "App/App-RndSample.html#inverse-transform-sampling",
    "title": "Appendix C — Random Sample",
    "section": "C.4 Inverse Transform Sampling",
    "text": "C.4 Inverse Transform Sampling\nMonte Carlo simulations often require generating random samples from a given probability distribution. Inverse Transform Sampling is a basic yet efficient method to achieve this.\n\nC.4.1 What is Inverse Transform Sampling?\nInverse Transform Sampling is a method to generate random numbers from a specific probability distribution using uniformly distributed random numbers.\nGiven a cumulative distribution function (CDF) \\(F_{X}(x)\\), the method finds values of \\(X\\) such that:\n\\[\nF_{X}(x) = r\n\\]\nwhere \\(r\\) is a uniform random number between \\(0\\) and \\(1\\) as \\(r \\sim \\mathrm{Uniform(0,1)}\\).\nHere, the key idea is matching the range of the CDF’s possible values, \\(0 \\leq F_{X}(x) \\leq 1\\)) and the range of \\(r\\)’s possible values ( \\(0 \\leq r \\leq 1\\) ). That is, the random number of \\(X\\) is generated by matching its value in the CDF with a randomly generated value from \\(r\\).\n\nC.4.1.1 Steps to Apply Inverse Transform Sampling\n\nGenerate a uniform random number\n\nSelect a value \\(r \\sim \\mathrm{Uniform(0,1)}\\) that is a random number between 0 and 1.\n\nCompute the inverse CDF\n\nFind the value of \\(X\\) such that: \\[\nX=F_{X}^{-1}(r)\n\\] This step converts the uniform random number \\(r\\) into a value from the target distribution.\n\nReturn the generated sample\n\nThe computed \\(X\\) follows the desired probability distribution.\n\n\nThis technique works for both discrete and continuous distributions.\n\n\n\nC.4.2 Example of Inverse Transform Sampling with Discrete RV\nInverse Transform Sampling can be applied to discrete random variables (RVs) by mapping uniform random numbers to predefined probability mass functions (PMFs).\n\nC.4.2.1 Step 1: Defining a Discrete Random Variable\nConsider a discrete RV with the following PMF \\(p_{X}(x)\\):\n\n\n\nTable C.4: The probability mass function of discrete random variable \\(X\\)\n\n\n\n\n\n\\(x_i\\)\n\\(p_X(x_i)\\)\n\n\n\n\n1\n0.2\n\n\n2\n0.3\n\n\n3\n0.5\n\n\n\n\n\n\nThe corresponding CDF of \\(X\\) is:\n\n\n\nTable C.5: The cumulative distribution function of discrete random variable \\(X\\)\n\n\n\n\n\nRange\n\\(F_X(x_i)\\)\n\n\n\n\n\\(x &lt; 1\\)\n0.0\n\n\n\\(1 \\leq x &lt; 2\\)\n0.2\n\n\n\\(2 \\leq x &lt; 3\\)\n0.5\n\n\n\\(3 \\leq x\\)\n1.0\n\n\n\n\n\n\n\n\nC.4.2.2 Step 2: Applying Inverse Transform Sampling\n\nGenerate a uniform random number \\(r \\sim \\mathrm{Uniform(0,1)}\\)\nCompute the inverse CDF\n\nFind the smallest \\(X\\) where $ F_{X}(x)$.\nExample:\n\nIf \\(r=0.15\\), we select \\(X=1\\) because \\(0.15&lt;0.2\\).\nIf \\(r=0.80\\), we select \\(X=3\\) because \\(0.5 \\leq 0.80\\).\nIf \\(r=0.35\\), we select \\(X=2\\) because \\(0.2 \\leq 0.35 &lt; 0.5\\).\n\n\nReturn the generated sample\n\nTake the selected values for \\(X\\) as the generated sample that follows the desired probability distribution.\nExample:\n\nThe sampled \\(X\\) values are \\(1\\) ,\\(3\\), and \\(2\\).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo generate random numbers in a specific format, some additional steps (e.g., transformations) can be consider in the procedure above.\nFor example, the random whole numbers between 0 and 99 can be obtained by\n\ngenerating the random number between 0 and 1 (e.g., 0.1575),\nmultiplying it by 100 (e.g., 15.75),\nand rounding off the decimal places (e.g., 15).\n\n\n\n\n\n\nC.4.3 Example of Inverse Transform Sampling with Continuous RV\nInverse Transform Sampling is also applicable to continuous RVs. Instead of mapping discrete values, we use the inverse cumulative distribution function (CDF) to generate continuous random samples.\n\nC.4.3.1 Step 1: Defining a Continuous Random Variable\nConsider a continuous RV with a probability density function (PDF) \\(f_{X}(x)\\):\n\\[\nf_{X}(x)=\n\\begin{cases}\n  \\frac{x}{8}, & 0 \\leq x &lt; 2 \\\\\n  0, & \\mathrm{otherwise}\n\\end{cases}      \n\\]\nThe corresponding cumulative distribution function (CDF) is:\n\\[\nF_{X}(x)=\n\\begin{cases}\n  0, & x &lt; 0 \\\\\n  \\frac{x^2}{16} & 0 \\leq x &lt; 4 \\\\\n  1 & x \\geq 4\n\\end{cases}      \n\\]\n\n\n\nC.4.4 Step 2: Applying Inverse Transform Sampling\n\nGenerate a uniform random number \\(r \\sim \\mathrm{Uniform(0,1)}\\)\nCompute the inverse CDF\n\nSolve for \\(X\\) using the inverse CDF \\[\nX = F_{X}^{-1}(r) = \\sqrt{16r}.\n\\]\nExample:\n\nIf \\(r=0.10\\), we select \\(X=1.26\\) because \\(0.15&lt;0.2\\).\nIf \\(r=0.90\\), we select \\(X=3.79\\) because \\(0.5 \\leq 0.80\\).\nIf \\(r=0.50\\), we select \\(X=2.83\\) because \\(0.2 \\leq 0.35 &lt; 0.5\\).\n\n\nReturn the generated sample\n\nTake the selected values for \\(X\\) as the generated sample that follows the desired probability distribution.\nExample:\n\nThe sampled \\(X\\) values are \\(1.26\\) ,\\(3.79\\), and \\(2.83\\).\n\n\n\nThis random sample is used to estimate the true demand distribution, forming the basis for the Monte Carlo Simulation process.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Random Sample</span>"
    ]
  },
  {
    "objectID": "App/App-ProbDist.html",
    "href": "App/App-ProbDist.html",
    "title": "Appendix D — Probability Distribution",
    "section": "",
    "text": "D.1 Overview\nThis section is designed to introduce you to the fundamental concepts of modeling and simulation for solving a real-world problem.\nSimulation is an imitative representation of a process or system that could exist in the real world.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "App/App-ProbDist.html#what-is-system",
    "href": "App/App-ProbDist.html#what-is-system",
    "title": "Appendix D — Probability Distribution",
    "section": "F.1 What is System?",
    "text": "F.1 What is System?\nA system is a group of interacting or interrelated elements that act together as a unified whole. Here, a system operates under a set of rules, meaning that its components follow specific interactions and behaviors.\nIn the real world, systems are all around us, and they exist in various forms:\n\nNatural systems: like the ecosystem, where plants, animals, and climate interact to maintain balance.\nTechnical systems: such as an automobile, which consists of an engine, transmission, and braking system working together.\nBusiness systems: like a supply chain, where suppliers, manufacturers, warehouses, and logistics collaborate to deliver goods to customers.\n\n\n\n\n\n\n\nHow a system can be defined?\n\n\n\nEach system is defined by having its own:\n\nPurpose – the objective or function of the system.\nBoundaries – defining what is inside and what is outside the system.\nStructure – specifying how components are arranged and interact (i.e., how the processes or operations in the system are performed with the components).\n\nFor example, a supply chain system includes multiple stakeholders—raw material suppliers, factories, warehouses, transportation services, and retailers—working together to ensure products reach customers efficiently. External factors such as market demand, government regulations, and technological advancements influence how the system operates.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "App/App-ProbDist.html#system-analysis-in-real-world",
    "href": "App/App-ProbDist.html#system-analysis-in-real-world",
    "title": "Appendix D — Probability Distribution",
    "section": "F.2 System Analysis in Real world",
    "text": "F.2 System Analysis in Real world\nWhen we want to analyze a system, our primary goal is usually to measure, improve, design, or control its characteristics and behaviors. The most direct way to do this is by studying the real system itself.\nHowever, in many cases, working directly with the actual system is either impractical or impossible. There are reasons why directly studying a real system can be challenging:\n\nDisruptive or Expensive: Intervening in a working system could cause major disruptions.\n\nExample: testing different traffic management strategies on a city’s roads could lead to congestion and inconvenience for commuters.\n\nDangerous: Experimenting with a real system could pose safety risks.\n\nExample: a nuclear power plant’s cooling system in real life could pose safety risks. Similarly, testing new medical treatments directly on patients without prior simulation could have serious health consequences.\n\nNonexistent Systems: In some cases, the system we want to study doesn’t yet exist.\n\nExample: If a company wants to build a new airport, there is no real system to test before construction. Instead, simulations are used to predict how passenger flows, security checks, and baggage handling will function.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "GS/GS-Front.html",
    "href": "GS/GS-Front.html",
    "title": "Getting Started",
    "section": "",
    "text": "Overview\nThis chapter gets you ready to start the journey in this course",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "GS/GS-Front.html#overview",
    "href": "GS/GS-Front.html#overview",
    "title": "Getting Started",
    "section": "",
    "text": "Installation\n\n(Optional) Anaconda\nPython\nJupiter Notebook\n\nRunning software\n\nAssociation Rules\nRecommendation Systems",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "S/S-Front.html",
    "href": "S/S-Front.html",
    "title": "Supervised Data Mining",
    "section": "",
    "text": "Overview\nThis chapter explores unsupervised data mining methods",
    "crumbs": [
      "Supervised Data Mining"
    ]
  },
  {
    "objectID": "S/S-Front.html#overview",
    "href": "S/S-Front.html#overview",
    "title": "Supervised Data Mining",
    "section": "",
    "text": "Classification\n\nPartial Clustering\nHierarchical Clustering\nDensity-based Clustering\n\nRegression\n\nLinear Models\nNonlinear Models\nModels with Regularization\nGeneralized Linear Models\nSupport Vector Regression\n\nPattern Mining\n\nAssociation Rules\nRecommendation Systems\n\nFeature Engineering\n\nFeature Extraction\nFeature Selection (Filter Methods)",
    "crumbs": [
      "Supervised Data Mining"
    ]
  },
  {
    "objectID": "U/U-Front.html",
    "href": "U/U-Front.html",
    "title": "Unsupervised Methods",
    "section": "",
    "text": "Overview\nThis chapter explores unsupervised data mining methods",
    "crumbs": [
      "Unsupervised Methods"
    ]
  },
  {
    "objectID": "U/U-Front.html#overview",
    "href": "U/U-Front.html#overview",
    "title": "Unsupervised Methods",
    "section": "",
    "text": "Pattern Mining\n\nAssociation Rules\nRecommendation Systems\n\nClustering\n\nClustering Methods\nClustering Validity\n\nAnomaly Detection",
    "crumbs": [
      "Unsupervised Methods"
    ]
  },
  {
    "objectID": "U/U-Front.html#related-reading",
    "href": "U/U-Front.html#related-reading",
    "title": "Unsupervised Methods",
    "section": "Related Reading",
    "text": "Related Reading\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Unsupervised Methods"
    ]
  },
  {
    "objectID": "U/U-Front.html#packages",
    "href": "U/U-Front.html#packages",
    "title": "Unsupervised Data Mining",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used for this section. Please install them to avoid potential interruptions/errors.\n\nreadxl, writexl, and readr for importing/exporting data\ntibble for tibble data object of tidyverse\n\n\n#import numpy as np\n#import pandas as pd\n1+1\n\n2",
    "crumbs": [
      "Unsupervised Data Mining"
    ]
  },
  {
    "objectID": "DV/DV-Front.html",
    "href": "DV/DV-Front.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Overview\nThis chapter explores data visualization methods",
    "crumbs": [
      "Data Visualization"
    ]
  },
  {
    "objectID": "DV/DV-Front.html#overview",
    "href": "DV/DV-Front.html#overview",
    "title": "Data Visualization",
    "section": "",
    "text": "Narrative\nTable\nDashboard\nAnimation",
    "crumbs": [
      "Data Visualization"
    ]
  },
  {
    "objectID": "DV/DV-Front.html#related-reading",
    "href": "DV/DV-Front.html#related-reading",
    "title": "Data Visualization",
    "section": "Related Reading",
    "text": "Related Reading\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Data Visualization"
    ]
  },
  {
    "objectID": "DV/DV-Front.html#packages",
    "href": "DV/DV-Front.html#packages",
    "title": "Data Visualization",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used for this section. Please install them to avoid potential interruptions/errors.\n\nreadxl, writexl, and readr for importing/exporting data\ntibble for tibble data object of tidyverse\n\n\n#import numpy as np\n#import pandas as pd\n1+1\n\n2",
    "crumbs": [
      "Data Visualization"
    ]
  },
  {
    "objectID": "BP/BP-Front.html",
    "href": "BP/BP-Front.html",
    "title": "Basics in Python Programming",
    "section": "",
    "text": "Overview\nThis chapter introduces basics in programmings with Python.\nThis chapter introduces the basics of objects, types, operations, conditions, loops, functions, and imports.",
    "crumbs": [
      "Basics in Python Programming"
    ]
  },
  {
    "objectID": "BP/BP-Front.html#overview",
    "href": "BP/BP-Front.html#overview",
    "title": "Basics in Python Programming",
    "section": "",
    "text": "Functions\nLogics\nSequence and dictionaries\nInterations\nBP-Misc.qmd",
    "crumbs": [
      "Basics in Python Programming"
    ]
  },
  {
    "objectID": "BP/BP-Front.html#related-reading",
    "href": "BP/BP-Front.html#related-reading",
    "title": "Basics in Python Programming",
    "section": "Related Reading",
    "text": "Related Reading\nThis chapter has benefited from the excellent Python Programming for Data Science book by Tomas Beuzen.\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Basics in Python Programming"
    ]
  },
  {
    "objectID": "BP/BP-Front.html#packages",
    "href": "BP/BP-Front.html#packages",
    "title": "Basics in Python Programming",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used for this section. Please install them to avoid potential interruptions/errors.\n\nreadxl, writexl, and readr for importing/exporting data\ntibble for tibble data object of tidyverse\n\n\n#import numpy as np\n#import pandas as pd\n1+1\n\n2",
    "crumbs": [
      "Basics in Python Programming"
    ]
  },
  {
    "objectID": "U/UC-Part.html",
    "href": "U/UC-Part.html",
    "title": "11  K-means",
    "section": "",
    "text": "11.1 Overview\nCluster analysis (or clustering) is the task of grouping a set of data objects in such a way that objects in the same group (cluster) are more similar to each other than to those in other groups (clusters), and aims to directly learns the structure of the data without external information (e.g., target labels) as a unsupervised data mining task. K-means clustering is a partition based clustering technique.\n“This section introduces K-means clustering as a partition based clustering technique. The implemention of k-means is discussed with general scatter plots and arranged plots for efficient visualization. Examples in this section demostrate the effect of random initialization on clustering outcomes, the iterative process of K-means, and the clustering for new data.”",
    "crumbs": [
      "Unsupervised Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>K-means</span>"
    ]
  },
  {
    "objectID": "U/UC-Part.html#initialization",
    "href": "U/UC-Part.html#initialization",
    "title": "8  K-means",
    "section": "11.1 Initialization",
    "text": "11.1 Initialization\nBy default, the initial centers are randomly selected in kmean(), and so the clustering results may differ every run. For example, the following is for the results from running kmeans() with the identical data and setting four times.\n\nres_km1 &lt;- kmeans(dat, centers=K)  \nres_km2 &lt;- kmeans(dat, centers=K)  \nres_km3 &lt;- kmeans(dat, centers=K)  \nres_km4 &lt;- kmeans(dat, centers=K)  \n\nTo compare the results, the centers and/or the cluster memberships of data points can be investigated although it may not be straight-forward as follows.\n\nres_four &lt;- data.frame(cbind(res_km1$centers, res_km2$centers, res_km3$centers, res_km4$centers))\ncolnames(res_four) &lt;- c('x_res1','y_res1','x_res2','y_res2','x_res3','y_res3','x_res4','y_res4')\nres_four\n\n      x_res1     y_res1     x_res2     y_res2     x_res3     y_res3      x_res4\n1  0.4607268 -1.4912271  0.4607268 -1.4912271 -1.1385941 -0.5559591 -0.58618089\n2  1.4194387  0.4692907 -0.3595425  1.1091151 -0.3595425  1.1091151 -0.06491265\n3 -0.3595425  1.1091151  1.4194387  0.4692907  1.4194387  0.4692907 -0.45317084\n4 -1.1385941 -0.5559591 -1.1385941 -0.5559591  0.4607268 -1.4912271  1.41943868\n      y_res4\n1  1.1492879\n2  1.0568904\n3 -0.9567883\n4  0.4692907\n\n\nAs a possible way for easier comparison, let’s create a single figure that shows the whole story at once from four plots side by side. For this, the patchwork package, which extends ggplot2’s use of + operator, can help us arrange multiple plots in a single frame. As shown in the figure below from the following codes, the clustering results from kmeans() were inconsistent because they were obtained with random initial centers.\n\nlibrary(patchwork)\np1 &lt;- factoextra::fviz_cluster(res_km1, data = dat)\np2 &lt;- factoextra::fviz_cluster(res_km2, data = dat)\np3 &lt;- factoextra::fviz_cluster(res_km3, data = dat)\np4 &lt;- factoextra::fviz_cluster(res_km4, data = dat)\n\np1 + p2 + p3 + p4 \n\n\n\n\n\n\n\n# or, directly \n# factoextra::fviz_cluster(res_km1, data = dat) +\n#   factoextra::fviz_cluster(res_km2, data = dat) +\n#   factoextra::fviz_cluster(res_km3, data = dat) +\n#   factoextra::fviz_cluster(res_km4, data = dat)\n\nFor a better control of clustering process, providing a set of fixed initial centers to kmeans() can be a good option. To include such predefined initial centers, an array (e.g., a matrix and a data.frame) with numerical values can be used, where the number of rows becomes the number of clusters and the number of columns must be the same with the number of columns of data.\n\n# 4-by-2 data.frame for K = 4\ninitial_center &lt;- data.frame(x = c(-1, -0.5,  0.5,  1), \n                             y = c( 1,    1,    1,  1))\nres_km &lt;- kmeans(dat, center = initial_center)\nres_km\n\nK-means clustering with 4 clusters of sizes 20, 23, 15, 17\n\nCluster means:\n           x          y\n1 -1.1385941 -0.5559591\n2 -0.3595425  1.1091151\n3  0.4607268 -1.4912271\n4  1.4194387  0.4692907\n\nClustering vector:\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n\nWithin cluster sum of squares by cluster:\n[1] 2.705477 2.658679 1.082373 3.641276\n (between_SS / total_SS =  93.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nAlternatively, a special initialization method such as k-means++ can be employed2.",
    "crumbs": [
      "Unsupervised Data Mining",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>K-means</span>"
    ]
  },
  {
    "objectID": "U/UC-Part.html#iterations",
    "href": "U/UC-Part.html#iterations",
    "title": "8  K-means",
    "section": "11.2 Iterations",
    "text": "11.2 Iterations\nThe maximum number of the iterations in kmeans() can be set with the argument iter.max (the default is 10).\nIf the iterations are not sufficient to have the clustering converged at the end of the process, a warning message will appear:\n\nres_iter1 &lt;- kmeans(\n  dat, \n  center = initial_center,\n  iter.max = 1)               # max number of assignment-update iterations\n\nWarning: did not converge in 1 iteration\n\nres_iter1\n\nK-means clustering with 4 clusters of sizes 20, 23, 15, 17\n\nCluster means:\n           x          y\n1 -1.1385941 -0.5559591\n2 -0.3595425  1.1091151\n3  0.4607268 -1.4912271\n4  1.4194387  0.4692907\n\nClustering vector:\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n\nWithin cluster sum of squares by cluster:\n[1] 2.705477 2.658679 1.082373 3.641276\n (between_SS / total_SS =  93.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \nWarning: did *not* converge in specified number of iterations\n\n\nWith the maximum, the kmeans() returns only3 the result after terminating the iterations. For the purpose of tracing the progress, the iteration process can be performed one by one. That is, by taking the centers of the previous iteration as the initial centers for the next iteration, we can get a result from every step. The following shows an example with a bit more complicated data4 .\n#{r} dat_agg &lt;- read.csv(\"aggregation2.csv\") dat_agg &lt;- apply(dat_agg,2,scale) res_iter1 &lt;- kmeans(dat_agg, center = 8, iter.max = 1) res_iter2 &lt;- kmeans(dat_agg, center = res_iter1$centers, iter.max = 1) res_iter3 &lt;- kmeans(dat_agg, center = res_iter2$centers, iter.max = 1) res_iter4 &lt;- kmeans(dat_agg, center = res_iter3$centers, iter.max = 1)\nThen, the progress can be visualized using the clustering plots: #```{r} dat_iter1 &lt;- data.frame(dat_agg, cluster=res_iter1\\(cluster)\npi1 &lt;- ggplot(dat_iter1, aes(x=x, y=y)) +\n  labs(title=\"Iter 1\") +\n  geom_point(aes(color=cluster)) +\n  geom_point(data=res_iter1\\)centers, aes(x=x, y=y,), shape=18, size=3, color=‘red’)\ndat_iter2 &lt;- data.frame(dat_agg, cluster=res_iter2\\(cluster)\npi2 &lt;- ggplot(dat_iter2, aes(x=x, y=y)) +\n  labs(title=\"Iter 2\") +\n  geom_point(aes(color = cluster)) +\n  geom_point(data=res_iter2\\)centers, aes(x=x, y=y), shape=18, size=3, color=‘red’)\ndat_iter3 &lt;- data.frame(dat_agg, cluster=res_iter3\\(cluster)\npi3 &lt;- ggplot(dat_iter3, aes(x=x, y=y)) +\n  labs(title=\"Iter 3\") +\n  geom_point(aes(color = cluster)) +\n  geom_point(data=res_iter3\\)centers, aes(x=x, y=y), shape=18, size=3, color=‘red’)\ndat_iter4 &lt;- data.frame(dat_agg, cluster=res_iter4\\(cluster)\npi4 &lt;- ggplot(dat_iter4, aes(x=x, y=y)) +\n  labs(title=\"Iter 4\") +\n  geom_point(aes(color = cluster)) +\n  geom_point(data=res_iter4\\)centers, aes(x=x, y=y), shape=18, size=3, color=‘red’)\npi1 + pi2 + pi3 + pi4\n\n## Assignment for New Data\n\nFor new data that were not used for training a clustering model with `kmeans()`, the cluster membership can be easily determined based on the distance between the new data and the obtained centers.\n\nFor example, calculate the distance between a new data `dat_new` and each center:\n#```{r}\ndat_new &lt;- data.frame(x=0.77, y=0.33)\ndist_dat_new &lt;- proxy::dist(dat_new, res_km$centers)\ndist_dat_new\nThen, assign the cluster of the shortest distance to dat_new: #{r} which( dist_dat_new == min(dist_dat_new) ) Here, which() function returns the position or the index of the value that satisfies the given condition5.",
    "crumbs": [
      "Unsupervised Data Mining",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>K-means</span>"
    ]
  },
  {
    "objectID": "U/UC-Part.html#k-means-clustering",
    "href": "U/UC-Part.html#k-means-clustering",
    "title": "11  K-means",
    "section": "11.3 K-means Clustering",
    "text": "11.3 K-means Clustering\nk-means is one of the most widely used unsupervised machine learning algorithms.\n\nIn Python, it is implemented in scikit-learn via the KMeans class.\nTo use the algorithm, you need to specify two main inputs:\n\nThe dataset to be clustered, and\nThe number of clusters (centers), K.\n\nLet’s run K-means on the processed dataset with K = 4 clusters:\n\nfrom sklearn.cluster import KMeans\n\ndat = df.to_numpy()\n\n# Set number of clusters\nK = 4\nkmeans_model = KMeans(n_clusters=K, random_state=2024)\nres_km = kmeans_model.fit(dat)\nres_km\n\nC:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning:\n\nCould not find the number of physical cores for the following reason:\n[WinError 2] The system cannot find the file specified\nReturning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n\n  File \"C:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n    cpu_info = subprocess.run(\n               ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\subprocess.py\", line 548, in run\n    with Popen(*popenargs, **kwargs) as process:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\subprocess.py\", line 1026, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"C:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\subprocess.py\", line 1538, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nC:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\n\n\nKMeans(n_clusters=4, random_state=2024)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeans?Documentation for KMeansiFittedKMeans(n_clusters=4, random_state=2024) \n\n\n\n\n\n\n\n\nNote\n\n\n\nHow to turn off the warning\n\n\n\n11.3.1 Cluster Summary Information\nThe KMeans object in scikit-learn stores several useful attributes after fitting.\nThe k-means clustering algorithm assigns the first three users to one cluster and the last three users to the second cluster. The results are consistent with our expectation. We can also display the centroid for each of the two clusters.\n\n# Cluster centers\ncenters = res_km.cluster_centers_\npd.DataFrame(\n    centers,\n    columns=df.columns, #['x-axis', 'y-axis'],\n    index=['cluster'+str(k) for k in range(K)]\n)\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\ncluster0\n4.416525\n2.116662\n\n\ncluster1\n-2.709697\n8.671982\n\n\ncluster2\n-6.898534\n-6.847019\n\n\ncluster3\n-9.037435\n7.349250\n\n\n\n\n\n\n\n\n# Cluster assignment for each data point\nlabels = res_km.labels_\ndf['cluster'] = labels\ndf\n\n\n\n\n\n\n\n\nx\ny\ncluster\n\n\n\n\n0\n-8.545255\n6.609171\n3\n\n\n1\n5.670878\n2.904450\n0\n\n\n2\n-8.316386\n7.620508\n3\n\n\n3\n-3.660191\n9.389984\n1\n\n\n4\n-8.251471\n9.513979\n3\n\n\n...\n...\n...\n...\n\n\n70\n-9.828864\n6.757225\n3\n\n\n71\n2.020134\n2.795072\n0\n\n\n72\n-6.819397\n-4.416867\n2\n\n\n73\n-9.621581\n7.001461\n3\n\n\n74\n-1.770731\n9.185654\n1\n\n\n\n\n75 rows × 3 columns\n\n\n\n\n# Number of points in each cluster\ndf['cluster'].value_counts()\n\ncluster\n0    19\n1    19\n2    19\n3    18\nName: count, dtype: int64\n\n\nWhile scikit-learn doesn’t directly expose totss and betweenss, we can approximate them:\n\n# Total sum of squares (TSS)\noverall_mean = np.mean(dat, axis=0)\ntotss = np.sum((dat - overall_mean) ** 2)\n\n# Within-cluster sum of squares (WSS) = inertia (sum of squared distances to the nearest cluster center)\nwithinss = res_km.inertia_\n\n# Between-cluster sum of squares (BSS)\nbetweenss = totss - withinss\n\n# Print metrics\nprint(f\"Total Sum of Squares (TSS): {totss:.2f}\")\nprint(f\"Total Within-cluster SS (WSS): {withinss:.2f}\")\nprint(f\"Between-cluster SS (BSS): {betweenss:.2f}\")\n\nTotal Sum of Squares (TSS): 4905.07\nTotal Within-cluster SS (WSS): 126.43\nBetween-cluster SS (BSS): 4778.64\n\n\nThis provides a comprehensive summary of the clustering result, similar to what you’d get from R’s kmeans() output.\n\n# Generate and scale data\nX, _ = make_blobs(n_samples=75, centers=4, cluster_std=1.0, random_state=42)\nscaler = StandardScaler()\ndat = scaler.fit_transform(X)\n\n# Run K-means\nK = 4\nres_km = KMeans(n_clusters=K, random_state=2024).fit(dat)\n\n# Prepare data for plotting\nclustered_df = pd.DataFrame(dat, columns=['x', 'y'])\nclustered_df['Cluster'] = res_km.labels_\ncenters = res_km.cluster_centers_\n\n# Visualization similar to fviz_cluster\nplt.figure(figsize=(6, 5))\nsns.scatterplot(data=clustered_df, x='x', y='y', hue='Cluster', palette='Set1', s=60)\nplt.scatter(centers[:, 0], centers[:, 1], c='black', marker='X', s=200, label='Centers')\nplt.title(\"K-means Clustering with Cluster Centers\")\nplt.xlabel(\"x (scaled)\")\nplt.ylabel(\"y (scaled)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nC:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\n\n\n\n\n\n\n\n\n\n\n\n11.3.2 Initialization\n\n11.3.2.1 Initialization with Random Centers\ncheck https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\nBy default, the initial cluster centers in KMeans are selected AUTO @@@@@@@@@@@@@@@@@@@@@@@@@@@@@randomly. As a result, the clustering results can vary across runs with identical data and parameters.\nBelow, we run K-means four times with different random seeds:\n\nfrom sklearn.cluster import KMeans\n\n# Run KMeans multiple times with different random states\n#km1 = KMeans(n_clusters=4, init='random', n_init=1, random_state=1).fit(dat)\nkm1 = KMeans(n_clusters=4, n_init=1).fit(dat)\nkm2 = KMeans(n_clusters=4, n_init=1).fit(dat)\nkm3 = KMeans(n_clusters=4, n_init=1).fit(dat)\n\nC:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\n\n\n(based on random_state)\nTo better illustrate the impact of different initializations, we can visualize all four clustering results side-by-side:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set up plotting\nfig, axes = plt.subplots(1, 3, figsize=(16, 5), sharex=True, sharey=True)\ntitles = [\"Init 1 (RS=1)\", \"Init 2 (RS=2)\", \"Init 3 (RS=3)\"]\nclusterings = [km1, km2, km3]\n\n# Plot each result\nfor ax, km, title in zip(axes, clusterings, titles):\n    sns.scatterplot(x=dat[:, 0], y=dat[:, 1], hue=km.labels_, palette='Set1', s=40, ax=ax, legend=False)\n    ax.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], c='black', marker='X', s=100)\n    ax.set_title(title)\n    ax.set_xlabel(\"x (scaled)\")\n    ax.set_ylabel(\"y (scaled)\")\n    ax.grid(True)\n\nfig.suptitle(\"Effect of Random Initialization on K-means Clustering\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n11.3.2.2 Initialization with Predefined Centers\nTo avoid random variation, a fixed set of initial centers can be supplied. The shape of the array must match the number of clusters and feature dimensions:\n\n# 4-by-2 array of initial centers\ninitial_centers = np.array([\n    [-1.0,  1.0],\n    [-0.5,  1.0],\n    [ 0.5,  1.0],\n    [ 1.0,  1.0]\n])\n\n# Fit K-means using predefined centers\nkm_fixed = KMeans(n_clusters=4, init=initial_centers, n_init=1)\nres_km_fixed = km_fixed.fit(dat)\n\nres_km_fixed.cluster_centers_\n\nC:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\n\n\narray([[-1.0670381 ,  0.7410179 ],\n       [-0.65605317, -1.55240989],\n       [ 0.14882218,  0.95470719],\n       [ 1.51810919, -0.10431426]])\n\n\n\n\n11.3.2.3 Initilization with Algorithm (K-means++)\nAlternatively, a smarter initialization method such as k-means++, which is the default in scikit-learn, can be used to improve stability and clustering quality. Such method determines the initial centers based on its algorithms :\n\n# K-means++ initialization (default)\nkm_plus = KMeans(n_clusters=4, init='k-means++', random_state=2024)\nkm_plus.fit(dat)\n\nC:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\n\n\nKMeans(n_clusters=4, random_state=2024)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeans?Documentation for KMeansiFittedKMeans(n_clusters=4, random_state=2024) \n\n\n\n\n\n11.3.3 Iterations\nThe maximum number of iterations in KMeans can be specified with the max_iter argument. If this limit is too low, the algorithm may terminate before convergence. In practice, tracing each iteration step-by-step provides useful insights into how cluster centers and assignments evolve.\nBelow, we simulate this process using a more complex dataset and perform K-means clustering one iteration at a time.\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n# Generate a clustered dataset (similar to R's aggregation2.csv)\nX_iter, _ = make_blobs(n_samples=300, centers=7, cluster_std=0.60, random_state=0)\nX_iter = StandardScaler().fit_transform(X_iter)\n\n# Run KMeans one iteration at a time\nres_iter1 = KMeans(n_clusters=4, init=initial_centers, n_init=1, max_iter=1, random_state=1).fit(X_iter)\nres_iter2 = KMeans(n_clusters=4, init=res_iter1.cluster_centers_, n_init=1, max_iter=1).fit(X_iter)\nres_iter3 = KMeans(n_clusters=4, init=res_iter2.cluster_centers_, n_init=1, max_iter=1).fit(X_iter)\nres_iter4 = KMeans(n_clusters=4, init=res_iter3.cluster_centers_, n_init=1, max_iter=1).fit(X_iter)\n\nC:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\nC:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\nC:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\nC:\\Users\\saaan\\anaconda3\\envs\\buda_py311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\n\n\nWe now visualize the results from each iteration to observe how the clustering evolves:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Store iteration outputs\nresults = [\n    (X_iter, res_iter1.labels_, res_iter1.cluster_centers_, \"Iter 1\"),\n    (X_iter, res_iter2.labels_, res_iter2.cluster_centers_, \"Iter 2\"),\n    (X_iter, res_iter3.labels_, res_iter3.cluster_centers_, \"Iter 3\"),\n    (X_iter, res_iter4.labels_, res_iter4.cluster_centers_, \"Iter 4\")\n]\n\n# Plot results side by side\nfig, axes = plt.subplots(4, 1, figsize=(5, 20), sharex=True, sharey=True)\n\nfor ax, (X, labels, centers, title) in zip(axes, results):\n    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels, palette='Set2', s=20, ax=ax, legend=False)\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=100)\n    ax.set_title(title, x=-.3, y=.5)\n    ax.set_xlabel(\"x (scaled)\")\n    ax.set_ylabel(\"y (scaled)\")\n    ax.grid(True)\n\n#fig.suptitle(\"K-means Iterative Progression (Step-by-Step)\", fontsize=16)\n#plt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nEach red X marks a cluster center at the current iteration. You can see how both the cluster memberships and center locations change as iterations progress. This provides an intuitive illustration of Lloyd’s algorithm, where K-means alternates between assigning data points to the nearest cluster and updating cluster centers accordingly.\n\n\n\n\n11.3.4 Assign (Predict) clusters for New Data\n\nOnce a K-means clustering model has been trained, new data points can be classified into one of the existing clusters by computing their distance to the learned cluster centers.\nStep 1: Define the New Data Point\n\nimport numpy as np\n\n# New data point to be assigned\ndat_new = np.array([[0.77, 0.33]])\n\nStep 2: Compute Distance to Cluster Centers We calculate the Euclidean distance from the new data point to each of the cluster centers:\n\nfrom sklearn.metrics import pairwise_distances\n\n# Calculate distances from the new point to each center\ndist_dat_new = pairwise_distances(dat_new, res_km.cluster_centers_)\ndist_dat_new\n\narray([[0.86504118, 0.88097728, 2.36158731, 1.88245709]])\n\n\nStep 3: Assign to the Nearest Cluster The new data point is assigned to the cluster whose center is closest to it:\n\n# Identify the index of the nearest center (cluster assignment)\nassigned_cluster = np.argmin(dist_dat_new)  # returns index starting from 0\nassigned_cluster\n\nnp.int64(0)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe cluster index returned represents the cluster number (0-based in Python). This assignment process is useful when applying a clustering model to unseen or incoming data, such as in real-time systems.\n\n\n\n\n11.3.5 How many clusters?\nTo determine the number of clusters in the data, we can apply k-means with varying number of clusters from 1 to 6 and compute their corresponding sum-of-squared errors (SSE) as shown in the example below. The “elbow” in the plot of SSE versus number of clusters can be used to estimate the number of clusters.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnumClusters = [1,2,3,4,5,6]\nSSE = []\nfor k in numClusters:\n    k_means = cluster.KMeans(n_clusters=k)\n    k_means.fit(data)\n    SSE.append(k_means.inertia_)\n\nplt.plot(numClusters, SSE)\nplt.xlabel('Number of Clusters')\nplt.ylabel('SSE')",
    "crumbs": [
      "Unsupervised Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>K-means</span>"
    ]
  },
  {
    "objectID": "U/UC-Part.html#hierarchical-clustering",
    "href": "U/UC-Part.html#hierarchical-clustering",
    "title": "8  K-means",
    "section": "12.2 8.2 Hierarchical Clustering",
    "text": "12.2 8.2 Hierarchical Clustering\nThis section demonstrates examples of applying hierarchical clustering to the vertebrate dataset used in Module 6 (Classification). Specifically, we illustrate the results of using 3 hierarchical clustering algorithms provided by the Python scipy library: (1) single link (MIN), (2) complete link (MAX), and (3) group average. Other hierarchical clustering algorithms provided by the library include centroid-based and Ward’s method.\nimport pandas as pd\n\ndata = pd.read_csv('vertebrate.csv',header='infer')\ndata\n\n\n\n\n\n\n\n\nName\n\n\nWarm-blooded\n\n\nGives Birth\n\n\nAquatic Creature\n\n\nAerial Creature\n\n\nHas Legs\n\n\nHibernates\n\n\nClass\n\n\n\n\n\n\n0\n\n\nhuman\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\nmammals\n\n\n\n\n1\n\n\npython\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\nreptiles\n\n\n\n\n2\n\n\nsalmon\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\nfishes\n\n\n\n\n3\n\n\nwhale\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\nmammals\n\n\n\n\n4\n\n\nfrog\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\namphibians\n\n\n\n\n5\n\n\nkomodo\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\nreptiles\n\n\n\n\n6\n\n\nbat\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\nmammals\n\n\n\n\n7\n\n\npigeon\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\nbirds\n\n\n\n\n8\n\n\ncat\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\nmammals\n\n\n\n\n9\n\n\nleopard shark\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\nfishes\n\n\n\n\n10\n\n\nturtle\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\nreptiles\n\n\n\n\n11\n\n\npenguin\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\nbirds\n\n\n\n\n12\n\n\nporcupine\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\nmammals\n\n\n\n\n13\n\n\neel\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\nfishes\n\n\n\n\n14\n\n\nsalamander\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\namphibians\n\n\n\n\n\n\n12.2.1 8.2.1 Single Link (MIN)\nfrom scipy.cluster import hierarchy\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnames = data['Name']\nY = data['Class']\nX = data.drop(['Name','Class'],axis=1)\nZ = hierarchy.linkage(X.as_matrix(), 'single')\ndn = hierarchy.dendrogram(Z,labels=names.tolist(),orientation='right')\n\n\n\npng\n\n\n\n\n12.2.2 8.2.2 Complete Link (MAX)\nZ = hierarchy.linkage(X.as_matrix(), 'complete')\ndn = hierarchy.dendrogram(Z,labels=names.tolist(),orientation='right')\n\n\n\npng\n\n\n\n\n12.2.3 8.3.3 Group Average\nZ = hierarchy.linkage(X.as_matrix(), 'average')\ndn = hierarchy.dendrogram(Z,labels=names.tolist(),orientation='right')\n\n\n\npng",
    "crumbs": [
      "Unsupervised Data Mining",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>K-means</span>"
    ]
  },
  {
    "objectID": "U/UC-Part.html#density-based-clustering",
    "href": "U/UC-Part.html#density-based-clustering",
    "title": "8  K-means",
    "section": "12.3 8.3 Density-Based Clustering",
    "text": "12.3 8.3 Density-Based Clustering\nDensity-based clustering identifies the individual clusters as high-density regions that are separated by regions of low density. DBScan is one of the most popular density based clustering algorithms. In DBScan, data points are classified into 3 types—core points, border points, and noise points—based on the density of their local neighborhood. The local neighborhood density is defined according to 2 parameters: radius of neighborhood size (eps) and minimum number of points in the neighborhood (min_samples).\nFor this approach, we will use a noisy, 2-dimensional dataset originally created by Karypis et al. [1] for evaluating their proposed CHAMELEON algorithm. The example code shown below will load and plot the distribution of the data.\nimport pandas as pd\n\ndata = pd.read_csv('chameleon.data', delimiter=' ', names=['x','y'])\ndata.plot.scatter(x='x',y='y')\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x1d5b0a1eb00&gt;\n\n\n\npng\n\n\nWe apply the DBScan clustering algorithm on the data by setting the neighborhood radius (eps) to 15.5 and minimum number of points (min_samples) to be 5. The clusters are assigned to IDs between 0 to 8 while the noise points are assigned to a cluster ID equals to -1.\nfrom sklearn.cluster import DBSCAN\n\ndb = DBSCAN(eps=15.5, min_samples=5).fit(data)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = pd.DataFrame(db.labels_,columns=['Cluster ID'])\nresult = pd.concat((data,labels), axis=1)\nresult.plot.scatter(x='x',y='y',c='Cluster ID', colormap='jet')\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x1d5b08dcc50&gt;\n\n\n\npng",
    "crumbs": [
      "Unsupervised Data Mining",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>K-means</span>"
    ]
  },
  {
    "objectID": "U/UC-Part.html#spectral-clustering",
    "href": "U/UC-Part.html#spectral-clustering",
    "title": "8  K-means",
    "section": "12.4 8.4 Spectral Clustering",
    "text": "12.4 8.4 Spectral Clustering\nOne of the main limitations of the k-means clustering algorithm is its tendency to seek for globular-shaped clusters. Thus, it does not work when applied to datasets with arbitrary-shaped clusters or when the cluster centroids overlapped with one another. Spectral clustering can overcome this limitation by exploiting properties of the similarity graph to overcome such limitations. To illustrate this, consider the following two-dimensional datasets.\nimport pandas as pd\n\ndata1 = pd.read_csv('2d_data.txt', delimiter=' ', names=['x','y'])\ndata2 = pd.read_csv('elliptical.txt', delimiter=' ', names=['x','y'])\n\nfig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\ndata1.plot.scatter(x='x',y='y',ax=ax1)\ndata2.plot.scatter(x='x',y='y',ax=ax2)\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x1d5b0be1160&gt;\n\n\n\npng\n\n\nBelow, we demonstrate the results of applying k-means to the datasets (with k=2).\nfrom sklearn import cluster\n\nk_means = cluster.KMeans(n_clusters=2, max_iter=50, random_state=1)\nk_means.fit(data1)\nlabels1 = pd.DataFrame(k_means.labels_,columns=['Cluster ID'])\nresult1 = pd.concat((data1,labels1), axis=1)\n\nk_means2 = cluster.KMeans(n_clusters=2, max_iter=50, random_state=1)\nk_means2.fit(data2)\nlabels2 = pd.DataFrame(k_means2.labels_,columns=['Cluster ID'])\nresult2 = pd.concat((data2,labels2), axis=1)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\nresult1.plot.scatter(x='x',y='y',c='Cluster ID',colormap='jet',ax=ax1)\nax1.set_title('K-means Clustering')\nresult2.plot.scatter(x='x',y='y',c='Cluster ID',colormap='jet',ax=ax2)\nax2.set_title('K-means Clustering')\nText(0.5,1,'K-means Clustering')\n\n\n\npng\n\n\nThe plots above show the poor performance of k-means clustering. Next, we apply spectral clustering to the datasets. Spectral clustering converts the data into a similarity graph and applies the normalized cut graph partitioning algorithm to generate the clusters. In the example below, we use the Gaussian radial basis function as our affinity (similarity) measure. Users need to tune the kernel parameter (gamma) value in order to obtain the appropriate clusters for the given dataset.\nfrom sklearn import cluster\nimport pandas as pd\n\nspectral = cluster.SpectralClustering(n_clusters=2,random_state=1,affinity='rbf',gamma=5000)\nspectral.fit(data1)\nlabels1 = pd.DataFrame(spectral.labels_,columns=['Cluster ID'])\nresult1 = pd.concat((data1,labels1), axis=1)\n\nspectral2 = cluster.SpectralClustering(n_clusters=2,random_state=1,affinity='rbf',gamma=100)\nspectral2.fit(data2)\nlabels2 = pd.DataFrame(spectral2.labels_,columns=['Cluster ID'])\nresult2 = pd.concat((data2,labels2), axis=1)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\nresult1.plot.scatter(x='x',y='y',c='Cluster ID',colormap='jet',ax=ax1)\nax1.set_title('Spectral Clustering')\nresult2.plot.scatter(x='x',y='y',c='Cluster ID',colormap='jet',ax=ax2)\nax2.set_title('Spectral Clustering')\nText(0.5,1,'Spectral Clustering')\n\n\n\npng",
    "crumbs": [
      "Unsupervised Data Mining",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>K-means</span>"
    ]
  },
  {
    "objectID": "U/UC-Part.html#summary",
    "href": "U/UC-Part.html#summary",
    "title": "8  K-means",
    "section": "12.5 8.5 Summary",
    "text": "12.5 8.5 Summary\nThis tutorial illustrates examples of using different Python’s implementation of clustering algorithms. Algorithms such as k-means, spectral clustering, and DBScan are designed to create disjoint partitions of the data whereas the single-link, complete-link, and group average algorithms are designed to generate a hierarchy of cluster partitions.\nReferences: [1] George Karypis, Eui-Hong Han, and Vipin Kumar. CHAMELEON: A Hierarchical Clustering Algorithm Using Dynamic Modeling. IEEE Computer 32(8): 68-75, 1999.",
    "crumbs": [
      "Unsupervised Data Mining",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>K-means</span>"
    ]
  },
  {
    "objectID": "U/UC-Part.html#footnotes",
    "href": "U/UC-Part.html#footnotes",
    "title": "8  K-means",
    "section": "",
    "text": "Check the discussion in Week 03.↩︎\nkmeanspp function of maotai package.↩︎\nIn the case of Lloyd algorithm that was discussed in our class.↩︎\nhttps://ecampus.wvu.edu/bbcswebdav/xid-153354184_1↩︎\nIn this example, whether the value on each position is equal to the minimum (i.e., smallest).↩︎",
    "crumbs": [
      "Unsupervised Data Mining",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>K-means</span>"
    ]
  },
  {
    "objectID": "MD/MD-Front.html#related-reading",
    "href": "MD/MD-Front.html#related-reading",
    "title": "Mining with Dependent Data",
    "section": "Related Reading",
    "text": "Related Reading\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Mining with Dependent Data"
    ]
  },
  {
    "objectID": "MD/MD-Front.html#packages",
    "href": "MD/MD-Front.html#packages",
    "title": "Mining with Dependent Data",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used for this section. Please install them to avoid potential interruptions/errors.\n\nreadxl, writexl, and readr for importing/exporting data\ntibble for tibble data object of tidyverse",
    "crumbs": [
      "Mining with Dependent Data"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Business Data Mining and Visualization",
    "section": "",
    "text": "Python and its packages\nJupiter notebook\nConda/Anaconda",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome Page</span>"
    ]
  },
  {
    "objectID": "U/UC-Part.html#overview",
    "href": "U/UC-Part.html#overview",
    "title": "11  K-means",
    "section": "",
    "text": "11.1.1 Packages for this class\n\ncluster for example data\nggplot and factoextra for visualization of clustering\npatchwork for arranging mulitple plots\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
    "crumbs": [
      "Unsupervised Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>K-means</span>"
    ]
  },
  {
    "objectID": "U/UC-Part.html#data-preparation",
    "href": "U/UC-Part.html#data-preparation",
    "title": "11  K-means",
    "section": "11.2 Data Preparation",
    "text": "11.2 Data Preparation\nFor this section, we will use a toy dataset. This dataset contains 75 observations that form four well-separated clusters, making it ideal for testing clustering algorithms.\nStep 1: Simulate or Load Dataset\n\nfrom sklearn.datasets import make_blobs\n\n# Generate synthetic data similar to R's ruspini\nX, _ = make_blobs(n_samples=75, centers=4, cluster_std=1.0, random_state=42)\ndf = pd.DataFrame(X, columns=['x', 'y'])\n\n# Preview the data\ndf.head()\n\ndf.describe()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\ncount\n75.000000\n75.000000\n\n\nmean\n-3.484217\n2.762365\n\n\nstd\n5.239377\n6.231662\n\n\nmin\n-10.445811\n-8.343625\n\n\n25%\n-7.856466\n-2.215633\n\n\n50%\n-4.234115\n3.537813\n\n\n75%\n0.545074\n7.795279\n\n\nmax\n5.670878\n10.866564\n\n\n\n\n\n\n\nStep 2: Visualize the Dataset We can visualize the two-dimensional data to inspect its structure and confirm the presence of visible clusters.\n\n# Scatter plot of the generated data\nplt.figure(figsize=(6, 5))\nsns.scatterplot(data=df, x='x', y='y', s=50)\nplt.title(\"Scatter Plot of Ruspini-like Data\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n11.2.1 Scale the Data\nFor clustering tasks, standardizing the variables helps prevent those with larger scales from dominating the distance calculations. While the ruspini dataset has roughly balanced variable scales, it’s still a good practice to normalize the data for consistent results.\nFor a good clustering result, it is essential to avoid an adverse effect[^inst2] from the distances calculated with dominating variables of larger scales. It may not be a severe issue for ruspini data, where the value ranges of the variables are not extremely different. Nonetheless, it would be recommendable to consider a normalization to ensure the proper data quality. In this example, let’s consider the standardization..\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize the data\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df)\n\ndf_scaled = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns, index=df.index)\n\n# Summary statistics after scaling\ndf_scaled.describe()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\ncount\n7.500000e+01\n7.500000e+01\n\n\nmean\n1.450691e-16\n-1.421085e-16\n\n\nstd\n1.006734e+00\n1.006734e+00\n\n\nmin\n-1.337654e+00\n-1.794189e+00\n\n\n25%\n-8.401175e-01\n-8.042028e-01\n\n\n50%\n-1.440913e-01\n1.252748e-01\n\n\n75%\n7.742190e-01\n8.130745e-01\n\n\nmax\n1.759130e+00\n1.309245e+00",
    "crumbs": [
      "Unsupervised Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>K-means</span>"
    ]
  },
  {
    "objectID": "U/UC-Part.html#variants",
    "href": "U/UC-Part.html#variants",
    "title": "11  K-means",
    "section": "11.4 Variants",
    "text": "11.4 Variants\n\nRadiusNeighborsClassifier: Based on the number of neighbors within a fixed radius r while KNeighborsClassifier is based on the k nearest neighbors.\nK-medoids Clustering\n\n\n11.4.1 Related Reading/Reference\n\nChapter 14.2, in Business Analytics: communicating with Numbers, 2nd ed. (Jaggia et al., 2023)\nChapter 7.1, 7.2 in Introduction to Data Mining, 2nd ed. (Tan et al., 2019)",
    "crumbs": [
      "Unsupervised Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>K-means</span>"
    ]
  },
  {
    "objectID": "S/S-Front.html#related-reading",
    "href": "S/S-Front.html#related-reading",
    "title": "Supervised Data Mining",
    "section": "Related Reading",
    "text": "Related Reading\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Supervised Data Mining"
    ]
  },
  {
    "objectID": "S/S-Front.html#packages",
    "href": "S/S-Front.html#packages",
    "title": "Supervised Data Mining",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used for this section. Please install them to avoid potential interruptions/errors.\n\nreadxl, writexl, and readr for importing/exporting data\ntibble for tibble data object of tidyverse\n\n\n#import numpy as np\n#import pandas as pd\n1+1\n\n2",
    "crumbs": [
      "Supervised Data Mining"
    ]
  },
  {
    "objectID": "BD/BD-Front.html",
    "href": "BD/BD-Front.html",
    "title": "Basics in Data Science with Python",
    "section": "",
    "text": "Overview\nThis chapter reviews fundamental skills for data science with Python.",
    "crumbs": [
      "Basics in Data Science with Python"
    ]
  },
  {
    "objectID": "BD/BD-Front.html#overview",
    "href": "BD/BD-Front.html#overview",
    "title": "Basics in Data Science with Python",
    "section": "",
    "text": "Data in Python\n\nNumPy\nPandas\n\nData Preprocessing\n\nData Inspection\nData Integration\nData Cleaning\nData Transformation",
    "crumbs": [
      "Basics in Data Science with Python"
    ]
  },
  {
    "objectID": "BD/BD-Front.html#related-reading",
    "href": "BD/BD-Front.html#related-reading",
    "title": "Basics in Data Science with Python",
    "section": "Related Reading",
    "text": "Related Reading\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Basics in Data Science with Python"
    ]
  },
  {
    "objectID": "BD/BD-Front.html#packages",
    "href": "BD/BD-Front.html#packages",
    "title": "Basics in Data Science with Python",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used for this section. Please install them to avoid potential interruptions/errors.\n\nreadxl, writexl, and readr for importing/exporting data\ntibble for tibble data object of tidyverse\n\n\n#import numpy as np\n#import pandas as pd\n1+1\n\n2",
    "crumbs": [
      "Basics in Data Science with Python"
    ]
  },
  {
    "objectID": "DV/DV-IntroVisual.html#sec-intro_dv",
    "href": "DV/DV-IntroVisual.html#sec-intro_dv",
    "title": "7  Introduction to Data Visualization",
    "section": "",
    "text": "7.1.0.1 Related Reading/Reference\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "BD/BD-Inspect.html",
    "href": "BD/BD-Inspect.html",
    "title": "7  Data Inspection",
    "section": "",
    "text": "7.1 Overview\nOften, data cannot be used directly for your analysis. For accurate, meaningful outcomes, data have to be prepared, prior to the application of data mining tools.\n“This material outlines basic methods for preparing data in R, which is essential for analysis. It covers inspecting data, organizing data by subsetting and sorting, integrating multiple datasets, cleaning data with missing elements, and transforming data by applying mathematical operations, binning, and combining categories.”",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Inspection</span>"
    ]
  },
  {
    "objectID": "BD/BD-Inspect.html#insp",
    "href": "BD/BD-Inspect.html#insp",
    "title": "7  Data Inspection",
    "section": "7.3 Inspecting Data",
    "text": "7.3 Inspecting Data\n\n7.3.1 Accessing Contents in DataFrame of pandas\nThe simplest way to check data is to inspect the values directly. For viewing in a Console or a Jupyter Notebook, simply type the object name:\n\ndf \n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\n0\n1\nNeutral\n0.5\n\n\n1\n2\nSatisfied\nNaN\n\n\n2\n3\nDissatisfied\n0.2\n\n\n3\n5\nDissatisfied\n0.1\n\n\n4\n4\nSatisfied\n0.9\n\n\n5\n3\nSatisfied\nNaN\n\n\n\n\n\n\n\nor, print the object 1:\n\nprint(df)\n\n   X1            X2   X3\n0   1       Neutral  0.5\n1   2     Satisfied  NaN\n2   3  Dissatisfied  0.2\n3   5  Dissatisfied  0.1\n4   4     Satisfied  0.9\n5   3     Satisfied  NaN\n\n\n\n\n\n\n\n\nWhy print()?\n\n\n\n\ndef add_one_without(x):\n  x + 1\n  return x + 1\n\nprint(\"Here, without print\")\nans1 = add_one_without(3)\n\ndef add_one_with(x):\n  print(x + 1)\n  return x + 1\n\nprint(\"Here, with print\")\nans2 = add_one_with(3)\n\nHere, without print\nHere, with print\n4\n\n\n\n\nFor neat data management, you can perform many basic operations2 with DataFrame of pandas.\nThe following are some useful methods for inspection of a DataFrame object.\n\ndf.head()           # View the first few rows\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\n0\n1\nNeutral\n0.5\n\n\n1\n2\nSatisfied\nNaN\n\n\n2\n3\nDissatisfied\n0.2\n\n\n3\n5\nDissatisfied\n0.1\n\n\n4\n4\nSatisfied\n0.9\n\n\n\n\n\n\n\n\ndf.info()           # View a summary, including structure and data types\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6 entries, 0 to 5\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   X1      6 non-null      int64  \n 1   X2      6 non-null      object \n 2   X3      4 non-null      float64\ndtypes: float64(1), int64(1), object(1)\nmemory usage: 276.0+ bytes\n\n\n\ndf.shape            # Get (rows, columns)\n\n(6, 3)\n\n\n\ndf.index            # Get row indices\n\nRangeIndex(start=0, stop=6, step=1)\n\n\n\ndf.columns          # Get column names\n\nIndex(['X1', 'X2', 'X3'], dtype='object')\n\n\n\ndf.dtypes           # Get data types\n\nX1      int64\nX2     object\nX3    float64\ndtype: object\n\n\n\ndf.memory_usage(deep=True)  # Detailed memory usage\n\nIndex    132\nX1        48\nX2       400\nX3        48\ndtype: int64\n\n\nLastly, the data of each column can be statistically summarized:\n\ndf.describe()  # Summary statistics including categorical\n\n\n\n\n\n\n\n\nX1\nX3\n\n\n\n\ncount\n6.000000\n4.000000\n\n\nmean\n3.000000\n0.425000\n\n\nstd\n1.414214\n0.359398\n\n\nmin\n1.000000\n0.100000\n\n\n25%\n2.250000\n0.175000\n\n\n50%\n3.000000\n0.350000\n\n\n75%\n3.750000\n0.600000\n\n\nmax\n5.000000\n0.900000\n\n\n\n\n\n\n\n\n7.3.1.1 Renaming Rows/Columns\nIn pandas, row indices are typically numeric by default. You can assign custom labels using dat.index:\n\ndf.index = [f\"rec{i+1}\" for i in range(len(df))]\ndf\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\nrec1\n1\nNeutral\n0.5\n\n\nrec2\n2\nSatisfied\nNaN\n\n\nrec3\n3\nDissatisfied\n0.2\n\n\nrec4\n5\nDissatisfied\n0.1\n\n\nrec5\n4\nSatisfied\n0.9\n\n\nrec6\n3\nSatisfied\nNaN\n\n\n\n\n\n\n\nTo store original index values in a new column before reindexing:\n\ndf_reset = df.copy()\ndf_reset = df_reset.reset_index() # '.reset_index(drop=True)' to not keep the old index\ndf_reset\n\n\n\n\n\n\n\n\nindex\nX1\nX2\nX3\n\n\n\n\n0\nrec1\n1\nNeutral\n0.5\n\n\n1\nrec2\n2\nSatisfied\nNaN\n\n\n2\nrec3\n3\nDissatisfied\n0.2\n\n\n3\nrec4\n5\nDissatisfied\n0.1\n\n\n4\nrec5\n4\nSatisfied\n0.9\n\n\n5\nrec6\n3\nSatisfied\nNaN\n\n\n\n\n\n\n\n\n\n\n7.3.2 Accessing/Modifying Metadata\nWhile pandas limits attributes to its defined structure (e.g., df.shape), you can attach arbitrary metadata3 using the .attrs dictionary:\n\ndf.attrs\n\n{}\n\n\n\ndf.attrs[\"comment\"] = \"This dataset is for a class example in BUDA-450.\"\ndf.attrs[\"custom_id\"] = [12, 23, 34, 45, 54]\ndf.attrs\n\n{'comment': 'This dataset is for a class example in BUDA-450.',\n 'custom_id': [12, 23, 34, 45, 54]}\n\n\n\nThis metadata will not be displayed in print(dat) or dat.info(); it’s purely for internal use.\n\n\n\n7.3.3 Exercise\nTry the above operations on both the original and row-renamed DataFrames. For example:\n\nprint(df.head())\nprint(df.attrs)\ndf.index = [f\"rec{i+1}\" for i in range(len(df))]\nprint(df)\ndf.describe(include=\"all\")\n\n      X1            X2   X3\nrec1   1       Neutral  0.5\nrec2   2     Satisfied  NaN\nrec3   3  Dissatisfied  0.2\nrec4   5  Dissatisfied  0.1\nrec5   4     Satisfied  0.9\n{'comment': 'This dataset is for a class example in BUDA-450.', 'custom_id': [12, 23, 34, 45, 54]}\n      X1            X2   X3\nrec1   1       Neutral  0.5\nrec2   2     Satisfied  NaN\nrec3   3  Dissatisfied  0.2\nrec4   5  Dissatisfied  0.1\nrec5   4     Satisfied  0.9\nrec6   3     Satisfied  NaN\n\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\ncount\n6.000000\n6\n4.000000\n\n\nunique\nNaN\n3\nNaN\n\n\ntop\nNaN\nSatisfied\nNaN\n\n\nfreq\nNaN\n3\nNaN\n\n\nmean\n3.000000\nNaN\n0.425000\n\n\nstd\n1.414214\nNaN\n0.359398\n\n\nmin\n1.000000\nNaN\n0.100000\n\n\n25%\n2.250000\nNaN\n0.175000\n\n\n50%\n3.000000\nNaN\n0.350000\n\n\n75%\n3.750000\nNaN\n0.600000\n\n\nmax\n5.000000\nNaN\n0.900000",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Inspection</span>"
    ]
  },
  {
    "objectID": "BD/BD-Inspect.html#organizing-data",
    "href": "BD/BD-Inspect.html#organizing-data",
    "title": "7  Data Inspection",
    "section": "7.4 Organizing Data",
    "text": "7.4 Organizing Data\n\n7.4.1 Subsetting Data\nIntro to data structure Indexing and Selecting Data\nOften, you may need only a part of large data. With pandas, subsetting data is easy and powerful.\n\n7.4.1.1 Element Selection\nYou can select a single element using iloc (by integer index) or loc (by label):\n\ndf.loc[\"rec3\", \"X2\"]   # by row label and column name\n#df.loc[\"rec3\", 1]      # row name and column index\n\n'Dissatisfied'\n\n\n\ndf.iloc[2, 1]          # 3rd row, 2nd column (indexing starts at 0)\ndf.iloc[2][\"X2\"]       # chaining style\n\n'Dissatisfied'\n\n\nNote: You must first set custom row names to use labels like “rec3”:\n\ndf.loc[\"rec3\"]\n\nX1               3\nX2    Dissatisfied\nX3             0.2\nName: rec3, dtype: object\n\n\n\ndf.index = [f\"rec{i+1}\" for i in range(len(df))]\n\n\n\n7.4.1.2 Entire Column/Row Selection\nTo select an entire column, we can include:\n\ndf.loc[:,\"X2\"]       # 2nd column by index\n\nrec1         Neutral\nrec2       Satisfied\nrec3    Dissatisfied\nrec4    Dissatisfied\nrec5       Satisfied\nrec6       Satisfied\nName: X2, dtype: object\n\n\n\ndf.iloc[:, 1]       # 2nd column by index\n\nrec1         Neutral\nrec2       Satisfied\nrec3    Dissatisfied\nrec4    Dissatisfied\nrec5       Satisfied\nrec6       Satisfied\nName: X2, dtype: object\n\n\nAlternatively,\n\ndf[\"X2\"]            # by column name\ndf.X2               # attribute-style access (if no space or special char in column name)\n\nrec1         Neutral\nrec2       Satisfied\nrec3    Dissatisfied\nrec4    Dissatisfied\nrec5       Satisfied\nrec6       Satisfied\nName: X2, dtype: object\n\n\nTo select an entire row:\n\ndf.iloc[1, :]       # 2nd row by index\ndf.loc[\"rec2\", :]   # by row name\n\nX1            2\nX2    Satisfied\nX3          NaN\nName: rec2, dtype: object\n\n\n\n\n7.4.1.3 Multiple Selection\nTo select multiple columns/rows, a list of labels or indices can be used\n\ndf.iloc[:, [0, 2]]         # 1st and 3rd columns\ndf.iloc[[1, 4], :]         # 2nd and 5th rows\n#df.iloc[[1, 4], [0, 2]]    # intersection of those rows and columns\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\nrec2\n2\nSatisfied\nNaN\n\n\nrec5\n4\nSatisfied\n0.9\n\n\n\n\n\n\n\n\ndf.loc[:, ['X1', 'X3']]    # 1st and 3rd columns\ndf.loc[['rec2', 'rec5'], :]         # 2nd and 5th rows\n#dat.iloc[[1, 4], [0, 2]]    # intersection of those rows and columns\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\nrec2\n2\nSatisfied\nNaN\n\n\nrec5\n4\nSatisfied\n0.9\n\n\n\n\n\n\n\n\n\n7.4.1.4 Conditional Selection\nYou can use a logical condition, which shows whether a condition for each element is met, to filter rows.\nFor example, df[\"X1\"] &gt; 3 returns a Boolean Series of logical test results for all the elements of the Series df[\"X1\"] :\n\ncondition = df[\"X1\"] &gt; 3\nprint(condition)\n\nrec1    False\nrec2    False\nrec3    False\nrec4     True\nrec5     True\nrec6    False\nName: X1, dtype: bool\n\n\nUse this Boolean mask to filter rows:\n\n# with saved condition\ndf[condition]  # or directly 'df[df[\"X1\"] &gt; 3]'\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\nrec4\n5\nDissatisfied\n0.1\n\n\nrec5\n4\nSatisfied\n0.9\n\n\n\n\n\n\n\nFor the logical operation, review the materials in BP-Logics.\n\n\n7.4.1.5 Assigning New Values to Selection\nYou can modify selected data by assigning new values, where the shape of the selected object (i.e., the left-hand side of the statement) is the same with the size of the shape of the new values (i.e., the right-hand side).\n\ndf.iloc[[1], [0, 2]] = [-100, -200] # overwrite the 1st and 3rd element of 2nd row with [-100, -200]\nprint(df)\n\n       X1            X2     X3\nrec1    1       Neutral    0.5\nrec2 -100     Satisfied -200.0\nrec3    3  Dissatisfied    0.2\nrec4    5  Dissatisfied    0.1\nrec5    4     Satisfied    0.9\nrec6    3     Satisfied    NaN\n\n\n\n\n\n7.4.2 Sorting Data\nData can be examined by simply checking the extreme (e.g., largest or smallest) values after sorting the values.\n\n7.4.2.1 Sorting DataFrame\nTo sort rows in a DataFrame based on values of a column, use .sort_values():\n\ndf.sort_values(by=\"X1\")                                   # by a Single Column in Ascending)\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\nrec2\n-100\nSatisfied\n-200.0\n\n\nrec1\n1\nNeutral\n0.5\n\n\nrec3\n3\nDissatisfied\n0.2\n\n\nrec6\n3\nSatisfied\nNaN\n\n\nrec5\n4\nSatisfied\n0.9\n\n\nrec4\n5\nDissatisfied\n0.1\n\n\n\n\n\n\n\n\ndf.sort_values(by=[\"X1\", \"X3\"])                           # by Multiple Columns (Ascending)\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\nrec2\n-100\nSatisfied\n-200.0\n\n\nrec1\n1\nNeutral\n0.5\n\n\nrec3\n3\nDissatisfied\n0.2\n\n\nrec6\n3\nSatisfied\nNaN\n\n\nrec5\n4\nSatisfied\n0.9\n\n\nrec4\n5\nDissatisfied\n0.1\n\n\n\n\n\n\n\n\ndf.sort_values(by=\"X1\", ascending=False)                 # by a Column (Descending)\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\nrec4\n5\nDissatisfied\n0.1\n\n\nrec5\n4\nSatisfied\n0.9\n\n\nrec3\n3\nDissatisfied\n0.2\n\n\nrec6\n3\nSatisfied\nNaN\n\n\nrec1\n1\nNeutral\n0.5\n\n\nrec2\n-100\nSatisfied\n-200.0\n\n\n\n\n\n\n\n\ndf.sort_values(by=[\"X1\", \"X3\"], ascending=[True, False]) # by Multiple Columns with Mixed Order\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\nrec2\n-100\nSatisfied\n-200.0\n\n\nrec1\n1\nNeutral\n0.5\n\n\nrec3\n3\nDissatisfied\n0.2\n\n\nrec6\n3\nSatisfied\nNaN\n\n\nrec5\n4\nSatisfied\n0.9\n\n\nrec4\n5\nDissatisfied\n0.1\n\n\n\n\n\n\n\n\n\n\n7.4.3 Ranking Values\nTo rank values, use .rank():\n\n# Add a new column with dense ranks of X1\ndf[\"X1\"].rank()\n\nrec1    2.0\nrec2    1.0\nrec3    3.5\nrec4    6.0\nrec5    5.0\nrec6    3.5\nName: X1, dtype: float64",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Inspection</span>"
    ]
  },
  {
    "objectID": "BD/BD-Inspect.html#footnotes",
    "href": "BD/BD-Inspect.html#footnotes",
    "title": "7  Data Inspection",
    "section": "",
    "text": "The display may differ, depending on the type of the object.↩︎\nDataFrame - pandas↩︎\nMetadata that provides information for other data.↩︎",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Inspection</span>"
    ]
  },
  {
    "objectID": "BD/BD-Integrate.html",
    "href": "BD/BD-Integrate.html",
    "title": "7  Data Integration",
    "section": "",
    "text": "7.1 Overview\nOften, data cannot be used directly for your analysis. For accurate, meaningful outcomes, data have to be prepared, prior to the application of data mining tools.\n“This material outlines basic methods for preparing data in R, which is essential for analysis. It covers inspecting data, organizing data by subsetting and sorting, integrating multiple datasets, cleaning data with missing elements, and transforming data by applying mathematical operations, binning, and combining categories.”",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Integration</span>"
    ]
  },
  {
    "objectID": "BD/BD-Integrate.html#data",
    "href": "BD/BD-Integrate.html#data",
    "title": "6  Data Integration",
    "section": "6.2 Data",
    "text": "6.2 Data\nLet’s consider two datasets, each of which has different attributes.\n\ndf1 = pd.DataFrame(\n  {\n    \"X1\": [1, 2, 4, 4, 5],\n    \"X2\": [\"Very Satisfied\", \"Satisfied\", \"Neutral\", \"Dissatisfied\", \"Very Dissatisfied\"],\n    \"X3\": [0.5, 0.8, 0.8, 0.2, None]  # last value is missing\n  }, \n  index = [\"rec1\", \"rec2\", \"rec3\", \"rec4\", \"rec5\"]\n)\n\ndf2 = pd.DataFrame(\n  {\n    \"X4\": [50, 40, 30, 20, 10],\n    \"X5\": [True, False, False, False, True]\n  },\n  index = [\"rec1\", \"rec2\", \"rec3\", \"rec5\", \"rec6\"]\n  \n)\n\nNote that the data df1 and df2 have some records for the common objects (i.e., rec1, rec2, rec3, rec5) but different attributes.",
    "crumbs": [
      "Data with Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Integration</span>"
    ]
  },
  {
    "objectID": "BD/BD-Integrate.html#integrating-data",
    "href": "BD/BD-Integrate.html#integrating-data",
    "title": "7  Data Integration",
    "section": "7.3 Integrating Data",
    "text": "7.3 Integrating Data\nIntegrating data from different sources can yield more comprehensive and reliable analysis, providing deeper insights and more reliable results.\n\n7.3.1 Column-wise Integration\n\n7.3.1.1 Concatenation\nThe DataFrame objects can be concatenated with axis=1, horizontally stacking columns:\n\npd.concat([df1, df2], axis=1)\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\n\n\n\n\nrec1\n1.0\nVery Satisfied\n0.5\n50.0\nTrue\n\n\nrec2\n2.0\nSatisfied\n0.8\n40.0\nFalse\n\n\nrec3\n4.0\nNeutral\n0.8\n30.0\nFalse\n\n\nrec4\n4.0\nDissatisfied\n0.2\nNaN\nNaN\n\n\nrec5\n5.0\nVery Dissatisfied\nNaN\n20.0\nFalse\n\n\nrec6\nNaN\nNaN\nNaN\n10.0\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndices in concat\n\n\n\nIf the indices are arbitrarily set (e.g., from 0 by default), the concatenation can mislead the link between the data objects.\nLet’s consider reset indices and their concatenation:\n\ndf1_reset = df1.copy().reset_index(names=\"rowname1\")\ndf2_reset = df2.copy().reset_index(names=\"rowname2\")\npd.concat([df1_reset, df2_reset], axis=1)\n\n\n\n\n\n\n\n\nrowname1\nX1\nX2\nX3\nrowname2\nX4\nX5\n\n\n\n\n0\nrec1\n1\nVery Satisfied\n0.5\nrec1\n50\nTrue\n\n\n1\nrec2\n2\nSatisfied\n0.8\nrec2\n40\nFalse\n\n\n2\nrec3\n4\nNeutral\n0.8\nrec3\n30\nFalse\n\n\n3\nrec4\n4\nDissatisfied\n0.2\nrec5\n20\nFalse\n\n\n4\nrec5\n5\nVery Dissatisfied\nNaN\nrec6\n10\nTrue\n\n\n\n\n\n\n\nAs you may notice, the result shows the concatenated data objects, regardless of the row names. Therefore, if records on the same row (e.g., rec4 in df1 and rec5 in df2) are for different objects, then the combined data will contain misleading information.\nIt’s important to check whether the indices of different DataFrame objects imply the same index system, before integrating them.\n\n\n\n\n7.3.1.2 Merging (Joins)\nThe join1 is an operation for the systematic integration of two DataFrames of different objects (i.e., different indices), avoiding potential issues such as data integrity.\nFor a join of two DataFrames (say, df1 and df2 as left and right data tables), you need to specify\n\nThe key (e.g., a column) in each DataFrame, whose values can be used to identify each object uniquely\nJoin type\n\nInner Join: Returns only the matching rows between two DataFrames.\n\n\nThis method is useful when you need only the common data present in both data.frames.\n\n\nFull Outer Join: Returns all rows from both DataFrames, filling with NaN when there are no matches.\n\n\nThis method ensures that no data is lost from either DataFrames.\n\n\nLeft Outer Join: Returns all rows from the left DataFrame and matched rows from the right.\n\n\nThis can be useful when you want to retain all information from the primary (left) dataset.\n\n\nRight Outer Join: Returns all rows from the right DataFrame and matched rows from the left.\n\n\nThis can be when the secondary dataset (dat2) is more comprehensive.\n\n\nYou can make a join of two DataFrames, using merge() with:\n\nthe keys\n\non = common_column_name, when both DataFrames have the key of the same name common_name\nleft_on = left_table_column_name and right_table_column_name for the left and right data tables, respectively, when the DataFrames have the key of the different names:\nleft_index=True and right_index=True, based on the indices of the DataFrames\n\nthe join type\n\nhow = inner for inner join\nhow = outer for full outer join\nhow = left for left outer join\nhow = right for right outer join\n\n\nThe following show some examples of joins:\n\npd.merge(df1_reset, df2_reset, left_on=\"rowname1\", right_on=\"rowname2\", how=\"inner\")\n\n\n\n\n\n\n\n\nrowname1\nX1\nX2\nX3\nrowname2\nX4\nX5\n\n\n\n\n0\nrec1\n1\nVery Satisfied\n0.5\nrec1\n50\nTrue\n\n\n1\nrec2\n2\nSatisfied\n0.8\nrec2\n40\nFalse\n\n\n2\nrec3\n4\nNeutral\n0.8\nrec3\n30\nFalse\n\n\n3\nrec5\n5\nVery Dissatisfied\nNaN\nrec5\n20\nFalse\n\n\n\n\n\n\n\n\npd.merge(df1, df2, left_index=True, right_index=True, how=\"outer\")\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\n\n\n\n\nrec1\n1.0\nVery Satisfied\n0.5\n50.0\nTrue\n\n\nrec2\n2.0\nSatisfied\n0.8\n40.0\nFalse\n\n\nrec3\n4.0\nNeutral\n0.8\n30.0\nFalse\n\n\nrec4\n4.0\nDissatisfied\n0.2\nNaN\nNaN\n\n\nrec5\n5.0\nVery Dissatisfied\nNaN\n20.0\nFalse\n\n\nrec6\nNaN\nNaN\nNaN\n10.0\nTrue\n\n\n\n\n\n\n\n\n\n\n7.3.2 Row-wise Integration\nLet’s assume you have a third dataset:\n\n# dat3 = pd.read_csv(\"dat3.csv\", index_col=0)\ndf3 = pd.DataFrame({\n    \"X1\": [15, 25, 35],\n    \"X3\": [0.01, 0.05, 0.99],\n    \"X2\": [\"Very Satisfied\", \"Satisfied\", \"Dissatisfied\"],\n}, index=[\"rec11\", \"rec12\", \"rec13\"])\n\ndf3_reset = df3.reset_index().rename(columns={\"index\": \"rowname3\"})\ndf3_reset\n\n\n\n\n\n\n\n\nrowname3\nX1\nX3\nX2\n\n\n\n\n0\nrec11\n15\n0.01\nVery Satisfied\n\n\n1\nrec12\n25\n0.05\nSatisfied\n\n\n2\nrec13\n35\n0.99\nDissatisfied\n\n\n\n\n\n\n\n\n7.3.2.1 Row Binding (Appending rows)\nIf both datasets have the same columns, rows can be stacked with axis=0. Order of columns doesn’t matter as long as names match (it’s handled by pandas).\nThis method is useful for appending similar data sets, such as adding new observations to an existing set.\n\npd.concat([df1, df3], axis=0)\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\nrec1\n1\nVery Satisfied\n0.50\n\n\nrec2\n2\nSatisfied\n0.80\n\n\nrec3\n4\nNeutral\n0.80\n\n\nrec4\n4\nDissatisfied\n0.20\n\n\nrec5\n5\nVery Dissatisfied\nNaN\n\n\nrec11\n15\nVery Satisfied\n0.01\n\n\nrec12\n25\nSatisfied\n0.05\n\n\nrec13\n35\nDissatisfied\n0.99\n\n\n\n\n\n\n\n:::\nTo unify row name column before combining:\n\ndf3_row1 = df3.reset_index(drop=True)\npd.concat([df1, df3_row1], axis=0)\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\nrec1\n1\nVery Satisfied\n0.50\n\n\nrec2\n2\nSatisfied\n0.80\n\n\nrec3\n4\nNeutral\n0.80\n\n\nrec4\n4\nDissatisfied\n0.20\n\n\nrec5\n5\nVery Dissatisfied\nNaN\n\n\n0\n15\nVery Satisfied\n0.01\n\n\n1\n25\nSatisfied\n0.05\n\n\n2\n35\nDissatisfied\n0.99\n\n\n\n\n\n\n\n\n\n7.3.2.2 Handling Different Columns\nIf two DataFrames have different columns, pd.concat() fills missing columns with NaN — like an outer join.\n\ndf3_z = df3.copy()\ndf3_z.columns = ['Y1','Y2','Y3']\npd.concat([df1, df3_z], axis=0)\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nY1\nY2\nY3\n\n\n\n\nrec1\n1.0\nVery Satisfied\n0.5\nNaN\nNaN\nNaN\n\n\nrec2\n2.0\nSatisfied\n0.8\nNaN\nNaN\nNaN\n\n\nrec3\n4.0\nNeutral\n0.8\nNaN\nNaN\nNaN\n\n\nrec4\n4.0\nDissatisfied\n0.2\nNaN\nNaN\nNaN\n\n\nrec5\n5.0\nVery Dissatisfied\nNaN\nNaN\nNaN\nNaN\n\n\nrec11\nNaN\nNaN\nNaN\n15.0\n0.01\nVery Satisfied\n\n\nrec12\nNaN\nNaN\nNaN\n25.0\n0.05\nSatisfied\n\n\nrec13\nNaN\nNaN\nNaN\n35.0\n0.99\nDissatisfied\n\n\n\n\n\n\n\nTo keep only the common columns (like an inner join):\n\ncommon_cols = df1.columns.intersection(df3.columns)\npd.concat([df1[common_cols], df3[common_cols]], axis=0)\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\nrec1\n1\nVery Satisfied\n0.50\n\n\nrec2\n2\nSatisfied\n0.80\n\n\nrec3\n4\nNeutral\n0.80\n\n\nrec4\n4\nDissatisfied\n0.20\n\n\nrec5\n5\nVery Dissatisfied\nNaN\n\n\nrec11\n15\nVery Satisfied\n0.01\n\n\nrec12\n25\nSatisfied\n0.05\n\n\nrec13\n35\nDissatisfied\n0.99\n\n\n\n\n\n\n\n\n\n7.3.2.3 Related Reading/Reference\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Integration</span>"
    ]
  },
  {
    "objectID": "BD/BD-Integrate.html#footnotes",
    "href": "BD/BD-Integrate.html#footnotes",
    "title": "7  Data Integration",
    "section": "",
    "text": "joins for tables in a relational database↩︎",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Integration</span>"
    ]
  },
  {
    "objectID": "GS/GS-Front.html#related-reading",
    "href": "GS/GS-Front.html#related-reading",
    "title": "Getting Started",
    "section": "Related Reading",
    "text": "Related Reading\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "GS/GS-Front.html#packages",
    "href": "GS/GS-Front.html#packages",
    "title": "Getting Started",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used for this section. Please install them to avoid potential interruptions/errors.\n\nreadxl, writexl, and readr for importing/exporting data\ntibble for tibble data object of tidyverse\n\n\n#import numpy as np\n#import pandas as pd\n1+1\n\n2",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "intro.html#course-summary",
    "href": "intro.html#course-summary",
    "title": "1  Introduction",
    "section": "",
    "text": "Getting Started\nBasics in Programming with Python\nBasics in Data Science\nData Visualization\nSupervised Data Mining\nUnsupervised Data Mining\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#this-website",
    "href": "index.html#this-website",
    "title": "Business Data Mining and Visualization",
    "section": "This Website",
    "text": "This Website\nThis website is for the lecture notes of { BUDA-450 Business Data Mining and Visualization }, and it will be updated with the development of the course.\nThe course aims to introduce to concepts and modeling techniques with simulation software and practical implementation examples.\nThe content of the course will be given with the following software and brief introductions.\n\nPython and its packages\nJupiter notebook (or jupiterlab)\nConda/Anaconda for virtual environments",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "GS/GS-Python.html#setting-your-own-computer",
    "href": "GS/GS-Python.html#setting-your-own-computer",
    "title": "1  Python Preliminaries",
    "section": "1.3 Setting Your Own Computer",
    "text": "1.3 Setting Your Own Computer\nThe following instruction is for a python environment on your own computer. The overall installation is as follow:\n\n(Optional) Install a software for virtual environments (Anaconda), and create a virtual environment\nInstall Python (in the virtual environment)\nInstall an IDE (Jupiter Notebook) (in the virtual environment)\n\n\n1.3.1 Virtual Environment\nDifferent projects often require different environment setups. That’s when a virtual environment comes in handy, which helps to work on isolated Python environments where you can freely create/delete them (with no risk of messing up your entire computer).\nAlso, considering rapid version updates, an easy way to manage a python environment for the compatibility of python and many packages is building it on a virtual environment.\n\n\n\n\n\n\nWhy Use Conda to Install Python?\n\n\n\n\nInstalling Python through Conda (via Anaconda or Miniconda) is highly recommended for this course, especially for users working with data science and analytics tools.\nHere’s why:\n\nIsolated Environments: Conda allows you to create separate environments for different projects. Each environment can have its own version of Python and packages—preventing version conflicts.\nBetter Package Management: Unlike pip (Python’s default package manager), Conda can install not just Python packages but also system-level dependencies like C or Fortran libraries. This is especially helpful for scientific packages like numpy, scipy, and pytorch.\nCross-Platform Compatibility: Conda works consistently across Windows, macOS, and Linux. It is also used by most cloud data science environments, helping ensure reproducibility.\nFast Setup with Anaconda: The Anaconda distribution includes over 250 pre-installed data science packages (e.g., pandas, matplotlib, scikit-learn, jupyterlab), making it ideal for beginners and fast onboarding.\nScientific Computing Support: Many data science and machine learning tools depend on optimized compiled libraries. Conda handles these dependencies more reliably than pip.\nReproducible, portable environments that “just work,” especially in data science: Conda is the most robust choice.\n\n\n\n\n\n1.3.2 Installing Anaconda (or Conda)\nAnaconda is a popular software for that.\nhttps://www.anaconda.com/docs/getting-started/getting-started\nhttps://www.anaconda.com/docs/getting-started/anaconda/install\n\n\n1.3.3 Virtual Environment through Conda\nhttps://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html\n\n1.3.3.1 Installing Python\nTo download and install Python, we’ll use the uv “distribution” of Python, which is available on all major operating systems. To install it, follow the instructions at this website. Unlike installing normal programmes, we’re going to use the command line to install Python. Linux, Mac, and Windows all have built-in command lines: search for Terminal on Mac or Linux, and Powershell on Windows. These apps will bring up boxes that you can type commands in. As of the time of writing, the commands are:\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nfor Linux and Mac, and\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\nfor Windows. Hit return to execute the commands.\nOnce you have installed uv, you can check it’s installed properly by running uv --version. You should see a message pop up that says “uv” and then the latest version number.\nWe didn’t actually download Python yet–we downloaded uv, which is going to manage our Python environment. So next we need to run\nuv python install\n\n\n1.3.3.2 Installing your integrated development environment, Visual Studio Code\nVisual Studio Code is a free and open source IDE from Microsoft that is available on all major operating systems. Just like Python itself, Visual Studio can be extended with packages, and it is those packages, called extensions in this case, that make it so useful. As well as Python, Visual Studio Code supports a ton of other languages.\nDownload and install Visual Studio Code. If you need some help, there is a video below that will walk you through downloading and installing Visual Studio Code, and then using it to run Python code in both scripts and in notebooks. We’ll go through these instructions in detail in the rest of this chapter.\nNote that if you are testing out notebooks and the interactive window, you will need to create a new folder and work in it in Visual Studio Code. Once you’ve opened a folder explicitly in VS Code, open the command line within Visual Studio Code then run uv init to create a Python installation, and uv add jupyter to provide this nascent Python environment with the means to create interactive windows.\n\n\nHow to install Visual Studio Code and use it to run Python code\n\n\n\n1.3.4 Coding in the cloud\nThese instructions are for if you wish to code in the cloud rather than on your own computer. There are many ways to do data science in the cloud, but we’re going to share with you the absolute simplest. For this, you will need to sign up for a Github Account. GitHub is an organisation that’s owned by Microsoft and which provides a range of services including a way to back-up code on the cloud, and cloud computing. One of the services offered is Github Codespaces. A GitHub Codespace is an online cloud computer that you connect to from your browser window. It has a generous 60 hours free of computing per month.\nIf you go over the free tier hours on GitHub Codespaces, your credit card will be charged for any further hours of GitHub Codespaces you use.\nOnce you’ve signed up for a GitHub account, head to Github Codespaces and click on “Get Started for Free”. You should see a menu of “quick start templates”. Under where it says “Jupyter Notebook”, hit “Use this template”.\nYou will find that a new page loads with several panels in. This is an online version of Visual Studio Code that works much like if you had installed it on your own computer. It will already have a version of Python installed—you can check which one by running python --version in the terminal. The terminal is usually found in the lowest panel of Visual Studio Code, and, in Codespaces, will typically display a welcome message.\n\n\n1.3.5 Alternative ways to run the code from the book\nAs well as following this book using your own computer or on the cloud via GitHub Codespaces, you can run the code online through a few other options. The first is the easiest to get started with.\n\nGoogle Colab notebooks. Free for most use. You can launch most pages in this book interactively by using the ‘Colab’ button under the rocket symbol at the top of most pages in this book. It will be in the form of a notebook (which mixes code and text) rather than a script (.py file) but the code you write is the same. Note that Colab doesn’t use Visual Studio Code.\nGitpod Workspace. An alternative to Codespaces. This is a remote, cloud-based version of Visual Studio Code with Python installed and will run Python scripts. Note that the free tier covers 50 hours per month.\n\nksexbctzgnqb Exercise Try going to the {ref}`code-basics` chapter now, click on the rocketship symbol (🚀) and then select \"Colab\". When the Colab notebook opens, select the first code cell and hit shift + enter to run it.\n\n\n1.3.6 Running your first Python code\n\n1.3.6.1 Getting to grips with Visual Studio Code\nOnce you have Visual Studio Code installed and opened (either on your own computer or in the cloud), navigate to the ‘extensions’ tab on the left hand side vertical bar of icons (it’s the one that looks like 4 squares). You’ll need to install the Python extension, which you can search for by using the text box within VS Code’s extensions panel. If you’re using the cloud version, you may find that it’s already installed.\nThere are some other extensions it’s useful to have and install (if they aren’t already):\n\nJupyter\nPylance\nindent-rainbow\n\nAlthough you won’t have any Python code to play with yet, or an interactive window to execute that Python code, it’s worth us spending a brief moment familiarising ourselves with the different bits of a typical view in Visual Studio Code.\n\n\n\nA typical user view in Visual Studio Code\n\n\nThe figure above shows the typical layout of Visual Studio Code once you have a Python session running, and a Python script open. The long vertical panel on the far left-hand side changes what is seen in panels 1 and 2; it currently has the file explorer selected. Let’s run through the numbered parts of the figure.\n\nWhen the explorer option is selected from the icons to the left of 1 and 2, the contents of the folder that’s currently open are shown in 1.\nThis is an outline of the key parts of the file that is open in 3.\nThis is just a fancy text editor. In the figure above, it’s showing a Python script (a file that contains code and has a name that ends in .py). Shortly, we’ll see how selecting code and pressing Shift + Enter (‘Enter’ is labelled as ‘Return’ on some keyboards) will execute code whose results appear in panel 5.\nThis is the command line or terminal, a place where you can type in commands that your computer will then execute. If you want to try a command, type date (Mac/Linux) or date /t (Windows). This is where we install extra packages.\nThis is the interactive Python window, which is where code and code outputs appear after you select and execute them from a script (see 3). It shows the code that you executed and any outputs from that execution—in the screenshot shown, the code has created a plot. The name and version of Python you’re using appear at the top of the interactive window.\n\nNote that there is lots of useful information arrayed right at the bottom of the window in the blue bar, including the version of Python currently being used by VS Code.\n\n\n1.3.6.2 Running Python code\nNow you will create and run your first code. If you get stuck, there’s a more in-depth tutorial over at the VS Code documentation.\nIn Visual Studio Code, click on the “Explorer” symbol (some files on the left-hand side of the screen) to bring up a file explorer. Check you’re in a good location on your computer to try things out and, if not, change the folder you’re in using File -&gt; Open Folder until you’re happy.\nNow open up a terminal within Visual Studio Code. Make sure it’s the same type of terminal that you installed uv in, eg use Powershell on Windows (you can select to use different types of terminals.) The shortcut to open the terminal panel is Ctrl + ` on Linux and Windows, and Cmd + ` on Mac. The terminal panel appears along the bottom of VS Code. Run uv init in the terminal to initiate an installation of Python. This Python installation only lives in your current folder. Then, to install the package we need for this section run uv add jupyter. (We will cover the terminal and installing packages in much more detail shortly.)\nNow, still with the explorer panel open on the left hand side, click on the symbol that looks like a blank piece of paper with a “+” sign on it. This will create a new file, and your cursor should move to name it. Name it hello_world.py. The file extension, .py, is very important as it implicitly tells Visual Studio Code that this is a Python script.\nIn the Visual Studio Code editor, add a single line to the file:\nprint('Hello World!')\nSave the file.\nIf you named this file with the extension .py then VS Code will recognise that it is Python code and you should see the name and version of Python pop up in the bar at the bottom of your VS Code window. (You can have multiple versions of Python installed—if you ever want to change which Python version your code uses, click on the version shown in the bar and select the version you want.)\nAlright, shall we actually run some code? Select/highlight the print(\"Hello world!\") text you typed in the file and right-click. You’ll get a lot of options here, but the one you want is “Run Selection/Line in Interactive Window”.\nThis should cause a new ‘interactive’ panel to appear within Visual Studio Code, and, hey presto you should see:\nprint(\"Hello world!\")\nHello world!\nThe interactive window is a convenient and flexible way to run code that you have open in a script or that you type directly into the interactive window code box. The interactive window will ‘remember’ any variables that have been assigned (for examples, code statements like x = 5), whether they came from running some lines in your script or from you typing them in directly. Working with the interactive window will feel familiar to anyone who has used Stata, Matlab, or R. It doesn’t require you to write the whole script, start to finish, ahead of time. Instead, you can jam, changing code as you go, (re-)running it line by line.\nIt would be cumbersome to have to right-click every time we wanted to run some code, so we’re going to make a keyboard shortcut to send whatever code is highlighted to the interactive window to be executed. To do this:\n\nOpen up the Visual Studio Code configuration menu (the cog on the lower left-hand side)\nGo to Settings\nType “jupyter send” in the box to make an entry “Interactive Window &gt; Text Editor: Execute Selection” appear\nEnsure the box next to this entry is ticked\n\nNow return to your script, put your cursor on the line with print(\"Hello world!\") on, and hit Shift+Enter. You should see “Hello world!” appear again, only this time, it was much easier.\n```ksexbctzgnqb Running code in the terminal instead :class: dropdown\nThe interactive window isn’t the only way to run code; you can do it in the terminal too. This is less popular for data science, but it does occasionally have its uses. If you want to do this, right-click on the selected code and choose “Run Python -&gt; Run Selection/Line in Terminal”.\n\nLet's make more use of the *interactive window*. At the bottom of it, there is a box that says 'Type code here and press shift-enter to run'. Go ahead and type `print('Hello World!')` directly in there to achieve the same effect as running the line from your script. Also, any variables you run in the interactive window (from your script or directly by entering them in the box) will persist.\n\nTo see how variables persist, type `hello_string = 'Hello World!'` into the interactive window's code entry box and hit shift-enter. If you now type `hello_string` and hit shift+enter, you will see the contents of the variable you just created. You can also click the grid symbol at the top of the interactive window (between the stop symbol and the save file symbol); this is the variable explorer and will pop open a panel showing all of the variables you've created in this interactive session. You should see one called `hello_string` of type `str` with a value `Hello World!`.\n\nThis shows the two ways of working with the interactive window--running (segments) from a script, or writing code directly in the entry box. It doesn't matter which way you entered variables, they will all be remembered within that session in your interactive window.\n\n```{admonition} Start interactive windows and terminals within your project directory\n:class: dropdown\nIn Visual Studio Code, you can ensure that the interactive window starts in the root directory of your project by setting \"Jupyter: Notebook File Root\" to `${workspaceFolder}` in the Settings menu. For the integrated command line, change \"Terminal › Integrated: Cwd\" to `${workspaceFolder}` too.\nksexbctzgnqb Exercise Create a new script that, when run, prints \"Welcome to Coding for Economists\" and run it in an interactive window.\n\n\n\n1.3.7 Packages and how to install them\nPackages (also called libraries) are key to extending the functionality of Python and it won’t be long before you’ll need to install some extra ones! There are packages for geoscience, for building websites, for analysing genetic data, and, yes, of course, for economics. Packages are typically not written by the core maintainers of the Python language but by enthusiasts, firms, researchers, academics, all sorts! Because anyone can write packages, they vary widely in their quality and usefulness. There are some that are key for an economics workflow, though, and you’ll be seeing them again and again.\n\n\nName a more iconic trio, I'll wait. pic.twitter.com/pGaLuUxQ3r\n\n— Vicki Boykis ((vboykis?)) August 23, 2018\n\n\nThe three Python packages numpy, pandas, and maplotlib, which respectively cover provide numerical, data, and plotting functionality, are ubiquitous. So many scripts begin by importing them, as in the tweet above!\nPython packages don’t come built-in (by definition) so you need to install them (just once, like installing any other application), and then import them into your scripts (whenever you use them in a script). When you issue an install command for a specific package, it is automatically downloaded from the internet and installed in the appropriate place on your computer.\nWe use the terminal or command line within Visual Studio Code to install additional Python packages. In the figure earlier in the Chapter, this is labelled as panel number 4.\nTo install extra Python packages, you issue install commands to a text-based window called the “terminal”.\n\n1.3.7.1 The Terminal in Brief\nThe terminal is also known as the command line and sometimes the command prompt. It was labelled 4 in the screenshot of Visual Studio Code from earlier in the chapter. The terminal is a text-based way to issue all kinds of commands to your computer (not just Python commands) and knowing a little bit about it is really useful for coding (and more) because managing packages, environments (which we haven’t yet discussed), and version control (ditto) can all be done via the terminal.\nTo open up the command line within Visual Studio Code, use the &lt;kbd&gt;⌃&lt;/kbd&gt; + &lt;kbd&gt;\\`&lt;/kbd&gt; keyboard shortcut (Mac) or &lt;kbd&gt;ctrl&lt;/kbd&gt; + &lt;kbd&gt;\\`&lt;/kbd&gt; (Windows/Linux), or click \"View &gt; Terminal\".\n\nIf you want to open up the command line independently of Visual Studio Code, search for \"Terminal\" on Mac and Linux, and \"Powershell\" on Windows. \nFirstly, everything you can do by clicking on icons to launch programmes on your computer, you can also do via the terminal (also known as the command line). The functionality of a lot of programmes can be accessed using the command line, and some programmes only have a command line interface (CLI). This includes some that are used for data science.\nThe command line interacts with your operating system and can be used to create, activate, or change python installations.\nUse Visual Studio Code to open a terminal window by clicking Terminal -&gt; New Terminal on the list of commands at the very top of the window. If you have installed uv on your computer, your terminal should look something like this as your ‘command prompt’:\nyour-username@your-computer current-directory %\non Mac, and the same but with ‘%’ replaced by ‘$’ on linux, and (using Powershell)\nPS C:\\Windows\\System32&gt;\non Windows.\nYou can check that uv has successfully installed Python in your current project’s folder by running\nuv run python --version\nYou can find out more about the terminal in the chapter on {ref}wrkflow-command-line.\n\n\n1.3.7.2 Installing Packages\nTo install any packages within the Python environment in the folder you’re using in Visual Studio Code, enter the following in Visual Studio Code’s terminal (the same place where you just ran uv run python --version).\nuv add packagename\nand hit return. In the above, packagename might be pandas, for example. If you have problems installing, make sure that you are connected to the internet, and that PyPI (the Python package index) isn’t blocked by your firewall or proxy.\nYou can see what packages you have previously installed by entering uv pip list into the command line.\n\n\n\n1.3.8 Review\nYou’re ready to move on to the next chapter! 🚀\nWell done if you made this far: starting is the hardest bit. (If you want to tweak your Visual Studio Code setup even more, there’s some tips at the end of {ref}code-further-advanced.)",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Preliminaries</span>"
    ]
  },
  {
    "objectID": "GS/GS-Python.html#others",
    "href": "GS/GS-Python.html#others",
    "title": "1  “Python Preliminaries”",
    "section": "1.8 Others",
    "text": "1.8 Others\n\n1.8.1 Markdown with Executable Code Chunks\nThis is by far the least common way of coding, though it has gained popularity in recent years and it’s great if you’re going to ultimately export to other formats like slides, documents, or even a website!\nWhen you have much more text combined code, even using Jupyter Notebooks can feel a bit onerous and, historically, editing the text in a notebook was a bit tedious - especially if you wanted to move cells around a lot. Markdown makes for a much more pleasant writing experience. But markdown on its own cannot execute code—but imagine you want to combine reproducibility, text, and code + code outputs: however, there is a tool called Quarto that allows you to do this by adding executable code chunks to markdown.\nAs this is a bit more of an advanced topic, and as much about communication as it is about writing code, we’ll come back to how to do it in {ref}quarto.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Preliminaries</span>"
    ]
  },
  {
    "objectID": "index.html#this-web-book",
    "href": "index.html#this-web-book",
    "title": "Business Data Mining and Visualization",
    "section": "This web-book",
    "text": "This web-book\nThis website is for the lecture notes of BUDA-450 Business Data Mining and Visualization, and it will be updated with the development of the course.\nThe courses aim to introduce to concepts and modeling techniques with simulation software and practical implementation examples.\nThe content will be with the following software and a brief introduction and the instruction of its installation will be given.\n\nPython and its packages\nJupiter notebook\nConda/Anaconda",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "BP/BP-IntroCoding.html#overview",
    "href": "BP/BP-IntroCoding.html#overview",
    "title": "3  Introduction to Coding",
    "section": "",
    "text": "Programming\nComment\nVariable”\n\n\n\n\n\n\n\nTip\n\n\n\nRemember, you can launch this page interactively by using the ‘Colab’ button under the rocket symbol () at the top of the page. You can also download this page as a Jupyter Notebook to run on your own computer: use the ‘download .ipynb’ button under the download symbol the top of the page and open that file using Visual Studio Code.",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Coding</span>"
    ]
  },
  {
    "objectID": "intro.html#buda-450",
    "href": "intro.html#buda-450",
    "title": "1  Introduction to Courses",
    "section": "",
    "text": "Getting Started\nBasics in Programming with Python\nBasics in Data Science\nData Visualization\nSupervised Data Mining\nUnsupervised Data Mining",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Courses</span>"
    ]
  },
  {
    "objectID": "BP/BP-IntroCoding.html#if-you-get-stuck",
    "href": "BP/BP-IntroCoding.html#if-you-get-stuck",
    "title": "3  Introduction to Coding",
    "section": "3.7 If you get stuck",
    "text": "3.7 If you get stuck\nIt’s worth saying at the outset that no-one memorises half of the stuff you’ll see in this book. 80% or more of time spent programming is actually time spent looking up how to do this or that online, ‘debugging’ a code for errors, or testing code. This applies to all programmers, regardless of level. You are here to learn the skills and concepts of programming, not the precise syntax (which is easy to look up later).\n\n\n\nxkcd-what-did-you-see\n\n\nKnowing how to Google is one of the most important skills of any coder. No-one remembers every function from every library. Here are some useful coding resources:\n\nwhen you have an error, look on Stack Overflow to see if anyone else had the same error (they probably did) and how they overcame it.\nif you’re having trouble navigating a new package or library, look up the documentation online. The best libraries put as much effort into documentation as they do the code base.\nuse cheat sheets to get on top of a range of functionality quickly. For instance, this excellent (mostly) base Python Cheat Sheet.\nif you’re having a coding issue, take a walk to think about the problem, or explain your problem to an animal toy on your desk (traditionally a rubber duck, but other animals are available).",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Coding</span>"
    ]
  },
  {
    "objectID": "GS/GS-Coding.html",
    "href": "GS/GS-Coding.html",
    "title": "2  Python Coding",
    "section": "",
    "text": "2.1 Overview\nThis section introduces how coding tasks look like, in terms of running and writing codes.\nIn this chapter, we’ll look at the different ways you can both write and run code. This is something that can be very confusing if you’re just getting into programming.\nThere are different ways to write and run code that suit different needs. For example, for creating a reproducible pipeline of tasks or writing production-grade software, you might opt for a script——a file that is mostly code. And you might even bundle that up in an installable package. But for sending instructions to a colleague or exploring a narrative, you might choose to write your code in a notebook because it can present text and code together more naturally than a script can.\nWe already met some ways to write and run code in previous chapters. Here, we’ll be a bit more systematic so that, by the end of the chapter, you’ll be comfortable writing code in both scripts (the most popular way) and notebooks. We’ll also look ahead to writing executable code chunks in markdown documents, which has some real strengths for communication to people who don’t need to see the code.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Coding</span>"
    ]
  },
  {
    "objectID": "GS/GS-Coding.html#running-codes",
    "href": "GS/GS-Coding.html#running-codes",
    "title": "2  Python Coding",
    "section": "2.3 Running Codes",
    "text": "2.3 Running Codes\nThere are several ways of running Python code, depending on your environment, use case, and preferences. Below is a categorized list, grounded in factual, well-documented sources (Python official docs, Jupyter docs, etc.):\n\n2.3.1 Local Execution Methods\n\n2.3.1.1 Interactive Interpreter (REPL)\nRead–eval–print loop You can run Python interactively in a terminal or command prompt:\npython\nThis method allows you to type and execute code line by line.\n\nPros\n\nInstant feedback — good for learning and testing snippets.\nNo need to create files.\nLightweight and fast.\n\nCons\n\nNo code persistence (unless manually saved).\nNot ideal for large programs or projects.\n\n\n\n\n2.3.1.2 .py Script Execution in Terminal\nWrite Python code in a .py file and run it from the command line:\npython script_name.py\n\nPros\n\nStandard way to run Python programs.\nSupports larger, modular projects.\nEasily used in automation and pipelines.\n\nCons\n\nNo graphical feedback.\nErrors halt the script unless handled explicitly.\n\n\n\n\n2.3.1.3 Integrated Development Environments (IDEs)\nPopular IDEs for Python include VS Code, PyCharm, Spyder\nFeatures typically include debugging, linting, autocompletion, and project management.\n\nPros\n\nRich development features (debugging, autocomplete, version control).\nSyntax highlighting and linting support.\nSuitable for professional development and large projects.\n\nCons\n\nCan be resource-heavy.\nSteeper learning curve for beginners.\nRequires installation and setup.\n\n\n\n\n2.3.1.4 IDLE (Python’s Built-in GUI)\nIDLE comes with the standard Python installation and provides a basic GUI for editing and running code.\n\nPros\n\nSimple and beginner-friendly.\nComes pre-installed with Python.\nGUI interface for editing and execution.\n\nCons\n\nLacks advanced development features.\nNot ideal for large projects or modern workflows.\n\n\n\n\n2.3.1.5 Jupyter Notebooks\nRun Python code in cells with inline outputs:\njupyter notebook\nIdeal for data analysis and visualization.\n\nPros\n\nGreat for interactive data analysis and visualization.\nSupports markdown, LaTeX, and inline plots.\nCode can be executed in chunks (cells).\n\nCons\n\nNot suitable for production code or software packaging.\nHarder to version control due to JSON notebook format.\nLong-running notebooks can become difficult to manage.\n\n\n\n\n\n2.3.2 Online & Cloud-Based Execution\n\n2.3.2.1 Google Colab\nA browser-based Jupyter-like environment with free GPU access.\n\nPros\n\nFree access to GPUs and TPUs.\nCloud-based — no installation required.\nEasy sharing via Google Drive.\n\nCons\n\nRequires internet access.\nLimited control over environment and execution timeouts.\nPrivacy concerns with sensitive data.\n\n\n\n\n2.3.2.2 Replit\nFull-featured online IDE for Python and other languages.\n\nPros\n\nFull IDE in the browser.\nGreat for collaboration and quick testing.\nSupports multiple languages and real-time editing.\n\nCons\n\nRequires account login.\nLimited compute resources on free tier.\nLess customizable than local environments.\n\n\n\n\n2.3.2.3 Other Online Runners\nExamples: PythonAnywhere, Trinket\n\nPros\n\nVery easy setup.\nGood for teaching or demonstrating code.\nNo local configuration needed.\n\nCons\n\nLimited functionality and compute power.\nMay lack support for packages or files.\nOften ad-supported or limited on free plans.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Coding</span>"
    ]
  },
  {
    "objectID": "GS/GS-Coding.html#writing-codes",
    "href": "GS/GS-Coding.html#writing-codes",
    "title": "2  Python Coding",
    "section": "2.2 Writing Codes",
    "text": "2.2 Writing Codes\nLet’s start with some definitions.\n\nthe terminal: the text interface that you use to send instructions to your computer’s operating system.\nthe interpreter: the programming language (e.g., Python) that has to be installed separately onto your computer. It is what takes your written commands and turns them into actions.\nscripts: files that almost exclusively contain code, which can be edited and run in an IDE or in the terminal.\n\nPython scripts always have the file extension .py.\n\nmarkdown: a lightweight language that turns simple text commands into professional looking documents (in a predetermined style).\n\nIt’s also what’s used for the text cells in Jupyter Notebooks.\nWhen not in a notebook, files containing markdown always have the extension .md.\n\nquarto markdown: a special variant of markdown, with file extension .qmd, that can be used to combine text and code that gets executed so that code outputs are inserted into final outputs such as html or pdf documents.\nnotebooks (aka Jupyter Notebooks): files that can contain code and text in different blocks called “cells”.\n\nThe code appears in code cells, while the text appears in markdown cells.\nNotebooks have the file extension .ipynb, and can be exported to other formats, like word documents, HTML pages, PDFs, markdowns, and even slides!\nThe code parts can be run in an IDE either all at once or however you like.\n\nIDE, or integrated development environment: the application that you write code of all different kinds in (scripts, notebooks, markdown).\n\nJupyterLab is an IDE, but one which is geared towards the use of notebooks.\n\n\nLet’s now turn to all of the different ways you can write code in a fully-featured integrated development environment like Visual Studio Code. They each have pros and cons, and you’re likely to want to use them at different times. The table below sets out all of the different ways you can write, and execute, code.\nIf you’re looking for a typical workflow, this book recommends working with scripts (files that end in .py) and the VS Code interactive window. Remember, if you’re working with a .py file, you can always open the Visual Studio Code interactive window by right-clicking somewhere within the script and selecting ‘Run in interactive window’.\n\nPython Coding Tools and Prerequisites\n\n\nWhat\nFile\nHow to use\nPrerequisites\n\n\n\n\n\nScript\nscript.py\n‘Run in interactive window’ in an integrated development environment (IDE)\nPython installation + an IDE with Python support, eg Visual Studio Code.\n\n\n\nJupyter Notebook\nnotebook.ipynb\nOpen the file with Visual Studio Code.\nUse Visual Studio Code and the VS Code Jupyter extension.\n\n\n\nMarkdown with executable code chunks using Quarto\nmarkdown_script.qmd\nTo produce output, write in a mix of markdown and code blocks and then export with commands like quarto render markdown_script.qmd --to html on the command line or using the Visual Studio Code extension. Other output types available.\nInstallations of Python and Quarto, plus their dependencies.\n\n\n\n\n\nPros and Cons of Different Python Coding Tools\n\n\nWhat\nPros\nCons\n\n\n\n\n\n\nScript\nCan be run all-in-one or step-by-step as needed. Very powerful tools available to aid coding in scripts. De facto standard for production-quality code. Can be imported by other scripts. Version control friendly.\nNot very good if you want to have lots of text alongside code.\n\n\n\n\nJupyter Notebook\nCode and text can alternate in the same document. Rich outputs of code can be integrated into document. Can export to PDF, HTML, and more, with control over whether code inputs/outputs are shown, and either exported directly or via Quarto. Can be run all-in-one or step-by-step as needed.\nFussy to use with version control. Code and text cannot be mixed in same ‘cell’. Not easy to import in other code files.\n\n\n\n\nMarkdown with executable code chunks using Quarto\nAllows for true mixing of text and code. Can export to wide variety of other formats, such as PDF and HTML, with control over whether code inputs/outputs are shown. Version control friendly.\nCannot be imported by other code files.\n\n\n\n\n\nSome of the options above make use of the command line, a way to issue text-based instructions to your computer. Remember, the command line (aka the terminal) can be accessed via the Terminal app on Mac, the Command Prompt app on Windows, or ctrl + alt + t on Linux. To open up the command line within Visual Studio Code, you can use the keyboard shortcut ⌃ + ` (on Mac) or ctrl + ` (Windows/Linux), or click “View &gt; Terminal”.\n\nSummary: Ways to Run Python Code\n\n\n\n\n\n\n\nMethod\nPros\nCons\n\n\n\n\nInteractive Interpreter (REPL)\n- Instant feedback- Great for quick tests\n- No code persistence- Not suitable for full scripts\n\n\n.py Script in Terminal\n- Standard and portable- Good for automation and projects\n- No graphical interface- Execution halts on unhandled errors\n\n\nJupyter Notebooks\n- Interactive and visual- Markdown & LaTeX support\n- Poor version control- Not suitable for production code\n\n\nIDEs (VS Code, PyCharm, etc.)\n- Feature-rich- Debugging, version control, linting\n- Resource-heavy- Steeper learning curve\n\n\nIDLE (Python’s GUI)\n- Beginner-friendly- No setup needed\n- Limited features- Not scalable for large projects\n\n\nGoogle Colab\n- Free GPU/TPU- Cloud-based, no install\n- Requires internet- Limited session length\n\n\nReplit\n- Browser-based IDE- Good for collaboration\n- Account required- Limited compute resources\n\n\nOther Online Runners\n- Easy to use- Great for teaching/demo\n- Feature-limited- May require payment for full access\n\n\n\nNow let’s look at each of these ways to run code in more detail using a common example: Hello World!\n\n2.2.1 Scripts\nMost code is written in scripts and they should be your go-to.\nWe already met scripts, but let’s have a recap. Create a new file in Visual Studio Code called hello_world.py. In the Visual Studio Code editor, add a single line to the file:\nprint('Hello World!')\nSave the file. Right-click and, to run the script, you can either use ‘Run current file in interactive window’, or ‘Run current file in terminal’, or ‘Run selection/line in interactive window’. These are two different methods of running the script: in the IDE (VS Code in this case) or in the command line.\nA typical workflow would be selecting some lines within a script, and then hitting ‘Run selection/line in interactive window’ or using the keyboard shortcut of shift + enter.\nAs an alternative for the latter, you can open up the command line yourself and run\npython hello_world.py\nwhich will execute the script.\n\n\n2.2.2 Jupyter Notebooks\nJupyter Notebooks are another popular way to write code, in addition to scripts (.py files). Notebooks mix code and text by having a series of “cells” that are either code or text. Jupyter Notebooks are for experimentation, tinkering, and keeping text and code together. They are the lab books of the coding world. This book is mostly written in Jupyter Notebooks, including this chapter! You can download the notebooks that make up most chapters of this book and run them on your own computer: look for the download symbol at the top of each page; “.ipynb” means ipython notebook.\nThe name, ‘Jupyter’, is a reference to the three original languages supported by Jupyter, which are Julia, Python, and R, and to Galileo’s notebooks recording the discovery of the moons of Jupiter. Jupyter notebooks now support a vast number of languages beyond the original three, including Ruby, Haskell, Go, Scala, Octave, Java, and more.\n\n2.2.2.1 Writing Your First Notebook\n(Alternatively to the instructions here, you can use Google Colab to try a kind of notebook with no setup at all.)\nTo get started with Jupyter Notebooks, you’ll need to have a Python installation and to have run pip install jupyterlab on the command line (to install the packages needed for Jupyter Notebooks). Then, in Visual Studio Code, creating a new notebook is as easy as File -&gt; New File -&gt; Jupyter Notebook. Save your new notebook file as hello_world.ipynb. (You can just open any new file and name it with a .ipynb extension, but you’ll need to close the file and re-open it for VS Code to recognise that it’s a notebook.)\nThe notebook interface should automatically load and you’ll see options to create cells with plus signs labelled ‘Code’ and ‘Markdown’. A cell is an independent chunk of either code or text. Text cells use markdown, a lightweight language for creating text outputs that you will find out more about in {ref}wrkflow-markdown.\nTry adding print(\"hello world!\") to the first (code) cell and hitting the play symbol on the left-hand side of the cell. You will be prompted to select a “kernel”, a version of Python on your system. For this, it doesn’t matter which kernel (Python interpreter) you use. In future, you may want to set the kernel using the “Select Kernel” option at the top right-hand side of the screen to tell Visual Studio Code what specific version of Python you want to use to execute any code.\nNow add a markdown cell (“+ Markdown”) and enter:\n# This is a title\n\n## This is a subtitle\n\nThis notebook demonstrates printing 'hello world!' to screen.\nClick the tick that appears at the top of this cell.\nNow, for the next cell, choose code and write:\nprint('another code cell')\nTo run the notebook, you can choose to run all cells (usually a double play button at the top of the notebook page) or just each cell at a time (a play button beside a cell). ‘Running’ a markdown cell will render the markdown in display mode; running a code cell will execute it and insert the output below. When you play the code cell, you should see the ‘hello world!’ message appear.\nNote that you can use the keyboard short-cut Shift+Enter to execute cells one-by-one instead of hitting the play button.\nJupyter Notebooks are versatile and popular for early exploration of ideas, especially in fields like data science. Jupyter Notebooks can easily be run in the cloud using a browser too (via Binder or Google Colab) without any prior installation. Although it’s not got any executable code in, the page you’re reading now can be loaded into Google Colab as a Jupyter Notebook by clicking ‘Colab’ under the rocket icon at the top of the page.\ndodioszkoqon Exercise What happens when you press \"clear outputs\"? What about \"run all?\".\nOne really nice feature of Jupyter Notebooks is that you can use them as the input files for Quarto instead of using .qmd files, and this opens up many export options and possibilities (like hiding some code inputs). You can find more information here (look for the guidance on Jupyter Notebooks aka .ipynb files) or look ahead to the chapters on {ref}wrkflow-markdown and {ref}quarto.\nYou can try a Jupyter Notebook without installing anything online at https://jupyter.org/try. Click on Try Classic Notebook for a tutorial. If you get stuck with getting started with notebooks, there’s a more in-depth VS Code and Jupyter tutorial available here.\n\n\n2.2.2.2 Tips when using Jupyter Notebooks\n\nVersion control: if you are using version control, be wary of saving the outputs of Jupyter Notebooks when you only want to save code. Most IDEs that support Jupyter Notebooks have a clear outputs option. You can also automate this as a pre-commit git hook (if you don’t know what that is, don’t worry). You could also pair your notebook to a script or markdown file (covered in the next section). Outputs or not, Jupyter Notebooks will render on GitHub, the popular remote repository for source code.\nTerminal commands: these can be run from inside a Jupyter Notebook by placing a ! in front of the command and executing the cell. For example, !ls gives the directory the notebook is in. You can also !pip install and !conda install in this way.\nMagic commands: statements that begin with % are magic commands. %whos displays information about defined variables. %run script.py runs a script called script.py. %timeit times how long the cell takes to execute. Finally, you can see many more magic commands using %quickref.\nNotebook cells can be executed in any sequence you choose. But if you’re planning to share your notebook or use it again for yourself, it’s good practice to check that its cells do what you want when run in sequence, from top to bottom.\nThere are tons of extensions to Jupyter Notebooks; you can find a list here. Of particular note is ipywidgets, which adds interactivity.\nGet help info on a command by running it but with ? appended to end.\n\n\n\n\n2.2.3 Markdown with Executable Code Chunks\nThis is by far the least common way of coding, though it has gained popularity in recent years and it’s great if you’re going to ultimately export to other formats like slides, documents, or even a website!\nWhen you have much more text combined code, even using Jupyter Notebooks can feel a bit onerous and, historically, editing the text in a notebook was a bit tedious - especially if you wanted to move cells around a lot. Markdown makes for a much more pleasant writing experience. But markdown on its own cannot execute code—but imagine you want to combine reproducibility, text, and code + code outputs: however, there is a tool called Quarto that allows you to do this by adding executable code chunks to markdown.\nAs this is a bit more of an advanced topic, and as much about communication as it is about writing code, we’ll come back to how to do it in {ref}quarto.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Coding</span>"
    ]
  },
  {
    "objectID": "GS/GS-Coding.html#related-readingreference",
    "href": "GS/GS-Coding.html#related-readingreference",
    "title": "2  Python Coding",
    "section": "2.4 Related Reading/Reference",
    "text": "2.4 Related Reading/Reference\n\nSections 2 & 5 of An Introduction to R: An introduction to R as one of the official manual documents by the R Development Core Team The R Manuals\nTextbook websites for R basics\n\nSection 5 of Hands-On Programming with R\nSection 2 of An Introduction to R",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Coding</span>"
    ]
  },
  {
    "objectID": "GS/GS-Install.html",
    "href": "GS/GS-Install.html",
    "title": "2  Installation",
    "section": "",
    "text": "2.1 Overview\nThis section provides you with a basic idea about this course’s main tool, Python, and a brief instruction of setting up the computational environment (e.g., Python version, libraries, etc.).",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Installation</span>"
    ]
  },
  {
    "objectID": "GS/GS-Install.html#overview",
    "href": "GS/GS-Install.html#overview",
    "title": "1  Installation",
    "section": "",
    "text": "We’ll begin with a brief introduction to key coding concepts.\nThen, you’ll have two options to start coding:\n\nSet up Python on your own computer, or\nUse a popular online cloud-based coding environment.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installation</span>"
    ]
  },
  {
    "objectID": "GS/GS-Install.html#preliminaries",
    "href": "GS/GS-Install.html#preliminaries",
    "title": "2  Installation",
    "section": "2.2 Preliminaries",
    "text": "2.2 Preliminaries\n\n2.2.1 Why Python?\nIn business analytics, learning a programming language is extremely valuable (often essential) since many tasks are more efficiently done with computer and code. Once you become familiar with a programming language and its core concepts (like variables, loops, data structures), learning other languages often becomes easier, though each has its own syntax and paradigm-specific features.\nOnce you become familiar with a language, learning other languages (including ones, like R or C++, specialized to specific tasks) becomes much easier, where most of the programming concepts similarly apply across most languages.\n\n\n\nZDNet’s programming language popularity index as of 2024 (Image source link).\n\n\nHere, Python is one of the most popular programming languages nowadays.\nIt’s one of the easiest to learn. Its simplicity and readability make it a common first language for students, while its deep ecosystem of libraries makes it suitable for advanced tasks.\nAlso, Python is a general-purpose language, meaning it’s highly versatile as it can handle tasks ranging from basic calculations to advanced machine learning (e.g., large language models in ChatGPT) in diverse domains, including data science, web development, automation, engineering, etc., and accordingly across the industry, academia, and the public sector.\nThis combination is why people often say Python has a “low floor and high ceiling.”\nBesides, Python also shines in cloud computing environments, thanks to strong support from cloud providers and its rich ecosystem of open-source packages.\nIn summary, Python is an outstanding language for analytics, data science, and various applications in business. It’s no surprise that it consistently ranks among the most popular programming languages in the world!\n\n\n2.2.2 Related Concepts\nBefore diving into coding, it’s important to understand a computational environment as the setup that allows you to write and run Python code. In this section, let’s briefly understand the key elements of a computational environment for coding.\nA computational environment typically includes elements that together define where and how your code will run:\n\nThe operating system you’re working on (e.g., macOS Catalina)\nThe version of the programming language (e.g., Python 3.11)\nInstalled packages or libraries (e.g., pandas 2.1.0)\n\nTo set up a working Python environment, you’ll need:\n\nA computer with an operating system (e.g., Windows, macOS, Linux, or a cloud-based platform)\nAn installation of Python, the programming language that enables the computer to interpret and execute the code\nVarious packages that extend Python’s capabilities for various tasks\nAnd, an Integrated Development Environment (IDE) that is a tool to write and execute code (in a convient way)\n\nLet’s walk through each of these in more detail.\n\n2.2.2.1 Operating System\nAn operating system (OS) is the core software that manages everything on your computer. It acts like a middleman between your hardware (like CPU, memory, keyboard, etc.) and the applications you use (like a web browser or software). In the context of a Python environment, the OS is the foundation that everything else runs on. It affects how Python is installed, how packages behave, and even how your code interacts with files, memory, and other system resources. To be specific, the OS does the following for Python:\n\nRuns the Python interpreter: The OS loads and executes the Python program when you run a script.\nHandles file paths and directories: Python uses OS-specific paths (e.g., C:\\folder\\file.txt on Windows and /home/user/file.txt on Linux).\nManages packages and dependencies: Some Python packages have OS-specific versions or behaviors.\nSupports external tools: Tools like compilers or system libraries that Python may depend on are managed by the OS.\n\n\n\n\n2.2.2.2 Python interpreter\nA Python interpreter is the program that reads and runs your Python code.\nWhen you write code in Python—like print(\"Hello\"), you’re writing instructions in a human-readable way. But your computer doesn’t understand Python directly. That’s where the interpreter comes in.\nhttps://www.geeksforgeeks.org/python/internal-working-of-python/\nYou write a Python code, and run it. Then, a Python interpreter processes and runs the instructions in your code. To execute Python code, a computer needs , which\nPython is both a programming language that humans can read, and a language that computers can read, interpret, and then carry out instructions based on. For a computer to be able to read and execute Python code, it needs to have a Python interpreter installed. There are lots of ways to install a Python “interpreter” on your own computer, this book recommends the uv distribution of Python for its flexibility, simplicity, and features. Cloud services often come with a Python interpreter installed, and we’ll see shortly how to install one on your own computer.\nIn the box below, you can see which version of Python was used to compile this book:\n\nimport sys\nprint(\"Compiled with Python version:\", sys.version)\n\nCompiled with Python version: 3.11.13 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:03:15) [MSC v.1929 64 bit (AMD64)]\n\n\nThink of it like a translator:\n\nYou (the programmer) write instructions in Python.\nThe interpreter translates those instructions into something the computer can understand and act on.\nThe computer then follows those instructions to do things—like print messages, do math, or analyze data.\nThere are different types of Python interpreters, like:\n\nCPython (the standard and most common one),\nPyPy (faster in some cases),\nor ones that run inside other environments (like Jupyter Notebooks or cloud platforms).\n\n\nWhen you install Python on your computer, you’re actually installing the interpreter along with tools to help you write and run Python programs. Without the interpreter, Python code is just text—the computer wouldn’t know what to do with it.\n\n\n2.2.2.3 Integrated Development Environment (IDE)\nAn integrated development environment (IDE) is a software application that helps you write, test, and debug your code more easily.\nThe most important of these is a way to write the code itself! IDEs are not the only way to programme, but they are perhaps the most useful.\nIf you have used Stata or Matlab, you may not have realised it, but these analytical tools bundle the interpreter and the IDE together. But they are separate things: the interpreter is a way of processing your instructions, the IDE is where you write those instructions.\nThere are a lot of integrated development environments (IDEs) out there, including PyCharm, IDLE, Visual Studio Code. In this course, Jupyter will be used as it works on all major operating systems and is one of the most popular.\nImagine you’re writing a story—an IDE is like a smart writing tool that not only lets you type but also checks your spelling, suggests words, helps you organize chapters, and even lets you publish the book when you’re done. For coding, an IDE does something similar.\nHere’s what an IDE typically includes:\n\nCode editor: where you type your code (like a fancy text editor).\nSyntax highlighting: colors and styles that make your code easier to read.\nAutocomplete: suggests code as you type, helping you write faster and with fewer errors.\nDebugger: lets you pause and inspect your code when something goes wrong.\nTerminal or console: to run your code and see results directly.\n\nSome popular Python IDEs are:\n\nJupyter Notebook: great for data analysis and visualization.\nPyCharm: powerful and feature-rich, good for large projects.\nVisual Studio Code: lightweight, customizable, and widely used.\nIDLE: a simple IDE that comes with Python by default.\n\nIn short, an IDE makes coding more convenient and productive by bringing all the tools you need into one place.\n\n\n2.2.2.4 Python Packages\nA Python package (also called library) is a collection of tools (typically for one-themed topic/goal) such as functions, classes, data, and documentation that help you do specific tasks more easily, without having to write everything from scratch, like a toolbox that has ready-made tools for specific jobs.\nYou may prepare a toolbox and take it out on your workbench when the tools are needed. Similarly, you will install a package and add the package to the setup in your code. By doing so, they give you useful tools that others have already written and tested, so you can focus on solving your problem instead of reinventing the wheel. Eventually, saving your time and effort, you can easily extend the capabilities of an installed basic Python on your computer.\nYou can use Python with extra packages for various tasks such as doing math, analyzing data, making graphs, building websites, or training machine learning models. For example, the math package has functions like math.sqrt() to calculate square roots; the pandas package helps you work with tables of data (like spreadsheets); the matplotlib package is used to make plots and charts; the scikit-learn package offers tools for machine learning.\n\n\n\n2.2.2.5 Typical workflow\nThe typical workflow for analysis with code might be something like this:\n\nOpen up your integrated development environment (IDE)\nWrite some code in a script (a text file with code in) in your IDE\nIf necessary for the analysis that you’re doing, install any extra packages\nUse the IDE to send bits of code from the script, or the entire script, to be executed by Python and add-on packages, and to display results\n(once the project is complete) ensure the script can be run from top to bottom to re-produce your analysis\n\nWe’ll see two ways to achieve this workflow:\n\nInstalling an IDE, a Python interpreter, and any extra Python packages on your own computer\nUsing a computer in the cloud that you access through your internet browser. Cloud computers often have an IDE and Python built-in, and you can easily install extra packages in them too. However, you should be aware that the cloud service we recommend has a 60 hours / month free tier. Beyond this you’ll need to pay for extra hours.\n\nYou should pick whichever you’re more comfortable with! Eventually, you’ll probably try both.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Installation</span>"
    ]
  },
  {
    "objectID": "GS/GS-Install.html#setting-your-own-computer",
    "href": "GS/GS-Install.html#setting-your-own-computer",
    "title": "2  Installation",
    "section": "2.3 Setting Your Own Computer",
    "text": "2.3 Setting Your Own Computer\nThe following instruction is for a python environment on your own computer. The overall installation is as follow:\n\nInstall Anaconda/Conda as a platform for virtual environments\nInstall Python in a virtual environment\nInstall Jupyter Notebook as an IDE in the virtual environment\n\n\n2.3.1 Installing Anaconda (or Conda)\n\n2.3.1.1 Virtual Environment\nDifferent projects often require different environment setups. That’s when a virtual environment comes in handy, which helps to work on isolated Python environments where you can freely create/delete them (with no risk of messing up your entire computer).\nAlso, considering rapid version updates, an easy way to manage a python environment for the compatibility of python and many packages is building it on a virtual environment.\n\n\n\n\n\n\nWhy Use Conda to Install Python?\n\n\n\n\nInstalling Python through Conda (via Anaconda or Miniconda) is highly recommended for this course, especially for users working with data science and analytics tools.\nHere’s why:\n\nIsolated Environments: Conda allows you to create separate environments for different projects. Each environment can have its own version of Python and packages—preventing version conflicts.\nBetter Package Management: Unlike pip (Python’s default package manager), Conda can install not just Python packages but also system-level dependencies like C or Fortran libraries. This is especially helpful for scientific packages like numpy, scipy, and pytorch.\nCross-Platform Compatibility: Conda works consistently across Windows, macOS, and Linux. It is also used by most cloud data science environments, helping ensure reproducibility.\nFast Setup with Anaconda: The Anaconda distribution includes over 250 pre-installed data science packages (e.g., pandas, matplotlib, scikit-learn, jupyterlab), making it ideal for beginners and fast onboarding.\nScientific Computing Support: Many data science and machine learning tools depend on optimized compiled libraries. Conda handles these dependencies more reliably than pip.\nReproducible, portable environments that “just work,” especially in data science: Conda is the most robust choice.\n\n\n\nAnaconda is a popular software for that.\nhttps://www.anaconda.com/docs/getting-started/getting-started\nhttps://www.anaconda.com/docs/getting-started/anaconda/install\n\n\n\n2.3.2 Installing Python in Virtual Environment through Conda\nhttps://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html\nTo install Python, we’re going to use the command-line interface (CLI), because it’s simple to lightly handle environments, packages, and version control via the terminal.\nKnowing a little bit about it is really useful for coding (and more)\n\n\n\n\n\n\nThe Terminal in Brief\n\n\n\nThe command-line interface (also known as the command-line  and sometimes the command prompt) is a text-based interface that allows users to interact with the system (software) via command, each of which is formatted as a line of text, to your computer. In there, a command interpreter (also known as a command-line interpreter, command processor, or shell) is a program that implements a user’s commands via a CLI, acting like a bridge between the user and the operating system’s internal functions.\nMost OS’s (Linux, Mac, and Windows) have a built-in command-line program:\n\nthe PowerShell on Windows.\n\nSearch PowerShell and open it (without activating a conda environment which will be discussed below).\nOr, install PowerShell Prompt in Anaconda and run it in the virtual environment (activating it).\n\nthe Terminal on macOS\n\nSearch “Terminal” and open it\nTerminal User Guide\n\nthe Terminal on Linux\n\nSearch “Terminal” (the default Bash and open it\nThe Linux command line for beginners\nUsingTheTerminal - Ubuntu documantation\n\n\n\n\n\n\n2.3.2.1 Installing Python in Conda Environments\nfollowing command in the terminal, and hit y to proceed\nconda create --name env_buda450 python=3.11\nIn the command, you can replace env_buda450 with the name you want for the environment. By including programs/packages (and its versions) in the command, you can install them together when the environment is created (e.g., python version 3.11).\nCheck more details from Creating an environment with commands.\nThen, let’s activate the created environment env_buda450 by hitting in your terminal.\nconda activate env_buda450\nthen, install python. For a specific version of python (e.g., python 3.13),\nconda install python=3.13\n\n\n2.3.2.2 Installing Python Packages\nPython packages are typically not written by the core developers/maintainers of the Python language, but by anyone, enthusiasts, firms, researchers, academics, and you where anyone can make one, and so they don’t come built-in (by definition) in the Python. Therefore, you need to additionally install them and then import them into your scripts when you use them in a script.\nWithin the Python environment (e.g., after activating your conda environment), you can install a package by executing a commend in your computer terminal.\nFor example, let’s install the numpy package, which provides comprehensive numerical, mathematical operations as a basis of many advanced data science packages, by entering the following commend into the command line in your terminal.\nconda install numpy\nWhen you hit yes y (after the install command), it will automatically download the package (and its required dependent packages) from the internet and install in the appropriate place on your computer.\n\n\n\n\n\n\nNote\n\n\n\nIn this course, we will use conda to install packages because it’s relatively more generic and convenient for tasks dealing with environments and complex dependencies, compared with pip. See the explanation in link for more details.\n\n\nSimilarly, let’s install two other Python packages: pandas for data analysis and manipulation and matplotlib for creating static, animated, and interactive visualizations. To install them together, the packages can be listed with commas:\nconda install pandas, matplotlib\n\n\n\n\n\n\nWarning\n\n\n\nSometimes, the name of a package for installation may differ from its full name or for import.\nFor example, scikit-learn is a package for data mining and machine learning tasks. To install it, we use the name scikit-learn:\nconda install scikit-learn\nHowever, the package is called sklearn.\n\nimport sklearn\nprint(sklearn.__version__)\n\n1.6.1\n\n\nSo, it’s recommended to always check the exact instruction from the original website (e.g., pandas and matplotlib).\n\n\n\n\n\n2.3.3 Installing Jupyter Notebook\n\n2.3.3.1 Other IDEs\nVisual Studio Code is a free and open source IDE from Microsoft that is available on all major operating systems. Just like Python itself, Visual Studio can be extended with packages, and it is those packages, called extensions in this case, that make it so useful. As well as Python, Visual Studio Code supports a ton of other languages.\nThese instructions are for if you wish to code in the cloud rather than on your own computer. There are many ways to do data science in the cloud, but we’re going to share with you the absolute simplest.\nYou can run the code online through a few other options. The first is the easiest to get started with.\n\nGithub Codespaces. Github owned by Microsoft provides a range of services, including a way to back-up code on the cloud, and cloud computing. Github Codespaces is an online cloud computer that you connect to from your browser window. For this, you will need to sign up for a Github Account.\nGoogle Colab notebooks. Free for most use. You can launch most pages in this book interactively by using the ‘Colab’ button under the rocket symbol at the top of most pages in this book. It will be in the form of a notebook (which mixes code and text) rather than a script (.py file) but the code you write is the same. Note that Colab doesn’t use Visual Studio Code.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Installation</span>"
    ]
  },
  {
    "objectID": "BD/BD-Pandas.html",
    "href": "BD/BD-Pandas.html",
    "title": "6  Pandas",
    "section": "",
    "text": "6.1 Overview\nOften, data cannot be used directly for your analysis. For accurate, meaningful outcomes, data have to be prepared, prior to the application of data mining tools.\n“This session offers a guide on handling data in R, focusing on common formats like CSV and Excel. It covers importing data from local files and online data, inspecting data, and exporting data. Also, it includes an introduction to the tibble format from the tidyverse for more advanced data handling.”\nThe following tutorial contains examples of using the numpy and pandas library modules. The notebook can be downloaded from http://www.cse.msu.edu/~ptan/dmbook/tutorials/tutorial2/tutorial2.ipynb. Read the step-by-step instructions below carefully. To execute the code, click on the cell and press the SHIFT-ENTER keys simultaneously.",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "BD/BD-Pandas.html#overview",
    "href": "BD/BD-Pandas.html#overview",
    "title": "6  Pandas",
    "section": "",
    "text": "6.1.0.1 Packages for this section\n\nimport numpy as np\nimport pandas as pd",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "BD/BD-Pandas.html#pandas",
    "href": "BD/BD-Pandas.html#pandas",
    "title": "6  Pandas",
    "section": "6.2 Pandas",
    "text": "6.2 Pandas\nPandas is a Python package that provides powerful and flexible data structures for data analysis and manipulation. Particularly, two main structures, Series and DataFrame, in Pandas are designed to make working with structured data intuitive and efficient.\nSeries is a one-dimensional labeled array capable of holding data of any type (integers, strings, Python objects, etc.), and DataFrame is a two-dimensional labeled array (i.e., matrix) with columns of potentially different types.\n\n6.2.1 Series\nA Series object consists of a one-dimensional array (i.e., vector) of values, whose elements can be referenced using an index array.\nYou can create a Series object from a list, a numpy array, or a Python dictionary. For example, the following shows how to create a Series object with a list.\n\nimport pandas as pd \n\nlist_data = [3.1, 2.4, -1.7, 0.2, -2.9, 4.5]\nsr = pd.Series(list_data)  # creating a series from a list\nprint('sr =\\n', sr, '\\n', sep=\"\")\n\nprint('sr.values =', sr.values)                   # display values of the Series\nprint('sr.index  =', sr.index)                     # display indices of the Series\nprint('sr.dtype  =', sr.dtype)                     # display the element type of the Series\n\nsr =\n0    3.1\n1    2.4\n2   -1.7\n3    0.2\n4   -2.9\n5    4.5\ndtype: float64\n\nsr.values = [ 3.1  2.4 -1.7  0.2 -2.9  4.5]\nsr.index  = RangeIndex(start=0, stop=6, step=1)\nsr.dtype  = float64\n\n\nEach element in a Series is automatically labeled with numeric indices in an ascending order, if no index data were provided. When creating a Series, you can manually set the labels with your labels (e.g., in a list) for the argument index:\n\nlist_index = ['Record2', 'Record3', 'Record4', 'Record5', 'Record6', 'Record7']\nprint('list_index =\\n', list_index, '\\n', sep=\"\")\n\nsr_labeled = pd.Series(list_data, index = list_index)\nprint('sr_labeled =\\n', sr_labeled, '\\n', sep=\"\")\n\nlist_index =\n['Record2', 'Record3', 'Record4', 'Record5', 'Record6', 'Record7']\n\nsr_labeled =\nRecord2    3.1\nRecord3    2.4\nRecord4   -1.7\nRecord5    0.2\nRecord6   -2.9\nRecord7    4.5\ndtype: float64\n\n\n\nAlternatively, you can assign (overwrite) new index data, accessing the Series’s index attribute:\n\nlist_index_new = ['Rec'+str(i) for i in range(0,6)]     # new list of indices\nprint('list_index_new =\\n', list_index_new, '\\n', sep=\"\")\n\nsr_labeled.index = list_index_new                       # assign a new list\nprint('sr_labeled =\\n', sr_labeled, '\\n', sep=\"\")\n\nlist_index_new =\n['Rec0', 'Rec1', 'Rec2', 'Rec3', 'Rec4', 'Rec5']\n\nsr_labeled =\nRec0    3.1\nRec1    2.4\nRec2   -1.7\nRec3    0.2\nRec4   -2.9\nRec5    4.5\ndtype: float64\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a Series object from a numpy object np.arange(3,7) with index ['a', 'b', 'c', 'd']\n\n\nShow the code\narr_data = np.arange(3,7)\narr_index = ['a', 'b', 'c', 'd']\nsr_arr = pd.Series(arr_data, index=arr_index) \nprint('sr_arr =\\n', sr_arr, '\\n', sep=\"\")\n\n\nsr_arr =\na    3\nb    4\nc    5\nd    6\ndtype: int64\n\n\n\nCreate a Series object from a dictionary {'MI': 'Lansing', 'CA': 'Sacramento', 'TX': 'Austin', 'MN': 'St Paul'}.\n\n\nShow the code\ndict_data = {'MI': 'Lansing', 'CA': 'Sacramento', 'TX': 'Austin', 'MN': 'St Paul'}\nsr_dict = pd.Series(dict_data)                # creating a series from dictionary object\nprint('sr_dict =\\n', sr_dict, '\\n', sep=\"\")\n\n\nsr_dict =\nMI       Lansing\nCA    Sacramento\nTX        Austin\nMN       St Paul\ndtype: object\n\n\n\n\n\nYou can apply most of operations/functions for numpy arrays on the Series object, because Series of Pandas is built on top of NumPy arrays and support many similar operations.\nThere are various functions available to find the number of elements in a Series. Result of the function depends on whether null elements are included.\n\nprint(sr_labeled['Rec3'])\n\n0.2\n\n\nA boolean filter can be used to select elements of a Series\n\nprint(sr[sr &gt; 0])   # applying filter to select non-negative elements of the Series\n\n0    3.1\n1    2.4\n3    0.2\n5    4.5\ndtype: float64\n\n\nScalar operations can be performed on elements of a numeric Series\n\nprint('sr      =\\n', sr, '\\n', sep=\"\")\nprint('sr + 1  =\\n', sr + 1, '\\n', sep=\"\")      # addition\nprint('sr - 1  =\\n', sr - 1, '\\n', sep=\"\")      # subtraction\nprint('sr * 2  =\\n', sr * 2, '\\n', sep=\"\")      # multiplication\nprint('sr // 2 =\\n', sr // 2, '\\n', sep=\"\")     # integer division\nprint('sr ** 2 =\\n', sr ** 2, '\\n', sep=\"\")     # square\nprint('sr % 2  =\\n', sr % 2, '\\n', sep=\"\")      # modulo  \nprint('1 / sr  =\\n', 1 / sr, '\\n', sep=\"\")      # division\n\nsr      =\n0    3.1\n1    2.4\n2   -1.7\n3    0.2\n4   -2.9\n5    4.5\ndtype: float64\n\nsr + 1  =\n0    4.1\n1    3.4\n2   -0.7\n3    1.2\n4   -1.9\n5    5.5\ndtype: float64\n\nsr - 1  =\n0    2.1\n1    1.4\n2   -2.7\n3   -0.8\n4   -3.9\n5    3.5\ndtype: float64\n\nsr * 2  =\n0    6.2\n1    4.8\n2   -3.4\n3    0.4\n4   -5.8\n5    9.0\ndtype: float64\n\nsr // 2 =\n0    1.0\n1    1.0\n2   -1.0\n3    0.0\n4   -2.0\n5    2.0\ndtype: float64\n\nsr ** 2 =\n0     9.61\n1     5.76\n2     2.89\n3     0.04\n4     8.41\n5    20.25\ndtype: float64\n\nsr % 2  =\n0    1.1\n1    0.4\n2    0.3\n3    0.2\n4    1.1\n5    0.5\ndtype: float64\n\n1 / sr  =\n0    0.322581\n1    0.416667\n2   -0.588235\n3    5.000000\n4   -0.344828\n5    0.222222\ndtype: float64\n\n\n\nNumpy functions can be applied to pandas Series.\n\n# applying numpy functions to a numeric Series\nprint(\"np.min(sr)  =\", np.min(sr))   # min \nprint(\"np.max(sr)  =\", np.max(sr))   # max \nprint(\"np.mean(sr) =\", np.mean(sr))  # mean/average\nprint(\"np.std(sr)  =\", np.std(sr))   # standard deviation\nprint(\"np.sum(sr)  =\", np.sum(sr))   # sum \n\n# applying numpy functions to a numeric Series\nprint('np.sign(sr)      =\\n', np.sign(sr))          # the sign of each element\nprint('np.abs(sr)       =\\n', np.abs(sr))           # the absolute value of each element\nprint('np.sqrt(abs(sr)) =\\n', np.sqrt(abs(sr)))     # the square root of each element\nprint('np.exp(sr)       =\\n', np.exp(sr))           # the exponentiation\nprint('np.log(sr)       =\\n', np.log(abs(sr)))      # the natural logarithm\nprint('np.sort(sr)      =\\n', np.sort(sr))          # the sorting array\n\n# applying numpy functions to numeric Series'\nss = pd.Series([1,3,5,7,9])\nprint('np.add(sr,ss)          =\\n', np.add(sr,ss))          # element-wise addition  \nprint('np.subtract(sr,ss)     =\\n', np.subtract(sr,ss))     # element-wise subtraction \nprint('np.multiply(sr,ss)     =\\n', np.multiply(sr,ss))     # element-wise multiplication\nprint('np.divide(sr,ss)       =\\n', np.divide(sr,ss))       # element-wise division\nprint('np.floor_divide(sr,ss) =\\n', np.floor_divide(sr,ss)) # element-wise integer division \nprint('np.mod(sr,ss)          =\\n', np.mod(sr,ss))          # element-wise division \nprint('np.power(sr,ss)        =\\n', np.power(sr,ss))        # element-wise exponentiation \nprint('np.maximum(sr,ss)      =\\n', np.maximum(sr,ss))      # element-wise maximum \nprint('np.minimum(sr,ss)      =\\n', np.minimum(sr,ss))      # element-wise minimum \n\nBut Series provide more than NumPy arrays. Some additional (statistically oriented) methods such as\n\nsr.describe()\n\ncount    6.000000\nmean     0.933333\nstd      2.889060\nmin     -2.900000\n25%     -1.225000\n50%      1.300000\n75%      2.925000\nmax      4.500000\ndtype: float64\n\n\nThe value_counts() function can be used for tabulating the counts of each discrete value in the Series.\n\ncolors = pd.Series(['red', 'blue', 'blue', 'yellow', 'red', 'green', 'blue', np.nan])\nprint('colors =\\n', colors, '\\n')\n\nprint('colors.value_counts() =\\n', colors.value_counts())\n\ncolors =\n 0       red\n1      blue\n2      blue\n3    yellow\n4       red\n5     green\n6      blue\n7       NaN\ndtype: object \n\ncolors.value_counts() =\n blue      3\nred       2\nyellow    1\ngreen     1\nName: count, dtype: int64\n\n\n\n\n6.2.2 DataFrame\nA DataFrame object is a tabular, spreadsheet-like data structure containing a collection of columns, each of which can be of different types (numeric, string, boolean, etc). Unlike Series, a DataFrame has distinct row and column indices. There are many ways to create a DataFrame object (e.g., from a dictionary, list of tuples, or even numpy’s ndarrays).\n\nfrom pandas import DataFrame\n\ncars = {'make': ['Ford', 'Honda', 'Toyota', 'Tesla'],\n       'model': ['Taurus', 'Accord', 'Camry', 'Model S'],\n       'MSRP': [27595, 23570, 23495, 68000]}          \ncarData = DataFrame(cars)            # creating DataFrame from dictionary\ncarData                              # display the table\n\n\n\n\n\n\n\n\nmake\nmodel\nMSRP\n\n\n\n\n0\nFord\nTaurus\n27595\n\n\n1\nHonda\nAccord\n23570\n\n\n2\nToyota\nCamry\n23495\n\n\n3\nTesla\nModel S\n68000\n\n\n\n\n\n\n\n\nprint('carData.index =', carData.index)         # print the row indices\nprint('carData.columns =', carData.columns)     # print the column indices\n\ncarData.index = RangeIndex(start=0, stop=4, step=1)\ncarData.columns = Index(['make', 'model', 'MSRP'], dtype='object')\n\n\nInserting columns to an existing dataframe\n\ncarData2 = DataFrame(cars, index = [1,2,3,4])  # change the row index\ncarData2['year'] = 2018    # add column with same value\ncarData2['dealership'] = ['Courtesy Ford','Capital Honda','Spartan Toyota','N/A']\ncarData2                   # display table\n\n\n\n\n\n\n\n\nmake\nmodel\nMSRP\nyear\ndealership\n\n\n\n\n1\nFord\nTaurus\n27595\n2018\nCourtesy Ford\n\n\n2\nHonda\nAccord\n23570\n2018\nCapital Honda\n\n\n3\nToyota\nCamry\n23495\n2018\nSpartan Toyota\n\n\n4\nTesla\nModel S\n68000\n2018\nN/A\n\n\n\n\n\n\n\nCreating DataFrame from a list of tuples.\n\ntuplelist = [(2011,45.1,32.4),(2012,42.4,34.5),(2013,47.2,39.2),\n              (2014,44.2,31.4),(2015,39.9,29.8),(2016,41.5,36.7)]\ncolumnNames = ['year','temp','precip']\nweatherData = DataFrame(tuplelist, columns=columnNames)\nweatherData\n\n\n\n\n\n\n\n\nyear\ntemp\nprecip\n\n\n\n\n0\n2011\n45.1\n32.4\n\n\n1\n2012\n42.4\n34.5\n\n\n2\n2013\n47.2\n39.2\n\n\n3\n2014\n44.2\n31.4\n\n\n4\n2015\n39.9\n29.8\n\n\n5\n2016\n41.5\n36.7\n\n\n\n\n\n\n\nCreating DataFrame from numpy ndarray\n\nimport numpy as np\n\nnpdata = np.random.randn(5,3)  # create a 5 by 3 random matrix\ncolumnNames = ['x1','x2','x3']\ndata = DataFrame(npdata, columns=columnNames)\ndata\n\n\n\n\n\n\n\n\nx1\nx2\nx3\n\n\n\n\n0\n-0.342766\n-0.484111\n2.829542\n\n\n1\n0.340921\n0.879999\n0.708066\n\n\n2\n-0.061290\n-0.342209\n1.254606\n\n\n3\n-0.914594\n-0.057879\n2.185776\n\n\n4\n0.090640\n-1.545489\n1.014667\n\n\n\n\n\n\n\nThere are many ways to access elements of a DataFrame object.\n\n# accessing an entire column will return a Series object\n\nprint(data['x2'])\nprint(type(data['x2']))\n\n0   -0.484111\n1    0.879999\n2   -0.342209\n3   -0.057879\n4   -1.545489\nName: x2, dtype: float64\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n# accessing an entire row will return a Series object\n\nprint('Row 3 of data table:')\nprint(data.iloc[2])       # returns the 3rd row of DataFrame\nprint(type(data.iloc[2]))\n\nprint('\\nRow 3 of car data table:')\nprint(carData2.iloc[2])   # row contains objects of different types\n\nRow 3 of data table:\nx1   -0.061290\nx2   -0.342209\nx3    1.254606\nName: 2, dtype: float64\n&lt;class 'pandas.core.series.Series'&gt;\n\nRow 3 of car data table:\nmake                  Toyota\nmodel                  Camry\nMSRP                   23495\nyear                    2018\ndealership    Spartan Toyota\nName: 3, dtype: object\n\n\n\n# accessing a specific element of the DataFrame\n\nprint('carData2 =\\n', carData2)\n\nprint('\\ncarData2.iloc[1,2] =', carData2.iloc[1,2])                # retrieving second row, third column\nprint('carData2.loc[1,\\'model\\'] =', carData2.loc[1,'model'])    # retrieving second row, column named 'model'\n\n# accessing a slice of the DataFrame\n\nprint('\\ncarData2.iloc[1:3,1:3]=')\nprint(carData2.iloc[1:3,1:3])\n\ncarData2 =\n      make    model   MSRP  year      dealership\n1    Ford   Taurus  27595  2018   Courtesy Ford\n2   Honda   Accord  23570  2018   Capital Honda\n3  Toyota    Camry  23495  2018  Spartan Toyota\n4   Tesla  Model S  68000  2018             N/A\n\ncarData2.iloc[1,2] = 23570\ncarData2.loc[1,'model'] = Taurus\n\ncarData2.iloc[1:3,1:3]=\n    model   MSRP\n2  Accord  23570\n3   Camry  23495\n\n\n\nprint('carData2 =\\n', carData2, '\\n')\n\nprint('carData2.shape =', carData2.shape)\nprint('carData2.size =', carData2.size)\n\ncarData2 =\n      make    model   MSRP  year      dealership\n1    Ford   Taurus  27595  2018   Courtesy Ford\n2   Honda   Accord  23570  2018   Capital Honda\n3  Toyota    Camry  23495  2018  Spartan Toyota\n4   Tesla  Model S  68000  2018             N/A \n\ncarData2.shape = (4, 5)\ncarData2.size = 20\n\n\n\n# selection and filtering\n\nprint('carData2 =\\n', carData2, '\\n')\n\nprint('carData2[carData2.MSRP &gt; 25000] =')  \nprint(carData2[carData2.MSRP &gt; 25000])\n\ncarData2 =\n      make    model   MSRP  year      dealership\n1    Ford   Taurus  27595  2018   Courtesy Ford\n2   Honda   Accord  23570  2018   Capital Honda\n3  Toyota    Camry  23495  2018  Spartan Toyota\n4   Tesla  Model S  68000  2018             N/A \n\ncarData2[carData2.MSRP &gt; 25000] =\n    make    model   MSRP  year     dealership\n1   Ford   Taurus  27595  2018  Courtesy Ford\n4  Tesla  Model S  68000  2018            N/A\n\n\n\n\n6.2.3 Arithmetic Operations\n\nprint(data)\n\nprint('\\nData transpose operation: data.T')\nprint(data.T)    # transpose operation\n\nprint('\\nAddition: data + 4')\nprint(data + 4)    # addition operation\n\nprint('\\nMultiplication: data * 10')\nprint(data * 10)   # multiplication operation\n\n         x1        x2        x3\n0 -0.342766 -0.484111  2.829542\n1  0.340921  0.879999  0.708066\n2 -0.061290 -0.342209  1.254606\n3 -0.914594 -0.057879  2.185776\n4  0.090640 -1.545489  1.014667\n\nData transpose operation: data.T\n           0         1         2         3         4\nx1 -0.342766  0.340921 -0.061290 -0.914594  0.090640\nx2 -0.484111  0.879999 -0.342209 -0.057879 -1.545489\nx3  2.829542  0.708066  1.254606  2.185776  1.014667\n\nAddition: data + 4\n         x1        x2        x3\n0  3.657234  3.515889  6.829542\n1  4.340921  4.879999  4.708066\n2  3.938710  3.657791  5.254606\n3  3.085406  3.942121  6.185776\n4  4.090640  2.454511  5.014667\n\nMultiplication: data * 10\n         x1         x2         x3\n0 -3.427665  -4.841110  28.295417\n1  3.409210   8.799992   7.080655\n2 -0.612897  -3.422092  12.546057\n3 -9.145940  -0.578791  21.857755\n4  0.906400 -15.454887  10.146665\n\n\n\nprint('data =\\n', data)\n\ncolumnNames = ['x1','x2','x3']\ndata2 = DataFrame(np.random.randn(5,3), columns=columnNames)\nprint('\\ndata2 =')\nprint(data2)\n\nprint('\\ndata + data2 = ')\nprint(data.add(data2))\n\nprint('\\ndata * data2 = ')\nprint(data.mul(data2))\n\ndata =\n          x1        x2        x3\n0 -0.342766 -0.484111  2.829542\n1  0.340921  0.879999  0.708066\n2 -0.061290 -0.342209  1.254606\n3 -0.914594 -0.057879  2.185776\n4  0.090640 -1.545489  1.014667\n\ndata2 =\n         x1        x2        x3\n0  0.278633  1.075644  0.013899\n1  1.719669  0.460085  0.446007\n2  0.093832 -0.385358 -1.009484\n3 -1.675789 -2.238411  1.274729\n4  0.935024 -0.915217 -0.938130\n\ndata + data2 = \n         x1        x2        x3\n0 -0.064133  0.591533  2.843440\n1  2.060590  1.340084  1.154072\n2  0.032542 -0.727567  0.245122\n3 -2.590383 -2.296290  3.460504\n4  1.025664 -2.460706  0.076537\n\ndata * data2 = \n         x1        x2        x3\n0 -0.095506 -0.520731  0.039327\n1  0.586271  0.404875  0.315802\n2 -0.005751  0.131873 -1.266504\n3  1.532667  0.129557  2.786270\n4  0.084751  1.414458 -0.951889\n\n\n\nprint(data.abs())    # get the absolute value for each element\n\nprint('\\nMaximum value per column:')\nprint(data.max())    # get maximum value for each column\n\nprint('\\nMinimum value per row:')\nprint(data.min(axis=1))    # get minimum value for each row\n\nprint('\\nSum of values per column:')\nprint(data.sum())    # get sum of values for each column\n\nprint('\\nAverage value per row:')\nprint(data.mean(axis=1))    # get average value for each row\n\nprint('\\nCalculate max - min per column')\nf = lambda x: x.max() - x.min()\nprint(data.apply(f))\n\nprint('\\nCalculate max - min per row')\nf = lambda x: x.max() - x.min()\nprint(data.apply(f, axis=1))\n\n         x1        x2        x3\n0  0.342766  0.484111  2.829542\n1  0.340921  0.879999  0.708066\n2  0.061290  0.342209  1.254606\n3  0.914594  0.057879  2.185776\n4  0.090640  1.545489  1.014667\n\nMaximum value per column:\nx1    0.340921\nx2    0.879999\nx3    2.829542\ndtype: float64\n\nMinimum value per row:\n0   -0.484111\n1    0.340921\n2   -0.342209\n3   -0.914594\n4   -1.545489\ndtype: float64\n\nSum of values per column:\nx1   -0.887089\nx2   -1.549689\nx3    7.992655\ndtype: float64\n\nAverage value per row:\n0    0.667555\n1    0.642995\n2    0.283702\n3    0.404434\n4   -0.146727\ndtype: float64\n\nCalculate max - min per column\nx1    1.255515\nx2    2.425488\nx3    2.121476\ndtype: float64\n\nCalculate max - min per row\n0    3.313653\n1    0.539078\n2    1.596815\n3    3.100370\n4    2.560155\ndtype: float64\n\n\nThe value_counts() function can also be applied to a pandas DataFrame\n\nobjects = {'shape': ['circle', 'square', 'square', 'square', 'circle', 'rectangle'],\n           'color': ['red', 'red', 'red', 'blue', 'blue', 'blue']}\n\nshapeData = DataFrame(objects)\nprint('shapeData =\\n', shapeData, '\\n')\n\nprint('shapeData.value_counts() =\\n', shapeData.value_counts().sort_values())\n\nshapeData =\n        shape color\n0     circle   red\n1     square   red\n2     square   red\n3     square  blue\n4     circle  blue\n5  rectangle  blue \n\nshapeData.value_counts() =\n shape      color\ncircle     blue     1\n           red      1\nrectangle  blue     1\nsquare     blue     1\n           red      2\nName: count, dtype: int64\n\n\n\n\n6.2.4 Plotting Series and DataFrame\nThere are many built-in functions available to plot the data stored in a Series or a DataFrame.\n(a) Line plot\n\nimport matplotlib.pyplot as plt\n\ns3 = pd.Series([1.2,2.5,-2.2,3.1,-0.8,-3.2,1.4], \n            index = ['Jan 1','Jan 2','Jan 3','Jan 4','Jan 5','Jan 6','Jan 7'])\ns3.plot(kind='line', title='Line plot')\n\n\n\n\n\n\n\n\n(b) Bar plot\n\ns3.plot(kind='bar', title='Bar plot')\n\n\n\n\n\n\n\n\n(c) Histogram\n\ns3.plot(kind='hist', title = 'Histogram')\n\n\n\n\n\n\n\n\n(d) Box plot\n\ntuplelist = [(2011,45.1,32.4),(2012,42.4,34.5),(2013,47.2,39.2),\n              (2014,44.2,31.4),(2015,39.9,29.8),(2016,41.5,36.7)]\ncolumnNames = ['year','temp','precip']\nweatherData = DataFrame(tuplelist, columns=columnNames)\nweatherData[['temp','precip']].plot(kind='box', title='Box plot')\n\n\n\n\n\n\n\n\n(e) Scatter plot\n\nprint('weatherData =\\n', weatherData)\n\nweatherData.plot(kind='scatter', x='temp', y='precip')\n\nweatherData =\n    year  temp  precip\n0  2011  45.1    32.4\n1  2012  42.4    34.5\n2  2013  47.2    39.2\n3  2014  44.2    31.4\n4  2015  39.9    29.8\n5  2016  41.5    36.7\n\n\n\n\n\n\n\n\n\n\n6.2.4.1 Related Reading/Reference\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "BD/BD-NumPy.html",
    "href": "BD/BD-NumPy.html",
    "title": "5  NumPy",
    "section": "",
    "text": "5.1 Overview\nOften, data cannot be used directly for your analysis. For accurate, meaningful outcomes, data have to be prepared, prior to the application of data mining tools.\n“This session offers a guide on handling data in R, focusing on common formats like CSV and Excel. It covers importing data from local files and online data, inspecting data, and exporting data. Also, it includes an introduction to the tibble format from the tidyverse for more advanced data handling.”\nThe following tutorial contains examples of using the numpy and pandas library modules. The notebook can be downloaded from http://www.cse.msu.edu/~ptan/dmbook/tutorials/tutorial2/tutorial2.ipynb. Read the step-by-step instructions below carefully. To execute the code, click on the cell and press the SHIFT-ENTER keys simultaneously.",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "BD/BD-NumPy.html#overview",
    "href": "BD/BD-NumPy.html#overview",
    "title": "5  NumPy",
    "section": "",
    "text": "5.1.0.1 Packages for this section\n\nimport numpy as np",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "BD/BD-NumPy.html#numpy",
    "href": "BD/BD-NumPy.html#numpy",
    "title": "5  NumPy",
    "section": "5.2 Numpy",
    "text": "5.2 Numpy\nNumpy, which stands for numerical Python, is a Python package that supports numerical computations. The basic data structure in Numpy is a multi-dimensional array object called ndarray. Numpy provides a suite of functions that can efficiently manipulate elements of the ndarray.\n\n5.2.1 Creating ndarray\n\n5.2.1.1 From data objects\nAn ndarray can be created from a list. For example, a 1-dimensional numpy array (i.e., vector) can be defined in a oneDim_array object, using a list oneDim_list as follows.\n\noneDim_list = [1.0,2,3,4,5]\noneDim_array = np.array(oneDim_list)        # a 1-dimensional array (vector)\nprint(type(oneDim_array))\nprint(oneDim_array)\n\n&lt;class 'numpy.ndarray'&gt;\n[1. 2. 3. 4. 5.]\n\n\nThe basic information about a ndarray object can be retrieved from its attributes. Here are some attributes1 frequently used:\n\nprint(\"No. Dimensions \\t=\", oneDim_array.ndim)\nprint(\"Dimension \\t=\", oneDim_array.shape)\nprint(\"Size \\t\\t=\", oneDim_array.size)\nprint(\"Array type \\t=\", oneDim_array.dtype)\n\nNo. Dimensions  = 1\nDimension   = (5,)\nSize        = 5\nArray type  = float64\n\n\nSimilarly, it is possible to create a multi-dimensional array.\n\ntwoDim_list = [[1,2],[3,4],[5,6],[7,8]]\ntwoDim_array_list = np.array(twoDim_list)    # a two-dimensional array (matrix)\nprint(twoDim_array_list)\n\n[[1 2]\n [3 4]\n [5 6]\n [7 8]]\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCheck the four attributes of the twoDim_array.\n\n\nShow the code\nprint(\"No. Dimensions \\t=\", twoDim_array_list.ndim)\nprint(\"Dimension \\t=\", twoDim_array_list.shape)\nprint(\"Size \\t\\t=\", twoDim_array_list.size)\nprint(\"Array type \\t=\", twoDim_array_list.dtype)\n\n\nNo. Dimensions  = 2\nDimension   = (4, 2)\nSize        = 8\nArray type  = int64\n\n\n\n\nBesides, the ndarray object can be created from a tuple object as well:\n\ntwoDim_tuple = [(1,'a',3.0),(2,'b',3.5)]\ntwoDim_array_tuple = np.array(twoDim_tuple)  # create ndarray from tuple\nprint(twoDim_array_tuple)\nprint(\"No. Dimensions \\t=\", twoDim_array_tuple.ndim)\nprint(\"Dimension \\t=\", twoDim_array_tuple.shape)\nprint(\"Size \\t\\t=\", twoDim_array_tuple.size)\nprint(\"Array type \\t=\", twoDim_array_tuple.dtype)\n\n[['1' 'a' '3.0']\n ['2' 'b' '3.5']]\nNo. Dimensions  = 2\nDimension   = (2, 3)\nSize        = 6\nArray type  = &lt;U32\n\n\n\n\n5.2.1.2 From functions\nndarray can be created by using a function in numpy as a result of its operation. Here are some functions that output new 1-dim ndarray objects.\n\nprint('Array of random numbers from a uniform distribution')\nprint(np.random.rand(5))      # random numbers from a uniform distribution between [0,1]\n\nprint('\\nArray of random numbers from a normal distribution')\nprint(np.random.randn(5))     # random numbers from a normal distribution\n\nprint('\\nArray of integers between -10 and 10, with step size of 2')\nprint(np.arange(-10,10,2))    # similar to range, but returns ndarray instead of list\n\nprint('\\nArray of values between 0 and 1, split into 10 equally spaced values')\nprint(np.linspace(0,1,10))    # split interval [0,1] into 10 equally separated values\n\nprint('\\nArray of values from 10^-3 to 10^3')\nprint(np.logspace(-3,3,7))    # create ndarray with values from 10^-3 to 10^3\n\nArray of random numbers from a uniform distribution\n[0.4535809  0.11339805 0.42981962 0.81590168 0.33055617]\n\nArray of random numbers from a normal distribution\n[-0.62594711 -0.40873728 -0.26805797 -1.88896438 -0.36221787]\n\nArray of integers between -10 and 10, with step size of 2\n[-10  -8  -6  -4  -2   0   2   4   6   8]\n\nArray of values between 0 and 1, split into 10 equally spaced values\n[0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556\n 0.66666667 0.77777778 0.88888889 1.        ]\n\nArray of values from 10^-3 to 10^3\n[1.e-03 1.e-02 1.e-01 1.e+00 1.e+01 1.e+02 1.e+03]\n\n\nAnd, here are some functions that output new 2-dim ndarray objects.\n\nprint('A 2 x 3 matrix of zeros')\nprint(np.zeros((2,3)))        # a matrix of zeros\n\nprint('\\nA 3 x 2 matrix of ones')\nprint(np.ones((3,2)))         # a matrix of ones\n\nprint('\\nA 3 x 3 identity matrix')\nprint(np.eye(3))              # a 3 x 3 identity matrix\n\nA 2 x 3 matrix of zeros\n[[0. 0. 0.]\n [0. 0. 0.]]\n\nA 3 x 2 matrix of ones\n[[1. 1.]\n [1. 1.]\n [1. 1.]]\n\nA 3 x 3 identity matrix\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\nYou can change the shape of such an object, using reshape function:\n\nprint('\\n2-dimensional array of integers from 0 to 11')\noneDim_example = np.arange(12)\n\nprint('\\nbefore reshape:')\nprint(oneDim_example)\n\nprint('\\nafter reshape:')\nprint(np.reshape(oneDim_example, (3,4)))  # reshape to a matrix\n\n\n2-dimensional array of integers from 0 to 11\n\nbefore reshape:\n[ 0  1  2  3  4  5  6  7  8  9 10 11]\n\nafter reshape:\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n\n\n\n\n\n\n\n\nMethod\n\n\n\nUsing a method associated with ndarray, you can get the same result, but through a compact/intuitive code:\n\nprint( np.reshape(oneDim_example, (3,4)) )    # using the reshape function\nprint( oneDim_example.reshape(3,4) )          # using the reshape method\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n\n\n\n\n\n\n\n5.2.2 Element-wise Operations\nYou can apply standard arithmetic operators such as addition and multiplication on each element of the ndarray.\n\nx = np.array([1,2,3,4,5])\n\nprint('x \\t=', x)\nprint('x + 1 \\t=', x + 1)       # addition\nprint('x - 1 \\t=', x - 1)       # subtraction\nprint('x * 2 \\t=', x * 2)       # multiplication\nprint('x // 2 \\t=', x // 2)     # integer division\nprint('x ** 2 \\t=', x ** 2)     # square\nprint('x % 2 \\t=', x % 2)       # modulo  \nprint('1 / x \\t=', 1 / x)       # division\n\nx   = [1 2 3 4 5]\nx + 1   = [2 3 4 5 6]\nx - 1   = [0 1 2 3 4]\nx * 2   = [ 2  4  6  8 10]\nx // 2  = [0 1 1 2 2]\nx ** 2  = [ 1  4  9 16 25]\nx % 2   = [1 0 1 0 1]\n1 / x   = [1.         0.5        0.33333333 0.25       0.2       ]\n\n\nSuch operators can be applied to the operation between two ndarray objects when they have the same shape.\n\nx = np.array([2,4,6,8,10])\ny = np.array([1,2,3,4,5])\n\nprint('x      =', x)\nprint('y      =', y)\n\nprint('x + y  =', x + y)      # element-wise addition\nprint('x - y  =', x - y)      # element-wise subtraction\nprint('x * y  =', x * y)      # element-wise multiplication \nprint('x / y  =', x / y)      # element-wise division\n\nprint('x // y =', x // y)     # element-wise integer division\nprint('x & y  =', x % y)      # element-wise modulo operation \nprint('x ** y =', x ** y)     # element-wise exponentiation\n\nx      = [ 2  4  6  8 10]\ny      = [1 2 3 4 5]\nx + y  = [ 3  6  9 12 15]\nx - y  = [1 2 3 4 5]\nx * y  = [ 2  8 18 32 50]\nx / y  = [2. 2. 2. 2. 2.]\nx // y = [2 2 2 2 2]\nx & y  = [0 0 0 0 0]\nx ** y = [     2     16    216   4096 100000]\n\n\n\n\n5.2.3 Indexing and Slicing\nThere are various ways to select and manage a subset of elements within a ndarray.\nTo access elements of an ndarray, you can use indices (e.g., row or column numbers). The following examples illustrate the indexing elements of 1-dim and 2-dim ndarrays.\n\n# 1-dim example\nmy1d_list = [0,1,2,3,4,5,6,7,8]          # a 1-dim list\nmy1d_arr = np.array(my1d_list)\nprint('my1d_arr =\\n', my1d_arr, '\\n')\n\nprint('my1d_arr[2] =', my1d_arr[2])      # access the third element\nprint('my1d_arr[:2] =', my1d_arr[:2])    # access the first two elements\nprint('my1d_arr[2:] =', my1d_arr[2:])    # access all from the third element to the end\n\nmy1d_arr =\n [0 1 2 3 4 5 6 7 8] \n\nmy1d_arr[2] = 2\nmy1d_arr[:2] = [0 1]\nmy1d_arr[2:] = [2 3 4 5 6 7 8]\n\n\n\n# 2-dim example\nmy2d_list = [[1,2,3,4],[5,6,7,8],[9,10,11,12]] # a 2-dim list\nmy2d_arr = np.array(my2d_list)\nprint('my2darr =\\n', my2d_arr, '\\n')\n\nprint('my2d_arr[2][:] =', my2d_arr[2][:])      # access the third row\nprint('my2d_arr[:][2] =', my2d_arr[:][2])      # access the third row (similar to 2d list)\n\nprint('my2d_arr[2,:] =', my2d_arr[2,:])        # access the third row\nprint('my2d_arr[:,2] =', my2d_arr[:,2])        # access the third column\nprint('my2d_arr[:2,2:] =\\n', my2d_arr[:2,2:])  # access the elements on the first two rows \n                                               #  and the columns from the third to the end\n\nmy2darr =\n [[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]] \n\nmy2d_arr[2][:] = [ 9 10 11 12]\nmy2d_arr[:][2] = [ 9 10 11 12]\nmy2d_arr[2,:] = [ 9 10 11 12]\nmy2d_arr[:,2] = [ 3  7 11]\nmy2d_arr[:2,2:] =\n [[3 4]\n [7 8]]\n\n\nAlso, indexing can be based on a list of direct element indices.\n\n# 1-dim example\nprint('my1d_arr =\\n', my1d_arr, '\\n')\n\nindices = [2,1,0,3]                                # element indices to be used for indexing\nprint('indices =', indices, '\\n')\nprint('my1d_arr[indices] =\\n', my1d_arr[indices])  # this will shuffle the rows of my1d_arr\n\nmy1d_arr =\n [0 1 2 3 4 5 6 7 8] \n\nindices = [2, 1, 0, 3] \n\nmy1d_arr[indices] =\n [2 1 0 3]\n\n\n\n# 2-dim example\nprint('my2d_arr =\\n', my2d_arr)\n\nrowIndex = [0,0,1,2,0]     # row index into my2d_arr\nprint('\\nrowIndex =', rowIndex)\n\ncolumnIndex = [0,2,0,1,2]  # column index into my2d_arr\nprint('columnIndex =', columnIndex, '\\n')\n\nprint('my2d_arr[rowIndex,columnIndex] =', my2d_arr[rowIndex,columnIndex])\n\nmy2d_arr =\n [[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\nrowIndex = [0, 0, 1, 2, 0]\ncolumnIndex = [0, 2, 0, 1, 2] \n\nmy2d_arr[rowIndex,columnIndex] = [ 1  3  5 10  3]\n\n\nBesides, Numpy ndarrays support Boolean (e.g., [True, False, True] to choose the 1st and 3rd in a list of three elements) indexing too.\n\n# 1-dim example\nprint('my1d_arr =\\n', my1d_arr, '\\n')\n\nmy1d_divBy3 = my1d_arr[my1d_arr % 3 == 0]\nprint('To return the elements of zero remainder:')\nprint('my1d_arr[ my1d_arr % 3 == 0 ] =\\n', my1d_divBy3)       # returns all the elements divisible by 3 in an ndarray\n\nmy1d_arr =\n [0 1 2 3 4 5 6 7 8] \n\nTo return the elements of zero remainder:\nmy1d_arr[ my1d_arr % 3 == 0 ] =\n [0 3 6]\n\n\n\n# 2-dim example\nprint('my2d_arr =\\n', my2d_arr)\n\nmy2d_divBy3 = my2d_arr[my2d_arr % 3 == 0]\nprint('\\nmy2d_arr[my2d_arr % 3 == 0] =', my2d_divBy3)         # returns all the elements divisible by 3 in an ndarray\n\nmy2d_divBy3LastRow = my2d_arr[2:, my2d_arr[2,:] % 3 == 0]\nprint('my2d_arr[2:, my2d_arr[2,:] % 3 == 0] =', my2d_divBy3LastRow)   # returns elements in the last row divisible by 3\n\nmy2d_arr =\n [[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\nmy2d_arr[my2d_arr % 3 == 0] = [ 3  6  9 12]\nmy2d_arr[2:, my2d_arr[2,:] % 3 == 0] = [[ 9 12]]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAssigning an array (or a subset of its elements) to another variable will simply pass a reference to the array instead of copying its values. Therefore, when the modification of the values in the new variable is linked to the modification of the values in the original variable.\n\nx = np.arange(-5,5)\nprint('Given x =', x)\n\ny = x[3:5]                 # y is a slice, i.e., pointer to a subarray in x\nprint('      y =', y, '\\n')\n\nprint('Modification in Y')\ny[:] = 777                 # assigning new values in y\nprint('  new y =', y)\nprint('      x =', x, '\\n')    # the new values in y will change x\n\nGiven x = [-5 -4 -3 -2 -1  0  1  2  3  4]\n      y = [-2 -1] \n\nModification in Y\n  new y = [777 777]\n      x = [ -5  -4  -3 777 777   0   1   2   3   4] \n\n\n\nTo create an ndarray that is independent of the orignial reference, you need to make a copy by explicitly calling the .copy() function.\n\nz = x[3:5].copy()   # makes a copy of the subarray\nprint('Before: x =', x)\nprint('        z =', z)\nz[:] = 500          # modifying the value of z will not affect x\nprint('After : z =', z)\nprint('        x =', x)\n\nBefore: x = [ -5  -4  -3 777 777   0   1   2   3   4]\n        z = [777 777]\nAfter : z = [500 500]\n        x = [ -5  -4  -3 777 777   0   1   2   3   4]\n\n\n\n\n\n\n5.2.4 Arithmetic and Statistical Functions\nNumpy provides various built-in mathematical functions available for manipulating elements of an ndarray.\nFirst, there are functions that return a single-value answer from the operation with the values in a ndarray:\n\nx = np.array([-1.452, 0.410, -3.287, 2.539])    \nprint('x            =', x, '\\n')\n\nprint(\"np.min(x)  =\", np.min(x))   # min \nprint(\"np.max(x)  =\", np.max(x))   # max \nprint(\"np.mean(x) =\", np.mean(x))  # mean/average\nprint(\"np.std(x)  =\", np.std(x))   # standard deviation\nprint(\"np.sum(x)  =\", np.sum(x))   # sum \n\nx            = [-1.452  0.41  -3.287  2.539] \n\nnp.min(x)  = -3.287\nnp.max(x)  = 2.539\nnp.mean(x) = -0.4474999999999999\nnp.std(x)  = 2.163692965741674\nnp.sum(x)  = -1.7899999999999996\n\n\nAlso, some other functions return an array of the same shape, applying element-wise operations:\n\nprint('x =', x, '\\n')\n\nprint('np.sign(x)      =', np.sign(x))          # the sign of each element\nprint('np.abs(x)       =', np.abs(x))           # the absolute value of each element\nprint('np.sqrt(abs(x)) =', np.sqrt(abs(x)))     # the square root of each element\nprint('np.exp(x)       =', np.exp(x))           # the exponentiation\nprint('np.log(x)       =', np.log(abs(x)))      # the natural logarithm\nprint('np.sort(x)      =', np.sort(x))          # the sorting array\n\nx = [-1.452  0.41  -3.287  2.539] \n\nnp.sign(x)      = [-1.  1. -1.  1.]\nnp.abs(x)       = [1.452 0.41  3.287 2.539]\nnp.sqrt(abs(x)) = [1.20498963 0.64031242 1.81300855 1.59342399]\nnp.exp(x)       = [ 0.23410162  1.50681779  0.03736578 12.66699764]\nnp.log(x)       = [ 0.37294192 -0.89159812  1.18997529  0.9317703 ]\nnp.sort(x)      = [-3.287 -1.452  0.41   2.539]\n\n\nIn addition, functions for many other mathematical operations are available:\n\ny = np.arange(2,6)\nprint('x =', x)\nprint('y =', y, '\\n')\n\nprint('np.add(x,y)          =', np.add(x,y))          # element-wise addition          x + y\nprint('np.subtract(x,y)     =', np.subtract(x,y))     # element-wise subtraction       x - y\nprint('np.multiply(x,y)     =', np.multiply(x,y))     # element-wise multiplication    x * y\nprint('np.divide(x,y)       =', np.divide(x,y))       # element-wise division          x / y\n\nprint('np.floor_divide(x,y) =', np.floor_divide(x,y)) # element-wise integer division  x // y\nprint('np.mod(x,y)          =', np.mod(x,y))          # element-wise division          x % y\nprint('np.power(x,y)        =', np.power(x,y))        # element-wise exponentiation    x ** y\n\nprint('np.maximum(x,y)      =', np.maximum(x,y))      # element-wise maximum           max(x,y)\nprint('np.minimum(x,y)      =', np.minimum(x,y))      # element-wise minimum           min(x,y)\n\nx = [-1.452  0.41  -3.287  2.539]\ny = [2 3 4 5] \n\nnp.add(x,y)          = [0.548 3.41  0.713 7.539]\nnp.subtract(x,y)     = [-3.452 -2.59  -7.287 -2.461]\nnp.multiply(x,y)     = [ -2.904   1.23  -13.148  12.695]\nnp.divide(x,y)       = [-0.726       0.13666667 -0.82175     0.5078    ]\nnp.floor_divide(x,y) = [-1.  0. -1.  0.]\nnp.mod(x,y)          = [0.548 0.41  0.713 2.539]\nnp.power(x,y)        = [2.10830400e+00 6.89210000e-02 1.16734389e+02 1.05514830e+02]\nnp.maximum(x,y)      = [2. 3. 4. 5.]\nnp.minimum(x,y)      = [-1.452  0.41  -3.287  2.539]\n\n\n\n\n5.2.5 Linear algebra\nNumpy provides many functions to support linear algebra operations.\n\nX = np.random.randn(2,3)                       # create a 2 x 3 random matrix\nprint('X =\\n', X, '\\n')\nprint('Transpose of X, X.T =\\n', X.T, '\\n')    # matrix transpose operation X^T\n\ny = np.random.randn(3) # random vector \nprint('y =', y, '\\n')\n\nprint('Matrix-vector multiplication')\nprint('X.dot(y) =\\n', X.dot(y), '\\n')          # matrix-vector multiplication  X * y\n\nprint('Matrix-matrix product')\nprint('X.dot(X.T) =', X.dot(X.T))              # matrix-matrix multiplication  X * X^T\nprint('\\nX.T.dot(X) =\\n', X.T.dot(X))          # matrix-matrix multiplication  X^T * X\n\nX =\n [[-1.50524155 -1.01909084 -0.29388291]\n [-0.62857548 -2.41681596  0.60981613]] \n\nTranspose of X, X.T =\n [[-1.50524155 -0.62857548]\n [-1.01909084 -2.41681596]\n [-0.29388291  0.60981613]] \n\ny = [ 1.31662873 -1.73096918  1.34826517] \n\nMatrix-vector multiplication\nX.dot(y) =\n [-0.61406151  4.17802727] \n\nMatrix-matrix product\nX.dot(X.T) = [[3.39066542 3.22989839]\n [3.22989839 6.60798223]]\n\nX.T.dot(X) =\n [[ 2.66085925  3.05312912  0.0590493 ]\n [ 3.05312912  6.87954552 -1.17431998]\n [ 0.0590493  -1.17431998  0.45824288]]\n\n\n\nX = np.random.randn(5,3)\nprint('X =\\n', X, '\\n')\n\nC = X.T.dot(X)               # C = X^T * X is a square matrix\nprint('C = X.T.dot(X) =\\n', C, '\\n')\n\ninvC = np.linalg.inv(C)      # inverse of a square matrix\nprint('Inverse of C = np.linalg.inv(C)\\n', invC, '\\n')\n\ndetC = np.linalg.det(C)      # determinant of a square matrix\nprint('Determinant of C = np.linalg.det(C) =', detC)\n\nS, U = np.linalg.eig(C)      # eigenvalue S and eigenvector U of a square matrix\nprint('Eigenvalues of C =\\n', S)\nprint('Eigenvectors of C =\\n', U)\n\nX =\n [[-0.07854639 -0.18564074 -0.75011234]\n [-0.61976432 -0.27966369  0.92352768]\n [ 1.00253456 -1.10338594  0.67986369]\n [-0.41192426  0.88047011 -0.41541338]\n [-2.5176366  -1.2061408  -1.71836145]] \n\nC = X.T.dot(X) =\n [[7.90352854 1.75566168 4.66546448]\n [1.75566168 3.56013805 0.83764901]\n [4.66546448 0.83764901 5.00312089]] \n\nInverse of C = np.linalg.inv(C)\n [[ 0.30532275 -0.08700582 -0.2701498 ]\n [-0.08700582  0.31720017  0.02802654]\n [-0.2701498   0.02802654  0.44710051]] \n\nDeterminant of C = np.linalg.det(C) = 56.039535292086214\nEigenvalues of C =\n [11.78351084  1.48869562  3.19458103]\nEigenvectors of C =\n [[ 0.78908465  0.61202867  0.05259584]\n [ 0.22662517 -0.21046287 -0.9509713 ]\n [ 0.57095223 -0.7623164   0.30477412]]\n\n\n\n5.2.5.1 Related Reading/Reference\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "BD/BD-NumPy.html#footnotes",
    "href": "BD/BD-NumPy.html#footnotes",
    "title": "5  NumPy",
    "section": "",
    "text": "Find more from the official document.↩︎",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "BD/BD-Inspect.html#preliminaries",
    "href": "BD/BD-Inspect.html#preliminaries",
    "title": "7  Data Inspection",
    "section": "7.2 Preliminaries",
    "text": "7.2 Preliminaries\n\n7.2.1 Packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n7.2.2 Data\nLet’s start by creating a DataFrame object for three attributes X1, X2, X3 using pandas.DataFrame():\n\ndf = pd.DataFrame(\n  {\n    \"X1\": [1, 2, 3, 5, 4, 3],\n    \"X2\": [\"Neutral\", \"Satisfied\", \"Dissatisfied\", \"Dissatisfied\", \"Satisfied\", \"Satisfied\"],\n    \"X3\": [0.5, None, 0.2, 0.1, 0.9, np.nan]\n  }\n)",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Inspection</span>"
    ]
  },
  {
    "objectID": "BD/BD-Inspect.html#related-readingreference",
    "href": "BD/BD-Inspect.html#related-readingreference",
    "title": "7  Data Inspection",
    "section": "7.5 Related Reading/Reference",
    "text": "7.5 Related Reading/Reference\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Inspection</span>"
    ]
  },
  {
    "objectID": "BD/BD-Integrate.html#preliminaries",
    "href": "BD/BD-Integrate.html#preliminaries",
    "title": "7  Data Integration",
    "section": "7.2 Preliminaries",
    "text": "7.2 Preliminaries\n\n7.2.1 Packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n7.2.2 Data\nLet’s consider two datasets, each of which has different attributes.\n\ndf1 = pd.DataFrame(\n  {\n    \"X1\": [1, 2, 4, 4, 5],\n    \"X2\": [\"Very Satisfied\", \"Satisfied\", \"Neutral\", \"Dissatisfied\", \"Very Dissatisfied\"],\n    \"X3\": [0.5, 0.8, 0.8, 0.2, None]  # last value is missing\n  }, \n  index = [\"rec1\", \"rec2\", \"rec3\", \"rec4\", \"rec5\"]\n)\n\ndf2 = pd.DataFrame(\n  {\n    \"X4\": [50, 40, 30, 20, 10],\n    \"X5\": [True, False, False, False, True]\n  },\n  index = [\"rec1\", \"rec2\", \"rec3\", \"rec5\", \"rec6\"]\n  \n)\n\nNote that the data df1 and df2 have some records for the common objects (i.e., rec1, rec2, rec3, rec5) but different attributes.",
    "crumbs": [
      "Basics in Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Integration</span>"
    ]
  },
  {
    "objectID": "App/App-Proximity.html",
    "href": "App/App-Proximity.html",
    "title": "Appendix A — Proximity",
    "section": "",
    "text": "A.1 Overview\nProximity measures—including similarity and dissimilarity (e.g., distance—are essential not only for data analysis but also as core components in many data mining algorithms. This session introduces methods for measuring proximity between data objects with single or multiple attributes, tailored to the type of data. For nominal attributes, techniques such as the Simple Matching Coefficient and Jaccard Index are applied, while numerical attributes rely on distance measures like Euclidean, Manhattan, and Minkowski distances. To ensure meaningful comparisons across attributes with different scales, normalization techniques such as standardization and min-max rescaling are also discussed.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Proximity</span>"
    ]
  },
  {
    "objectID": "App/App-Proximity.html#preliminaries",
    "href": "App/App-Proximity.html#preliminaries",
    "title": "Appendix A — Proximity",
    "section": "A.2 Preliminaries",
    "text": "A.2 Preliminaries\n\nA.2.1 Packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ntibble, dplyr\nproxy, factoextra, philentropy for proximity calculations\nscales, datawizard for data preprocessing (rescaling)\n\n\n\nA.2.2 Data\nXXX",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Proximity</span>"
    ]
  },
  {
    "objectID": "App/App-Proximity.html#measures-for-single-attribute-data",
    "href": "App/App-Proximity.html#measures-for-single-attribute-data",
    "title": "Appendix A — Proximity",
    "section": "A.3 Measures for Single Attribute Data",
    "text": "A.3 Measures for Single Attribute Data\nWhen two data objects have only a single attribute, their proximity can be measured in a function of a direct comparison between the attribute values.\nWhen two data objects have only a single attribute, their proximity can be assessed by directly comparing their attribute values.\n\nA.3.1 Nominal Attributes\nFor nominal (categorical) attributes, logical operators can be used to compare values. In Python, the equality operator == checks whether the values are the same, and the result is a boolean (True or False). To quantify similarity, we can cast these boolean results to integers (1 for True, 0 for False):\n\n# Define nominal attribute values\nx = 'WV'\ny = 'PA'\nz = 'WV'\n\n# Similarity (1 if equal, 0 otherwise)\nsim_xy = int(x == y)\nsim_xz = int(x == z)\n\nprint(\"Similarity between x and y:\", sim_xy)\nprint(\"Similarity between x and z:\", sim_xz)\n\nSimilarity between x and y: 0\nSimilarity between x and z: 1\n\n\nTo compute dissimilarity, we use the inequality operator != and apply the same numeric conversion:\n\n# Dissimilarity (1 if different, 0 otherwise)\ndissim_xy = int(x != y)\ndissim_xz = int(x != z)\n\nprint(\"Dissimilarity between x and y:\", dissim_xy)\nprint(\"Dissimilarity between x and z:\", dissim_xz)\n\nDissimilarity between x and y: 1\nDissimilarity between x and z: 0\n\n\n\n\nA.3.2 Ordinal Attributes\nProximity measures for ordinal attributes are based on the rankings of attribute values. That is, ordinal values are mapped to numeric labels (e.g., the integer coding) according to their rank, and then the proximity is assessed. For example, let dat_grade represent the grades of five students in an ordinal attribute. If we rank the values alphabetically1, we can use Python’s rankdata() function from the scipy.stats module.\n\nfrom scipy.stats import rankdata\n\n# Sample ordinal data\ndat_grade = [\"A\", \"B\", \"C\", \"D\", \"C\"]\n\n# Rank the data alphabetically; method='min' handles ties by assigning the lowest rank\ndat_grade_rank = rankdata(dat_grade, method='min')\nprint(\"Ranked grades:\", dat_grade_rank)\n\nRanked grades: [1 2 3 5 3]\n\n\nWith these ranks, the (dis)similarity between two data objects can be computed. For instance, the dissimilarity between the first and third elements is calculated as the normalized rank difference:\n\n# Extract ranks of the first and third items\nx = dat_grade_rank[0]\ny = dat_grade_rank[2]\nn = len(dat_grade)\n\n# Dissimilarity: normalized absolute difference\ndis_xy = abs(x - y) / (n - 1)\nprint(\"Dissimilarity between x and y:\", dis_xy)\n\n# Similarity: 1 - dissimilarity\nsim_xy = 1 - dis_xy\nprint(\"Similarity between x and y:\", sim_xy)\n\nDissimilarity between x and y: 0.5\nSimilarity between x and y: 0.5\n\n\nHere, abs() returns the absolute value of the difference, and the result is scaled by dividing by n-1 to normalize the range between 0 and 1.\n\n\nA.3.3 Numerical attributes\nFor numerical attributes, proximity can be directly computed using numerical operations.\nA common measure of dissimilarity is the absolute difference between two values.\n\n# Sample numerical values\nx = 65\ny = 45\n\n# Dissimilarity: absolute difference\ndis_xy = abs(x - y)\nprint(\"Dissimilarity:\", dis_xy)\n\nDissimilarity: 20\n\n\nA corresponding similarity can then be defined as the negative of that dissimilarity or transformed via other mathematical functions.\n\n# Similarity as negative dissimilarity\nsim_xy = -dis_xy\nprint(\"Similarity (negated difference):\", sim_xy)\n\nSimilarity (negated difference): -20\n\n\nOther mathematical transformations can be used to define similarity on a bounded or decaying scale:\n\n# Similarity using inverse transformation\nsim_xy = 1 / (1 + dis_xy)\nprint(\"Similarity (inverse):\", sim_xy)\n\nSimilarity (inverse): 0.047619047619047616\n\n\nHere, math.exp() returns the exponential of the input (i.e., \\(e^x\\)) where \\(e\\) is the base of the natural logarithm. This form of similarity decays rapidly as the dissimilarity increases.\n\nimport math\n# Similarity using exponential decay\nsim_xy = math.exp(-dis_xy)\nprint(\"Similarity (exponential decay):\", sim_xy)\n\nSimilarity (exponential decay): 2.061153622438558e-09",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Proximity</span>"
    ]
  },
  {
    "objectID": "App/App-Proximity.html#measures-for-multiple-attribute-data",
    "href": "App/App-Proximity.html#measures-for-multiple-attribute-data",
    "title": "Appendix A — Proximity",
    "section": "A.4 Measures for Multiple Attribute Data",
    "text": "A.4 Measures for Multiple Attribute Data\nIn most real-world data mining applications, each data object is described by multiple attributes (features). Measuring proximity—such as similarity or dissimilarity—between these multi-attribute objects is essential for tasks like clustering, classification, and nearest-neighbor search.\nWhen multiple attributes are involved, we consider the entire feature vector of each object and apply a mathematical function that aggregates the differences across all attributes.\nFor example, common dissimilarity measures include:\n\nEuclidean distance (straight-line distance)\nManhattan distance (sum of absolute differences)\nMinkowski distance (generalized form)\nCosine dissimilarity (based on angle between vectors)\n\nWe’ll explore these with Python examples in the following sections.\n\nA.4.1 Nominal Attributes — Simple Matching Coefficient\nLet’s consider two data records, xi and xj, each with five binary (two-class nominal) attributes:\n\n# Define two categorical vectors\nxi = ['Yes', 'No', 'No', 'Yes', 'No']\nxj = ['Yes', 'Yes', 'No', 'Yes', 'No']\n\nIn Python, we can use list comprehension or NumPy to compare elements element-wise. The equality operator == applied to each pair of attributes produces a list of Boolean values:\n\n# Element-wise comparison (Boolean)\nmatches = [a == b for a, b in zip(xi, xj)]\nprint(\"Matching positions:\", matches)\n\nMatching positions: [True, False, True, True, True]\n\n\nWhen performing numerical operations, Python treats True as 1 and False as 0. Thus, summing these values counts the number of matches, and dividing by the total number of attributes gives the Simple Matching Coefficient (SMC):\n\n# Compute simple matching coefficient\nnumer = sum(matches)       # Number of matching attributes\ndenom = len(xi)            # Total number of attributes\nsmc = numer / denom        # Simple Matching Coefficient\n\nprint(\"Simple Matching Coefficient:\", smc)\n\nSimple Matching Coefficient: 0.8\n\n\nOf course, this can also be done concisely in a single line:\n\n# One-liner SMC computation\nsmc = sum([a == b for a, b in zip(xi, xj)]) / len(xi)\nprint(\"SMC (one-liner):\", smc)\n\nSMC (one-liner): 0.8\n\n\nThe Simple Matching Coefficient is useful when all attributes are equally important and symmetrically valued (i.e., no distinction between presence and absence).\n\nA.4.1.1 Simple Matching Coefficient for Multiple Objects\nNow, let’s consider a dataset that consists of three objects xi, xj, and xk:\n\nimport pandas as pd\n\n# Define three categorical records\nxi = ['Yes', 'No', 'No', 'Yes', 'No']\nxj = ['Yes', 'Yes', 'No', 'Yes', 'No']\nxk = ['No', 'Yes', 'Yes', 'Yes', 'No']\n\n# Combine into a DataFrame\ndat1 = pd.DataFrame([xi, xj, xk], index=['xi', 'xj', 'xk'],\n                    columns=['X1', 'X2', 'X3', 'X4', 'X5'])\ndat1\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\n\n\n\n\nxi\nYes\nNo\nNo\nYes\nNo\n\n\nxj\nYes\nYes\nNo\nYes\nNo\n\n\nxk\nNo\nYes\nYes\nYes\nNo\n\n\n\n\n\n\n\nWhen calculating similarity measures for multiple records, it is helpful to define a function for repeated use. While Python doesn’t have a built-in “simple matching” function like R’s proxy::simil(), we can implement it or use existing tools after converting the data.\n\nConvert Categorical to Binary\nWe need to convert each categorical column into binary form, for example by checking whether each value equals Yes:\n\n# Convert to binary (logical) format based on 'Yes' responses\ndat1_bin = pd.DataFrame({\n    'X1': dat1['X1'] == 'Yes',\n    'X2': dat1['X2'] == 'Yes',\n    'X3': dat1['X3'] == 'Yes',\n    'X4': dat1['X4'] == 'Yes',\n    'X5': dat1['X5'] == 'Yes'\n}, index=dat1.index)\ndat1_bin\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\n\n\n\n\nxi\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\nxj\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\nxk\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n\n\n\n\n\n\n\nCompute Simple Matching Coefficients\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html#scipy.spatial.distance.pdist\nPython does not have a built-in “simple matching” metric in sklearn or scipy, but you can define and extend your own as shown above. Alternatively, similarity/distance measures like Jaccard, Hamming, or cosine similarity can be used for different purposes depending on your data.\n\nfrom scipy.spatial.distance import pdist, squareform\n\n# Compute the pairwise distance (i.e., dissimilarity) matrix \nhamm_condensed = pdist(dat1_bin, metric='hamming')\nhamm_matrix = squareform(hamm_condensed)\nhamm_matrix\n\n# Compute the pairwise coefficient (i.e., similarity) matrix \nsmc_matrix = 1 - hamm_matrix\nsmc_matrix\n\narray([[1. , 0.8, 0.4],\n       [0.8, 1. , 0.6],\n       [0.4, 0.6, 1. ]])\n\n\nTo access specific similarity values, for example between xi and xj, you can do:\n\ni, j = 0, 2\nsmc_matrix[i, j]\n\nnp.float64(0.4)\n\n\n\n\nCustom function\nSometimes, the measure you want to use may be unavailable in such existing function/package. Then, creating a custom function can be a good solution.\nFor example, you can create a custom function simple_matching() that takes two input arguments x and y for the calculation for the matching coefficient between two objects x and y.\nWe can now compute the simple matching coefficient using the formula:\n\n# Define a custom function for SMC\ndef simple_matching(x, y):\n    return sum(x == y) / len(x)\n\n# Compute pairwise similarity matrix manually\nsmc_matrix = pd.DataFrame(index=dat1_bin.index, columns=dat1_bin.index, dtype=float)\nfor i in dat1_bin.index:\n    for j in dat1_bin.index:\n        smc_matrix.loc[i, j] = simple_matching(dat1_bin.loc[i], dat1_bin.loc[j])\n\nsmc_matrix\n\n\n\n\n\n\n\n\nxi\nxj\nxk\n\n\n\n\nxi\n1.0\n0.8\n0.4\n\n\nxj\n0.8\n1.0\n0.6\n\n\nxk\n0.4\n0.6\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA task with Python can be performed in many possible approaches, and an approach is chosen based on a different criterion (e.g., computing, coding difficulty, compactness, compatibility with other code/software/language, etc.). For that, you may select the way best for you:\n\nbased on the existing function(s) in some package, following the format required (so that you may need to do some preprocessing).\nbased on a custom function, in which you can specify all needed for the calculation (in this example, you could get the result directly by using the original categorical data without the preprocessing).\nor, based on some mix like an example below:\n\n\n# A custom function\ndef simple_nonmatching(x, y):\n    return sum(x != y) / len(x)\n\n# Compute pairwise similarity matrix with pdist in scipy\nsmc_condensed = pdist(dat1_bin, metric=simple_nonmatching)\nsmc_matrix = 1 - squareform(smc_condensed)\nsmc_matrix\n\narray([[1. , 0.8, 0.4],\n       [0.8, 1. , 0.6],\n       [0.4, 0.6, 1. ]])\n\n\n\n\n\n\n\nA.4.1.2 Jaccard Coefficient\nThe Jaccard coefficient (or Jaccard index) measures the similarity between two records by focusing on the co-occurring values among the values that exist in any of the records. In other words, the similarity between two binary vectors focuses on the shared positive values.\nLet’s compare the two binary records xj and xk from our earlier dat1_bin dataset:\n\n# Extract binary vectors for xj and xk\nxj_bin = dat1_bin.loc['xj']\nxk_bin = dat1_bin.loc['xk']\n\n# Display both for comparison\npd.DataFrame([xj_bin, xk_bin], index=['xj', 'xk'])\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\n\n\n\n\nxj\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\nxk\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n\n\n\n\n\nThere are four possible value combinations per attribute across the two records:\n\nTrue–True → positive-positive (count_pp)\nTrue–False → positive-negative (count_pn)\nFalse–True → negative-positive (count_np)\nFalse–False → negative-negative (count_nn)\n\nWe can count each case manually:\n\n# Count different value combinations\ncount_pp = ((xj_bin == True) & (xk_bin == True)).sum()\ncount_pn = ((xj_bin == True) & (xk_bin == False)).sum()\ncount_np = ((xj_bin == False) & (xk_bin == True)).sum()\ncount_nn = ((xj_bin == False) & (xk_bin == False)).sum()\n\n# Show as a 2x2 matrix\nimport numpy as np\nnp.array([[count_pp, count_pn], [count_np, count_nn]])\n\narray([[2, 1],\n       [1, 1]])\n\n\nAlternatively, you can get this same result using a crosstab (like R’s table()):\n\n# Crosstabulation using pandas\npd.crosstab(xj_bin.astype(bool), xk_bin.astype(bool),\n            rownames=['xj'], colnames=['xk'])\n\n\n\n\n\n\n\nxk\nFalse\nTrue\n\n\nxj\n\n\n\n\n\n\nFalse\n1\n1\n\n\nTrue\n1\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo ensure proper handling of boolean categories, we explicitly cast values using .astype(bool).\n\n\nThen, Jaccard similarity is computed as:\n\\[\nJ(x_j, x_k) = \\frac{\\text{# of } True\\text{-}True}{\\text{# of attributes with at least one } True}\n\\]\nThat is:\n\nnumer = count_pp\ndenom = count_pp + count_pn + count_np\njaccard_coeff = numer / denom\njaccard_coeff\n\nnp.float64(0.5)\n\n\nYou can also calculate the Jaccard coefficient matrix using sklearn.metrics:\n\nfrom sklearn.metrics import jaccard_score\n\n# Apply Jaccard similarity on binary rows (note: must convert to numpy)\njaccard_score(dat1_bin.loc['xj'], dat1_bin.loc['xk'], average='binary')\n\nnp.float64(0.5)\n\n\nOr compute pairwise Jaccard distance between all rows using scipy’s pdist sklearn’s pairwise_distances:\n\ndat1_bin\n# Compute Jaccard distance (1 - similarity)\njaccard_dist = pdist(dat1_bin, metric='jaccard')\njaccard_sim_matrix = 1 - squareform(jaccard_dist)\n\n# Format as a DataFrame\npd.DataFrame(jaccard_sim_matrix, index=dat1_bin.index, columns=dat1_bin.index)\n\n\n\n\n\n\n\n\nxi\nxj\nxk\n\n\n\n\nxi\n1.000000\n0.666667\n0.25\n\n\nxj\n0.666667\n1.000000\n0.50\n\n\nxk\n0.250000\n0.500000\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn scikit-learn, pairwise_distances(…, metric=‘jaccard’) returns the distance (i.e., 1 - similarity).\n\n\n\n\n\nA.4.2 Numerical Attributes\nLet’s consider a dataset with three objects—xi, xj, and xk—each described by two numerical attributes:\n\nX1: Average time spent online (hours)\nX2: Number of orders during the last month\n\n\nimport pandas as pd\n\n# Create a DataFrame with numerical attributes\ndat2 = pd.DataFrame({\n    'X1': [2, 3, 3],  # Average time spent online\n    'X2': [3, 0, 4]   # Number of orders during the last month\n}, index=['xi', 'xj', 'xk'])\n\ndat2\n\n\n\n\n\n\n\n\nX1\nX2\n\n\n\n\nxi\n2\n3\n\n\nxj\n3\n0\n\n\nxk\n3\n4\n\n\n\n\n\n\n\nFor simplicity in the following examples, we can extract the first two rows (xi and xj) as individual vectors:\n\n# Extract specific data points as Series\nxi = dat2.loc['xi']\nxj = dat2.loc['xj']\n\nprint(\"xi:\", xi.to_list())\nprint(\"xj:\", xj.to_list())\n\nxi: [2, 3]\nxj: [3, 0]\n\n\nFrom here, you can compute various distance or similarity measures, such as:\n\nEuclidean distance\nManhattan distance\nCosine similarity\nCorrelation coefficient\n\n\nA.4.2.1 Distance\nDistances are commonly used as dissimilarity measures between numerical data objects.\nUsing the formula\n\\[\nd(\\mathbf{x}_i, \\mathbf{x}_j) = \\sqrt{ \\sum_{k=1}^{N}(x_{ik} - x_{jk})^2 },\n\\]\nwe can manually compute the Euclidean distance between xi and xj in the order:\n\nCompute element-wise difference\nSquare of differences\nSum of squared differences\nSquare root of the sum\n\n\ndiff = xi - xj                              # 1) xik-xjk for each attribute k\ndiff_sqr = diff ** 2                        # 2) (xik-xjk)^2 for each k\ndiff_sqr_sum = diff_sqr.sum()               # 3) sum((xi-xj)^2)\neuclidean_manual = np.sqrt(diff_sqr_sum)    # 4) sqrt(sum((xi-xj)^2))\neuclidean_manual\n\nnp.float64(3.1622776601683795)\n\n\n\nEuclidean Distance Using Built-in Function\nYou can also use scipy.spatial.distance or sklearn.metrics2 to compute the distance between rows:\n\nfrom scipy.spatial.distance import euclidean\n\n# Combine xi and xj into a matrix\ndat2_ij = dat2.loc[['xi', 'xj']]\n\n# Compute Euclidean distance\neuclidean(dat2_ij.loc['xi'], dat2_ij.loc['xj'])\n\nnp.float64(3.1622776601683795)\n\n\nTo compute pairwise distances for all rows, use scipy.spatial.distance.pdist with squareform:\n\nfrom scipy.spatial.distance import pdist, squareform\n\n# Euclidean distances for all pairs\ndist_matrix_euclidean = pd.DataFrame(\n    squareform(pdist(dat2, metric='euclidean')),\n    index=dat2.index, columns=dat2.index\n)\ndist_matrix_euclidean\n\n\n\n\n\n\n\n\nxi\nxj\nxk\n\n\n\n\nxi\n0.000000\n3.162278\n1.414214\n\n\nxj\n3.162278\n0.000000\n4.000000\n\n\nxk\n1.414214\n4.000000\n0.000000\n\n\n\n\n\n\n\n\n\n\nA.4.2.2 Other Distance Metrics\nManhattan Distance (City Block):\n\n# Manhattan (L1) distance matrix\ndist_matrix_manhattan = pd.DataFrame(\n    squareform(pdist(dat2, metric='cityblock')),\n    index=dat2.index, columns=dat2.index\n)\ndist_matrix_manhattan\n\n\n\n\n\n\n\n\nxi\nxj\nxk\n\n\n\n\nxi\n0.0\n4.0\n2.0\n\n\nxj\n4.0\n0.0\n4.0\n\n\nxk\n2.0\n4.0\n0.0\n\n\n\n\n\n\n\nMinkowski Distance (Generalized form with parameter p):\n\n# Minkowski distance with p = 1.5\ndist_matrix_minkowski = pd.DataFrame(\n    squareform(pdist(dat2, metric='minkowski', p=1.5)),\n    index=dat2.index, columns=dat2.index\n)\ndist_matrix_minkowski\n\n\n\n\n\n\n\n\nxi\nxj\nxk\n\n\n\n\nxi\n0.000000\n3.373505\n1.587401\n\n\nxj\n3.373505\n0.000000\n4.000000\n\n\nxk\n1.587401\n4.000000\n0.000000\n\n\n\n\n\n\n\nChebyshev Distance (Minkowski with \\(p\\rightarrow\\infty\\):\n\n# Chebyshev (maximum coordinate difference)\ndist_matrix_chebyshev = pd.DataFrame(\n    squareform(pdist(dat2, metric='chebyshev')),\n    index=dat2.index, columns=dat2.index\n)\ndist_matrix_chebyshev\n\n\n\n\n\n\n\n\nxi\nxj\nxk\n\n\n\n\nxi\n0.0\n3.0\n1.0\n\n\nxj\n3.0\n0.0\n4.0\n\n\nxk\n1.0\n4.0\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAll of these metrics are available through scipy.spatial.distance.pdist, which is efficient and widely used for pairwise distance calculations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Proximity</span>"
    ]
  },
  {
    "objectID": "App/App-Proximity.html#related-readingreference",
    "href": "App/App-Proximity.html#related-readingreference",
    "title": "Appendix A — Proximity",
    "section": "A.5 Related Reading/Reference",
    "text": "A.5 Related Reading/Reference\n\nChapter 11.2 in Business Analytics: communicating with Numbers, 2nd ed. (Jaggia et al., 2023)\nChapter 2.4 in Introduction to Data Mining, 2nd ed. (Tan et al., 2019)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Proximity</span>"
    ]
  },
  {
    "objectID": "App/App-Proximity.html#footnotes",
    "href": "App/App-Proximity.html#footnotes",
    "title": "Appendix A — Proximity",
    "section": "",
    "text": "The rankings of categorical values from rank() are meaningful only if the alphabetical orders of the values are meaningful.↩︎\nAs the distance calculation is widely used in data science, there are many packages available for the same computation.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Proximity</span>"
    ]
  },
  {
    "objectID": "DV/DV-IntroVisual.html#what-is-data-visualization",
    "href": "DV/DV-IntroVisual.html#what-is-data-visualization",
    "title": "9  Introduction to Data Visualization",
    "section": "9.2 What is Data Visualization?",
    "text": "9.2 What is Data Visualization?\nData visualization is a core component of data analysis, and there are many packages and approaches available when working in Python.\nBefore diving into specific tools, it’s useful to consider the different philosophies and purposes behind visualizations.\n\n9.2.1 Philosophies of Data Visualization\nThere are two broad approaches to creating visualizations with code:\n\nImperative – You specify step by step what to draw.\n\nPros: Maximum flexibility and control.\n\nCons: Often verbose; more effort for common plots.\n\nDeclarative – You declare what you want, and the library handles the details.\n\nPros: Quick and concise for standard chart types.\n\nCons: Requires data in the correct format; customization can be limited.\n\n\nChoosing between them depends on your goals and your data.\n\n\n9.2.2 Purposes of Data Visualization\nIt is also helpful to keep in mind the three broad purposes of visualization:\n\nExploratory – To understand your data and discover patterns.\n\nScientific – To communicate precise quantitative findings.\n\nNarrative – To tell a data-driven story and engage an audience.\n\nPython excels in exploratory and scientific visualization. For narrative visualization, the tools are improving but still less streamlined for rapid story creation.\n\n9.2.2.1 Exploratory Visualization\nThe first type of data visualization is exploratory visualization that you use when you are looking at data to understand it. Simply plotting your data is often the best way to spot patterns, anomalies, or issues before doing any sophisticated analysis.\nA classic example that demonstrates the importance of visualization is Anscombe’s Quartet—four datasets that share the same mean, standard deviation, and correlation but have very different underlying distributions.\nFor this example, let’s construct four different datasets:\n\n# Define the four datasets\nx = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\ndatasets = {\"I\": (x, y1), \"II\": (x, y2), \"III\": (x, y3), \"IV\": (x4, y4)}\n\nThen, visualize them side by side:\n\n# Create subplots\nfig, axs = plt.subplots(\n    1, 4, sharex=True, sharey=True, figsize=(12, 3),\n    gridspec_kw={\"wspace\": 0.08, \"hspace\": 0.08},\n)\n\naxs[0].set(xlim=(0, 20), ylim=(2, 14))\naxs[0].set(xticks=(0, 10, 20), yticks=(4, 8, 12))\n\n# Plot each dataset with regression line and summary stats\nfor ax, (label, (x, y)) in zip(axs.flat, datasets.items()):\n    ax.text(0.1, 0.9, label, fontsize=20, transform=ax.transAxes, va=\"top\")\n    ax.tick_params(direction=\"in\", top=True, right=True)\n    ax.plot(x, y, \"o\")\n\n    # Linear regression\n    slope, intercept = np.polyfit(x, y, deg=1)\n    ax.axline(xy1=(0, intercept), slope=slope, color=\"r\", lw=2)\n\n    # Statistics box\n    stats = (\n        f\"$\\\\mu$ = {np.mean(y):.2f}\\n\"\n        f\"$\\\\sigma$ = {np.std(y):.2f}\\n\"\n        f\"$r$ = {np.corrcoef(x, y)[0][1]:.2f}\"\n    )\n    bbox = dict(boxstyle=\"round\", fc=\"blanchedalmond\", ec=\"orange\", alpha=0.5)\n    ax.text(\n        0.95, 0.07, stats, fontsize=9, bbox=bbox,\n        transform=ax.transAxes, horizontalalignment=\"right\",\n    )\n\nplt.suptitle(\"Anscombe's Quartet\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nExploratory visualization is often quick, flexible, and informal. It is primarily for your own understanding, or at most for co-authors and collaborators. For such purposes, many automated EDA (Exploratory Data Analysis) tools exist.\n\n\n\n\n9.2.2.2 Scientific Visualization\nThe second type of data visualization is scientific visualization as the refined version of your exploratory plot(s) that:\n\nOften contains a high density of information;\nIs designed for technical audiences, such as journal readers or subject-matter experts;\nMust allow accurate reading of key values.\n\nThese are the figures you might include in a technical paper the kind of images that truly embody the saying “A picture is worth a thousand words.”.\nIn short-format journals, such as Physical Review Letters (with its 8-page limit), it is common to see compact, information-rich plots.\nThese figures are typically less flashy than narrative charts but are critical for conveying “killer results” in your research.\n\n\n9.2.2.3 Narrative Visualization\nThe final type is narrative visualization as the the “bold font” of visualization that:\n\nCommunicates insight, not just data;\nGuides the viewer’s attention, often with annotations, highlights, or visual emphasis;\nTargets a broad audience, including non-experts or policymakers.\n\nThis style of visualization requires the most thought and careful design, moving from raw data views to a polished, insight-driven story. Examples of this type include the visualizations you might encounter in expert articles in The Financial Times, The Economist, etc.\nNarrative visualizations are particularly valuable when:\n\nYou are summarizing research for a wide readership.\nWriting blog posts or public reports.\nCommunicating with stakeholders without deep technical expertise.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "DV/DV-IntroVisual.html#quick-guide-to-data-visualization",
    "href": "DV/DV-IntroVisual.html#quick-guide-to-data-visualization",
    "title": "9  Introduction to Data Visualization",
    "section": "9.3 Quick Guide to Data Visualization",
    "text": "9.3 Quick Guide to Data Visualization\nAddressing data visualization in full is far beyond the scope of this course. However, a few general pointers will serve you very well if you follow them consistently.\n\nClarify the Message: A picture may tell a thousand words—but you must decide which words.\n\nAsk yourself: “What does this plot tell the viewer?”\nIdentify the key takeaway and make it immediately apparent.\nEnsure the viewer is left with no ambiguity about your intended message.\n\nMake Every Plot Count: Articles typically include a limited number of plots (e.g., academic papers with fewer than 10 plots), each of which should advance the narrative of your work:\n\nAvoid redundancy: If your data are normally distributed, a plot may not add value.\nHighlight contrasts: Two distributions with important differences deserve a visual comparison.\n\nChoose the Right Plot Type: Selecting the appropriate chart type is critical\n\nA chart type may deliver a certain type of messages better than other chart types:\n\nScatter plot – For independent observations with no autocorrelation.\nLine chart – For *time series** or data with sequential correlation.\nBar charts – For categorical comparisons; consider stacked vs. grouped based on your story.\n\nIn addition to the data presented in the chosen chart, you can encode extra information for another dimension with:\n\nColor (e.g., categories or magnitude)\nShape (for multiple groups)\nSize (for magnitude comparisons)\n\n\nLabel Clearly: Many otherwise strong plots fail due to missing context:\n\nAxis labels – Always include them, and if there are units, state them clearly (e.g., “Salary (2015 USD)”).\n\nTick labels – Tailor them to the scale (linear vs. logarithmic).\n\nTitles – Use them if axes and the plot alone do not fully convey the message.\n\nGuide the Viewer’s Attention: If you have certain features more important, you may highlight them on the chart in such ways of:\n\nUsing text annotations to point out key features.\n\nFading less important elements with transparency.\n\nKeeping the key line, bar, or point in solid color to draw the eye.\n\n\nKey takeaway: &gt; Good visualization communicates insight clearly, makes every plot count, and guides the reader to the story you want to tell.\n\n\n\n\n\n\nRecommended Resources\n\n\n\nFor further guidance and inspiration:\n\nFundamentals of Data Visualization – Short, accessible, and highly practical.\n\nFT Visual Vocabulary – Helps you choose the right chart for the right message.\n\nVector Graphics – Always use vector formats for sharp, professional plots.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "DV/DV-IntroVisual.html#python-data-visualization-libraries",
    "href": "DV/DV-IntroVisual.html#python-data-visualization-libraries",
    "title": "9  Introduction to Data Visualization",
    "section": "9.4 Python Data Visualization Libraries",
    "text": "9.4 Python Data Visualization Libraries\nPython has a rich ecosystem of visualization libraries, each with different strengths depending on whether you need static figures, interactivity, or web integration.\nThe following sections introduce some popular python libraries. For a comprehensive list, see the PyViz Tools Overview.\n\n9.4.1 Core Visualization Libraries\nHere are the core libraries you are most likely to use either directly or indirectly (as many other high-level libraries are built on these core libraries):\n\nMatplotlib is the most important and widely used visualization library in Python.\n\nKey Features:\n\nImperative approach – build plots piece by piece for complete control.\n\nHighly flexible but can be verbose for complex plots.\n\nSupports static plots, diagrams, animations, and 3D visualizations (3D should be used sparingly for clarity).\n\nWhen to Use:\n\nYou need fine-grained control over every visual element.\n\nYou are making bespoke or unusual charts.\n\nYou require careful graphical design or incremental plot building.\n\n\nPlotly is a declarative-oriented library designed for interactive visualizations.\n\nKey Features:\n\nGenerates interactive charts with tooltips and zooming.\n\nIdeal for dashboards and web applications.\n\nCan export static images but shines in browser-based interactivity.\n\nWhen to Use:\n\nYou are building a data dashboard or web-based visualization.\n\nYou need interactive plots to explore data beyond static figures.\n\n\nBokeh is another interactive visualization library for Python.\n\nKey Features:\n\nEnables browser-based, interactive plotting.\n\nCan handle streaming and real-time data.\n\nIntegrates with Jupyter notebooks, dashboards, and web apps.\n\nWhen to Use:\n\nYou need interactive visualizations with real-time or large data handling.\n\nYou want standalone HTML outputs for sharing results without extra dependencies.\n\n\n\n\n\n9.4.2 Other Libraries\nPython has many specialized visualization libraries that complement the core ones. These are useful for statistical plotting, interactive dashboards, machine learning visualization, or domain-specific applications.\n\nPandas Built-in plotting provides quick, convenient plots:\n\nSyntax: df.plot.* (e.g., df.plot.scatter).\n\nInternally uses Matplotlib.\n\nExcellent for fast exploratory data analysis (EDA), but not suitable for polished publication figures.\n\nSeaborn is a high-level statistical visualization library built on Matplotlib:\n\nWorks best with tidy data (one row per observation, one column per variable).\n\nProvides high-level functions for common statistical plots (e.g., box plots, violin plots, heatmaps).\n\nIdeal for exploratory analysis and quick statistical graphics.\n\nAltair is a declarative, web-oriented library:\n\nProduces beautiful, minimalist charts with minimal code.\n\nBest for interactive, browser-friendly visualizations.\n\nBuilt on Vega-Lite, widely used in newsroom data visualization.\n\n\nBeyond the core and high-level libraries, Python offers many specialized tools for niche visualization needs:\n\nproplot: Lightweight Matplotlib wrapper for publication-quality plots.\n\nseaborn-image: Brings Seaborn-like workflows to image data.\n\nLit: Visualization and interpretability for NLP models.\n\nWordcloud: Generates word clouds (use sparingly!).\n\nVisPy: GPU-accelerated visualization for very large datasets.\n\nHoloViews: Simplifies data-to-visualization workflows; builds on Bokeh and Matplotlib.\n\nchartify: Quick, high-level plotting library from Spotify.\n\npalettable: Additional color palettes for Matplotlib and Seaborn.\n\ncolorcet: Perceptually uniform color maps for accurate data perception.\n\nmissingno: Visualizes patterns of missing data in your dataset.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "DV/DV-IntroVisual.html#related-readingreference",
    "href": "DV/DV-IntroVisual.html#related-readingreference",
    "title": "9  Introduction to Data Visualization",
    "section": "9.5 Related Reading/Reference",
    "text": "9.5 Related Reading/Reference\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R\n\n\n\n9.5.1 Review\nIf you know:\n\n✅ a little bit about how to use data visualisation; and\n✅ what some of the most popular libraries for data vis are in Python\n\nthen you are well on your way to being a whizz with data vis!\nhttps://datavizproject.com/ https://policyviz.com/wp-content/uploads/2021/02/DataCatalogs.pdf\ntechnical https://www.labri.fr/perso/nrougier/python-opengl/#raw-lines",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "U/UP-AssocRules.html",
    "href": "U/UP-AssocRules.html",
    "title": "10  Association Rules",
    "section": "",
    "text": "10.1 Overview\nAssociation rule learning (or simply association rules) is a rule-based approach for uncovering interesting patterns or regularities within large datasets. This technique focuses on discovering relationships between items in transactional datasets using metrics such as support, confidence, and lift.\nThis section walks through the process of association rule mining in Python. A transaction dataset is analyzed to identify frequently co-occurring itemsets using the Apriori algorithm. Based on user-defined thresholds for support and confidence, association rules are extracted and evaluated. Visualizations can further aid interpretation of the discovered patterns.",
    "crumbs": [
      "Unsupervised Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Association Rules</span>"
    ]
  },
  {
    "objectID": "U/UP-AssocRules.html#preliminaries",
    "href": "U/UP-AssocRules.html#preliminaries",
    "title": "10  Association Rules",
    "section": "10.2 Preliminaries",
    "text": "10.2 Preliminaries\n\n10.2.1 Packages\n\nmlxtend for Apriori and association rule mining\n\npandas for data manipulation\n\nmatplotlib and seaborn for visualization (optional)\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n10.2.2 Data\nWe begin with a list of transactions, each containing a set of items.\n\ntransactions = [\n    ['Milk', 'Bread', 'Apple'],\n    ['Milk', 'Bread'],\n    ['Milk', 'Bread', 'Apple'],\n    ['Bread', 'Nuts'],\n    ['Milk', 'Apple'],\n    ['Bread', 'Apple', 'Nuts'],\n    ['Milk', 'Bread', 'Nuts'],\n    ['Bread'],\n]\n\n\n\n\n\n\n\nArrayed Data to Transaction Lists\n\n\n\n\nImport the Transaction data into a Pandas DataFrame (table) using read_csv and label it myData.\n\nMake sure to set header to 'None' since there is no header row in the CSV file. Enter:\n\nmyData = pd.read_csv('Transaction.csv', header = None)\nmyData.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nkeyboard\nmouse\nheadphone\nNaN\nNaN\n\n\n1\nkeyboard\nSDcard\nmouse\nNaN\nNaN\n\n\n2\nheadphone\nNaN\nNaN\nNaN\nNaN\n\n\n3\nkeyboard\nNaN\nNaN\nNaN\nNaN\n\n\n4\nkeyboard\nSDcard\nmouse\nUSBdrive\nNaN\n\n\n\n\n\n\n\n\nThen, convert the transactions to a suitable list structure using list comprehension and store it in the variable Transactions.\n\nThe Pandas isnull function is used in the code below to remove missing values (i.e., as indicated by NaN values in the output above). More specifically, the Python bitwise operator (i.e., ~) is used in conjunction with isnull to keep only not null values. Enter:\n\nlist_trans = [\n    tran[~pd.isnull(tran)]\n    for tran in myData.values\n]\n\nTo see the first five transactions, we can use a slice. Enter:\n\nlist_trans[:5]\n\n[array(['keyboard', 'mouse', 'headphone'], dtype=object),\n array(['keyboard', 'SDcard', 'mouse'], dtype=object),\n array(['headphone'], dtype=object),\n array(['keyboard'], dtype=object),\n array(['keyboard', 'SDcard', 'mouse', 'USBdrive'], dtype=object)]\n\n\n\n\n\n10.2.2.1 Convert to Transaction Format\nTo create a frequency table and frequency plot, we can use the TransactionEncoder in the package mlxtend and its fit_transform method.\nhttps://anaconda.org/conda-forge/mlxtend\nconda install conda-forge::mlxtend\n\nfrom mlxtend.preprocessing import TransactionEncoder\n\nencoder = TransactionEncoder()\ncoded_trans = encoder.fit_transform(transactions)\ncoded_trans        # array (n_transactions, n_unique_items)\n\narray([[ True,  True,  True, False],\n       [False,  True,  True, False],\n       [ True,  True,  True, False],\n       [False,  True, False,  True],\n       [ True, False,  True, False],\n       [ True,  True, False,  True],\n       [False,  True,  True,  True],\n       [False,  True, False, False]])\n\n\nhttps://rasbt.github.io/mlxtend/api_subpackages/mlxtend.preprocessing/\n\n\n10.2.2.2 Explore Basic Properties\nYou can inspect the encoded data and retrieve basic statistics:\n\n# (no. transactions, no. unique items)\nnum_transactions, num_items = coded_trans.shape \nprint(f\"No. transactions = {num_transactions}, \\tNo. items = {num_items}\")\n\nNo. transactions = 8,   No. items = 4\n\n\nFrom the encoder, the column information about the encoded item matrix (i.e., the sorted item name list) can be retrieved:\n\nencoder.columns_          # a sorted list of item names\nencoder.columns_mapping_  # a mapping dictionary for item names and column indices\n\n{'Apple': 0, 'Bread': 1, 'Milk': 2, 'Nuts': 3}\n\n\nYou can also visualize the transactions as a binary image:\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(coded_trans, aspect='auto', cmap='Greys')\nplt.title(\"Transaction Matrix\")\nplt.xlabel(\"Items\")\nplt.ylabel(\"Transactions\")\nplt.show()",
    "crumbs": [
      "Unsupervised Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Association Rules</span>"
    ]
  },
  {
    "objectID": "U/UP-AssocRules.html#association-rules",
    "href": "U/UP-AssocRules.html#association-rules",
    "title": "10  Association Rules",
    "section": "10.3 Association Rules",
    "text": "10.3 Association Rules\nTo conduct the association rule analysis, we use the apriori function in the efficient-apriori package, which is available on PyPI and can be installed via pip in a code cell using %pip install efficient-apriori. Once installed, we can call the function and specify our desired min_support (minimum support) and min_confidence (minimum confidence) to match the values used in the R example in the text (i.e., 0.1 and 0.5, respectively). Enter:\n\n10.3.1 Frequency of Single Items\nFor easy of handling the data, let’s keep the encoded data in a Pandas DataFrame:\n\ndf_trans = pd.DataFrame(coded_trans, columns = encoder.columns_)\n\nThe absolute support of each item (i.e., how many times it appears across all transactions) can be calculated using sum() :\n\n# Absolute support (frequency)\ndf_trans.sum().sort_values(ascending=False)\n\nBread    7\nMilk     5\nApple    4\nNuts     3\ndtype: int64\n\n\nSimilarly, the (relative) support of each item (i.e., how often it appears across all transactions) can be calculated using the mean() method on the binary matrix.\n\n# Relative support (frequency proportion)\ndf_trans.mean().sort_values(ascending=False)\n\nBread    0.875\nMilk     0.625\nApple    0.500\nNuts     0.375\ndtype: float64\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe supports can be manually calculated by dividing them by the number of transactions:\n\ndf_trans.sum().sort_values(ascending=False) / df_trans.shape[0]\n\nBread    0.875\nMilk     0.625\nApple    0.500\nNuts     0.375\ndtype: float64\n\n\n\n\n\n10.3.1.1 Bar Plot of Frequent Items\nVisualizing item frequency with a bar chart:\nwe use the Pandas sort_values function to display the result sorted in descending order (i.e., ascending is set to False).\n\nimport matplotlib.pyplot as plt\n\nitem_support = df_trans.mean().sort_values(ascending=False)\ntop_items = item_support.head(3)\n\nplt.figure(figsize=(8,4))\ntop_items.plot(kind='bar', color='skyblue')\nplt.title(\"Top 3 Items by Support\")\nplt.ylabel(\"Support (Relative)\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Frequent Itemset Generation\nThe total number of non-empty itemsets given d as the number of unique items is \\(2^d -1\\).\n\nd = df_trans.shape[1]\n2**d - 1\n\n15\n\n\nSo, instead of generating all itemsets, we use a smart algorithm to extract only the frequent ones. In this example, let’s the Apriori algorithm in mlxtend.\n\nfrom mlxtend.frequent_patterns import apriori\n\nfrequent_itemsets = apriori(\n    df_trans, \n    min_support=0.2, \n    use_colnames=True\n)\nfrequent_itemsets.sort_values(by=\"support\", ascending=False).head(10)\n\n\n\n\n\n\n\n\nsupport\nitemsets\n\n\n\n\n1\n0.875\n(Bread)\n\n\n2\n0.625\n(Milk)\n\n\n0\n0.500\n(Apple)\n\n\n6\n0.500\n(Milk, Bread)\n\n\n3\n0.375\n(Nuts)\n\n\n4\n0.375\n(Bread, Apple)\n\n\n5\n0.375\n(Milk, Apple)\n\n\n7\n0.375\n(Nuts, Bread)\n\n\n8\n0.250\n(Bread, Milk, Apple)\n\n\n\n\n\n\n\n\nYou can filter frequent itemsets by length (e.g., 2-itemsets only):\n\nfrequent_2_itemsets = frequent_itemsets[\n    frequent_itemsets['itemsets'].apply(lambda x: len(x) == 2)\n]\nfrequent_2_itemsets\n\n\n\n\n\n\n\n\nsupport\nitemsets\n\n\n\n\n4\n0.375\n(Bread, Apple)\n\n\n5\n0.375\n(Milk, Apple)\n\n\n6\n0.500\n(Milk, Bread)\n\n\n7\n0.375\n(Nuts, Bread)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor larger datasets, too-loosen parameters (e.g., small support and minlen) that generate a huge number of itemsets may cause an error due to running out of memory.\n\n\n\n\n10.3.3 Association Rule Generation\nNow we generate association rules from the frequent itemsets using a minimum support and confidence threshold:\n\nfrom mlxtend.frequent_patterns import association_rules\n\nrules = association_rules(\n    frequent_itemsets, \n    metric=\"confidence\", \n    min_threshold=0.2\n)\nrules.shape\nrules.columns\n\nIndex(['antecedents', 'consequents', 'antecedent support',\n       'consequent support', 'support', 'confidence', 'lift',\n       'representativity', 'leverage', 'conviction', 'zhangs_metric',\n       'jaccard', 'certainty', 'kulczynski'],\n      dtype='object')\n\n\nTo sort and inspect the top 3 rules by lift:\n\nrules_sorted = rules.sort_values(by='lift', ascending=False)\nrules_sorted.head(3)[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nsupport\nconfidence\nlift\n\n\n\n\n3\n(Apple)\n(Milk)\n0.375\n0.750000\n1.200000\n\n\n2\n(Milk)\n(Apple)\n0.375\n0.600000\n1.200000\n\n\n7\n(Bread)\n(Nuts)\n0.375\n0.428571\n1.142857",
    "crumbs": [
      "Unsupervised Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Association Rules</span>"
    ]
  },
  {
    "objectID": "U/UP-AssocRules.html#rule-evaluation",
    "href": "U/UP-AssocRules.html#rule-evaluation",
    "title": "10  Association Rules",
    "section": "10.4 Rule Evaluation",
    "text": "10.4 Rule Evaluation\nThe association_rules() function provides multiple interestingness metrics, including: - Support - Confidence - Lift - Leverage - Conviction\nTo access them:\n\nrules[['support', 'confidence', 'lift', 'leverage', 'conviction']].head(3)\n\n\n\n\n\n\n\n\nsupport\nconfidence\nlift\nleverage\nconviction\n\n\n\n\n0\n0.375\n0.428571\n0.857143\n-0.0625\n0.875\n\n\n1\n0.375\n0.750000\n0.857143\n-0.0625\n0.500\n\n\n2\n0.375\n0.600000\n1.200000\n0.0625\n1.250\n\n\n\n\n\n\n\nAlthough mlxtend does not support additional metrics like “chiSquared” or “gini” natively, these can be computed manually or using other libraries like Orange3 or sklearn if needed for deeper evaluation.",
    "crumbs": [
      "Unsupervised Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Association Rules</span>"
    ]
  },
  {
    "objectID": "U/UP-AssocRules.html#association-rule-visualizations",
    "href": "U/UP-AssocRules.html#association-rule-visualizations",
    "title": "10  Association Rules",
    "section": "10.5 Association Rule Visualizations",
    "text": "10.5 Association Rule Visualizations\nWhile the true value of association rule mining often lies in large-scale applications, visualization is a powerful way to interpret and communicate the discovered patterns. Python offers several options to visualize association rules and frequent itemsets.\n\n10.5.1 Scatterplot of Rules\nWe can plot each rule using support, confidence, and lift to explore their distribution:\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(\n    data=rules, \n    x='support', \n    y='confidence', \n    size='lift', \n    hue='lift', \n    palette='viridis', \n    alpha=0.7, \n    sizes=(20, 200)\n)\nplt.title(\"Scatter Plot of Association Rules\")\nplt.xlabel(\"Support\")\nplt.ylabel(\"Confidence\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n10.5.2 Network Graph of Association Rules\nFor a more intuitive, graph-based representation of how items are associated:\n\nimport networkx as nx\n\n# Use top N rules with highest lift\ntop_rules = rules.sort_values(by='lift', ascending=False).head(10)\n\nG = nx.DiGraph()\n\nfor idx, row in top_rules.iterrows():\n    for ante in row['antecedents']:\n        for cons in row['consequents']:\n            G.add_edge(ante, cons, weight=row['lift'])\n\nplt.figure(figsize=(10,8))\npos = nx.spring_layout(G, k=0.5, seed=42)\nedges = G.edges()\nweights = [G[u][v]['weight'] for u,v in edges]\n\nnx.draw_networkx_nodes(G, pos, node_size=700, node_color='lightblue')\nnx.draw_networkx_edges(G, pos, width=weights, edge_color='gray', alpha=0.7)\nnx.draw_networkx_labels(G, pos, font_size=12)\n\nplt.title(\"Network Graph of Top Association Rules (by Lift)\")\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese visualizations help uncover dominant associations, item importance, and structural patterns within the rules—bringing clarity to large or complex datasets.\n\n10.5.2.1 Related Reading/Reference\n\nChapter 5.1, 5.2, 5.3, 5.7 in Introduction to Data Mining, 2nd ed. (Tan et al., 2019)\nChapter 14.3 in Business Analytics: communicating with Numbers, 2nd ed. (Jaggia et al., 2023)",
    "crumbs": [
      "Unsupervised Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Association Rules</span>"
    ]
  },
  {
    "objectID": "App/App-ProbDist.html#overview",
    "href": "App/App-ProbDist.html#overview",
    "title": "Appendix D — Probability Distribution",
    "section": "",
    "text": "D.1.0.1 Related Reading/Reference\n\nChapter 1 in Borshchev, A. and Grigoryev, I. The big book of simulation modeling: multimethod modeling with AnyLogic 8. https://www.anylogic.com/resources/books/big-book-of-simulation-modeling/\nChapter 1 in Robinson, S. (2014). Simulation: the practice of model development and use. Bloomsbury Publishing.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "App/App-ProbDist.html#system",
    "href": "App/App-ProbDist.html#system",
    "title": "Appendix D — Probability Distribution",
    "section": "D.2 System",
    "text": "D.2 System\n\nD.2.1 What is System?\nA system is a group of interacting or interrelated elements that act together as a unified whole. Here, a system operates under a set of rules, meaning that its components follow specific interactions and behaviors.\nIn the real world, systems are all around us, and they exist in various forms:\n\nNatural systems: like the ecosystem, where plants, animals, and climate interact to maintain balance.\nTechnical systems: such as an automobile, which consists of an engine, transmission, and braking system working together.\nBusiness systems: like a supply chain, where suppliers, manufacturers, warehouses, and logistics collaborate to deliver goods to customers.\n\n\n\n\n\n\n\nHow a system can be defined?\n\n\n\nEach system is defined by having its own:\n\nPurpose – the objective or function of the system.\nBoundaries – defining what is inside and what is outside the system.\nStructure – specifying how components are arranged and interact (i.e., how the processes or operations in the system are performed with the components).\n\nFor example, a supply chain system includes multiple stakeholders—raw material suppliers, factories, warehouses, transportation services, and retailers—working together to ensure products reach customers efficiently. External factors such as market demand, government regulations, and technological advancements influence how the system operates.\n\n\n\n\nD.2.2 System Analysis in Real world\nWhen we want to analyze a system, our primary goal is usually to measure, improve, design, or control its characteristics and behaviors. The most direct way to do this is by studying the real system itself.\nHowever, in many cases, working directly with the actual system is either impractical or impossible. There are reasons why directly studying a real system can be challenging:\n\nDisruptive or Expensive: Intervening in a working system could cause major disruptions.\n\nExample: testing different traffic management strategies on a city’s roads could lead to congestion and inconvenience for commuters.\n\nDangerous: Experimenting with a real system could pose safety risks.\n\nExample: a nuclear power plant’s cooling system in real life could pose safety risks. Similarly, testing new medical treatments directly on patients without prior simulation could have serious health consequences.\n\nNonexistent Systems: In some cases, the system we want to study doesn’t yet exist.\n\nExample: If a company wants to build a new airport, there is no real system to test before construction. Instead, simulations are used to predict how passenger flows, security checks, and baggage handling will function.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "intro.html#overview",
    "href": "intro.html#overview",
    "title": "1  Introduction to Courses",
    "section": "",
    "text": "First, we’ll define what data mining is.\nThen, we’ll explore why it is used and where it’s most impactful.\nWe’ll talk about situations and industries where data mining becomes essential.\nNext, we’ll discuss how it is used—the processes and methodologies involved.\nFinally, we’ll look at who actually uses data mining.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Courses</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-mining",
    "href": "intro.html#what-is-data-mining",
    "title": "1  Introduction to Courses",
    "section": "1.2 What is Data Mining?",
    "text": "1.2 What is Data Mining?\n\n1.2.1 Fuels for the Information Age\nTo understand data mining, let’s first start by answering the question What exactly is data mining? There is a famous analogy from “Clive Humby, a British mathematician and entrepreneur, that said, ‘Data is the new oil.’ This analogy helps us understand why data mining is such a crucial process in the modern information age.\nWhat does that mean? Just like crude oil, raw data on its own isn’t very useful. Crude oil has to be refined into products like gasoline, plastic, or jet fuel to be valuable. Similarly, data has to be refined—through cleaning, processing, and analysis—before it generates value.\nThe Information Age has been built on this process. In the past, major waves of development were driven by steam, electricity, and oil. Now, computers, networks, and data form the foundation of progress. Think about it: every time you use Google Maps, watch Netflix, or buy something on Amazon, you’re experiencing data as fuel. Those companies don’t just store data—they refine it into insights: recommending the next show, predicting traffic, or suggesting products.\n\n\n1.2.2 Data Mining in Simple Terms\nAt its simplest, data mining means turning raw data into value. You can think of it as a process of discovering hidden patterns and relationships from data. In the process,\n\nData is the raw material. Think of sales receipts, website clicks, or medical records.\nMining is the process—digging into that data to find useful patterns. Just like miners dig into rock to find gold, we dig into data to find knowledge.\nValue is the outcome, as insights that help us make a difference such as better decisions, efficiency improvement, or predictions.\n\nHere’s a business example: imagine a supermarket chain. They have massive amounts of checkout data. By itself, it’s just numbers. But when mined, it might reveal that customers who buy diapers also buy beer. That insight has value—it can influence store layout or promotions.\nSo whenever you hear the term data mining, think: data goes in, and actionable value comes out. In short: data in, value out.\n\n\n\n\n\n\nNote\n\n\n\nThink of a situation: You’ve got tons of information, but it’s just sitting there. What’s one example where mining that information could turn it into value?\n\n\n\n\n1.2.3 Definition of Data Mining\nData mining is the knowledge discovery from data. It’s the process of extracting interesting patterns or knowledge from huge datasets.\nLet’s highlight a few key parts of that definition: - Interesting means non-trivial—it’s not something obvious. - It should be previously unknown. If everyone already knows it, it’s not knowledge discovery. - And it should be useful—either for making predictions, guiding strategy, or saving resources.\nData mining goes by other names too: knowledge discovery in databases (KDD), knowledge extraction, business intelligence, information harvesting. Besides, there are many but similar and related definitions of data mining, like data analysis and data dredging.\nBut, here, the important distinction is this: data mining is not about finding any pattern—it’s about finding meaningful and valid patterns.\n\n\n\n\n\n\nNote\n\n\n\nIf you notice that every morning the sun rises in the east, does that count as data mining? Why or why not? (Hint: it’s not new or unknown!)\n\n\n\n\n1.2.4 Foundations of Data Mining\nWe just started studying Data mining in this course, but data mining is not a brand-new discipline—it borrows from several established fields. Here are three major fields.\n\nDatabase systems and computing: Without advances in storing and managing large amounts of data, mining would not be possible. Think about SQL databases or distributed systems like Hadoop.\nStatistics: This provides the foundation for measurement, inference, and testing. For example, probability theory underlies many mining methods.\nArtificial Intelligence (AI) and Machine Learning (ML): These give us algorithms that learn patterns from data, like decision trees, clustering, and neural networks.\n\nNow, we may have one following question: why do we need something beyond traditional statistics or databases? Because modern data has new challenges:\n\nIt’s complex—your task may involve not only well-structured tabular format but also, text, video, images, not just numbers. The traditional methods often are not suitable for data mining tasks with such complex data.\nSimilarly, modern data are often heterogeneous—as it may consist of different formats and sources.\nIn addition, data for your tasks may be at a large-scale with millions or billions of records, and high-dimensional with thousands of variables.\nFurthermore, data can be distributed as stored across many servers and locations.\n\nTraditional techniques simply weren’t built to handle this. Data mining techniques evolved to tackle exactly these challenges.\n\n\n\n\n\n\nNote\n\n\n\nCan you think of one dataset you’ve encountered—maybe in your work, studies, or even social media—that seems too big or too complex to handle with just Excel or basic statistics?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Courses</span>"
    ]
  },
  {
    "objectID": "intro.html#why-is-data-mining-used",
    "href": "intro.html#why-is-data-mining-used",
    "title": "1  Introduction to Courses",
    "section": "1.3 Why is data mining used?",
    "text": "1.3 Why is data mining used?\n\n1.3.1 Data and its Explosive Growth\nHere’s the big driver behind why data mining matters: the sheer explosion of data. Data is everywhere, and it keeps growing faster than we can imagine. This growth is driven by advances in:\n\nData generation – Every second, people are generating posts, clicks, videos, GPS pings, and transactions.\nData collection – Sensors, apps, and platforms record nearly everything.\nData storage – Cloud computing now makes it cheap to keep almost unlimited amounts of data.\n\nWe’ve moved from terabytes (10¹² bytes) to petabytes, exabytes, and now zettabytes (10¹⁸ bytes). To give you perspective, one zettabyte is a billion terabytes.\nThe sources of this data are diverse:\n\nBusiness: Online purchases, stock trades, loyalty cards.\nSociety: Social media posts, news feeds, YouTube uploads.\nScience: Remote sensing data from satellites, DNA sequencing, simulation models.\n\nLike these, nowadays, we can collect data nearly everywhere. Here, the reality is–—we’re in an era where we generate more data than we can manually process. That’s why data mining isn’t optional anymore; it’s necessary to make sense of the flood of information.\nExample: Domo’s annual Data Never Sleeps report shows that in just one minute, millions of emails are sent, hundreds of thousands of dollars are spent on e-commerce, and hours of video are uploaded to YouTube. This highlights the nonstop pace of data growth.\n\n\n\n\n\n\nNote\n\n\n\n‘Think about your own life for a moment—your phone, your social media, your browsing history. How much data do you think you generate in a single day? Is most of it being used, or just stored?’”\n\n\n\n\n1.3.2 Value from Data and Data Mining\nNow, here’s the critical point: while data is everywhere, value is not. Collecting data doesn’t guarantee it will be useful to create value. Keep in mind that our goal of data mining here is not to create data, but to create value. To achieve it, we can consider two main ways:\n\nFor its intended purpose.\n\nExample: a hospital records patient vital signs so doctors can monitor health.\n\nFor a new, unanticipated purpose.\n\nExample: analyzing purchase data not just for receipts, but to build customer recommendation systems.\n\n\nData mining is the bridge that transforms massive raw data into valuable insights. Then, what makes this possible today?\n\nFirst, we have now cheaper and more powerful computing. Cloud platforms and GPUs let us crunch massive datasets quickly.\nAt the same time, there are competitive pressure. Companies must use their data to stay ahead—think of how Netflix, Amazon, or Spotify personalize services to keep customers engaged.\n\nReal-world example: Netflix doesn’t just track what you watch; it tracks when you pause, when you stop, how often you binge, and even the time of day you watch. By do data mining, that raw data becomes insights that guide personalized recommendations and even influence what original shows they invest in. So the real message here is: without mining, data is just overhead. However, with data mining, it can become a strategic asset.\n\n\n\n\n\n\nNote\n\n\n\nHere’s a question: Can you think of an example where you know a company is using your data to give you personalized value—like a recommendation, an offer, or an alert? Did it feel helpful or a little creepy?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Courses</span>"
    ]
  },
  {
    "objectID": "intro.html#whenwhere-is-data-mining-needed",
    "href": "intro.html#whenwhere-is-data-mining-needed",
    "title": "1  Introduction to Courses",
    "section": "1.4 When/where is data mining needed?",
    "text": "1.4 When/where is data mining needed?\n\n1.4.1 Process for DM\nData mining isn’t just one single action—it’s part of a larger process that requires careful planning and execution. At its core, data mining means examining large datasets to identify patterns and then using those patterns to generate valuable business insights. But if we just jump into modeling without structure, we risk wasting time or drawing wrong conclusions.\nThat’s why a systematic approach is critical. In industry, there are established standards with clearly defined steps, the two most popular being: - CRISP-DM (Cross-Industry Standard Process for Data Mining) - KDD (Knowledge Discovery in Databases) process\nBoth provide a roadmap, ensuring that projects move from messy raw data to actionable insights in a repeatable and reliable way.\nAlso, remember that data mining is part of a broader data analytics ecosystem that includes machine learning, statistical analysis, and business intelligence tools.\n\n\n\n\n\n\nNote\n\n\n\nImagine you’re hired by a company that wants to reduce customer churn. What could go wrong if you just start building a model without following a systematic process?\n\n\n\n\n1.4.2 Cross-Industry Standard Process for Data Mining (CRISP-DM)\nCRISP-DM is one of the most widely used frameworks for data mining. It consists of six iterative steps:\n\nBusiness understanding – First, we define the context and objectives. What problem are we solving? For example: predicting loan defaults.\nData understanding – Collect raw data and perform preliminary analysis. What data do we have? What’s missing? What hypotheses are emerging?\nData preparation – Clean, transform, and preprocess data. This includes removing errors, handling missing values, and selecting useful variables.\nModeling – Apply appropriate data mining techniques, like classification, clustering, or regression.\nEvaluation – Assess the models. Are the results valid? Do they answer the business question? If not, we loop back.\nDeployment – Translate results into actionable insights or strategies. For example: build a dashboard, send alerts, or automate decision rules.\n\nIt’s important to note that CRISP-DM is not linear. Analysts often move back and forth between stages as new discoveries are made.\n\n\n\n\n\n\nNote\n\n\n\n‘Looking at these six steps, which do you think might take the most time in a real project: modeling or data preparation?’ (Hint: in practice, data preparation usually eats up the majority of time!)”\n\n\n\n\n1.4.3 Knowledge Discovery (KDD) Process\nThe KDD process is another structured framework, often discussed in academic and research communities, especially those focused on databases and data warehousing. The KDD steps look very similar to CRISP-DM but are framed slightly differently: - Data selection – From large databases, choose relevant subsets. - Preprocessing – Clean and prepare the data. - Transformation – Reduce dimensions, select features, normalize variables. - Data mining – Apply algorithms to extract patterns. - Interpretation and evaluation – Translate patterns into useful knowledge.\nSo while CRISP-DM is popular in business practice, KDD is the more theoretical backbone you would often see in academic papers.\n\n\n\n\n\n\nNote\n\n\n\nWhy do you think industry and academia developed slightly different process models for the same overall idea?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Courses</span>"
    ]
  },
  {
    "objectID": "intro.html#how-can-data-mining-be-used",
    "href": "intro.html#how-can-data-mining-be-used",
    "title": "1  Introduction to Courses",
    "section": "1.5 How can data mining be used?",
    "text": "1.5 How can data mining be used?\nLet’s look at the typical tasks of data mining.\nHere’s a sample dataset—things like refund status, marital status, income, and whether someone cheats on taxes. From a dataset like this, what kinds of tasks might we perform?\n\nPredictive modeling: Can we predict whether someone is likely to cheat, given their attributes?\nClustering: Can we group taxpayers into categories—like high risk, medium risk, low risk—without labels?\nAnomaly detection: Can we find cases that look suspiciously different from the rest?\nAssociation rules: Can we uncover relationships—for example, divorced individuals with high income are more or less likely to cheat?\n\nEach of these tasks aligns with a type of real-world decision problem.\n\n\n\n\n\n\nNote\n\n\n\n‘Looking at this small tax dataset, what question would you want to ask first? Prediction, clustering, or anomaly detection?’”\n\n\n\n1.5.1 Predictive Modeling\n“redictive modeling is one of the most powerful applications of data mining. The idea is simple: we use training data—examples where the outcomes are known—to build a model that captures the relationship between inputs (features) and outputs (labels). Then, we apply that model to new, unseen data to predict outcomes.\nThis applies across many domains: - In finance: predicting loan defaults. - In marketing: predicting customer churn. - In healthcare: predicting disease risk.\nWe’ll focus on two main branches:\n\nClassification – when outcomes are categorical.\nRegression – when outcomes are numerical.\n\n\n\n\n\n\n\nNote\n\n\n\nCan you think of a business scenario where we would want to predict a number, and another where we’d want to predict a category?\n\n\n\n1.5.1.1 Predictive Modeling: Classification\nClassification is about predicting non-numerical categorical labels.\nExample: Think about a bank monitoring transactions. Classification models flag suspicious transactions as ‘fraud’ or ‘not fraud.’\nThere are many methods: decision trees, naïve Bayes, support vector machines, neural networks, logistic regression, and rule-based approaches.\n\n\n\n\n\n\nNote\n\n\n\n‘How many of you have had a credit card transaction declined because the bank thought it was suspicious? That’s classification in action—sometimes right, sometimes wrong!’”\n\n\n\n\n1.5.1.2 Predictive Modeling: Regression\nRegression is similar to classification, but here the outcome is numerical. We use regression when we want to predict values like:\n\nFuture sales for a product.\nThe price of a house.\nThe index of a stock market.\n\nExample: A real estate company might use regression to predict house prices based on features like square footage, neighborhood, and number of bedrooms.\nMethods include multiple linear regression, Poisson regression, kernel regression, and support vector regression.\n\n\n\n\n\n\nNote\n\n\n\n‘If Amazon wants to predict how many units of a new product will sell next month, is that classification or regression?’”\n\n\n\n\n\n1.5.2 Clustering\nClustering is about grouping data points into clusters without prior labels. Unlike predictive models where labels are predefined and given for training, clustering discovers categories automatically. Applications include:\n\nMarket segmentation: Grouping customers by behavior or preferences.\nDocument clustering: Grouping related news articles.\nStock market analysis: Grouping stocks with similar trends.\nSummarization: Reducing large datasets into manageable categories.\n\nMethods include k-means, hierarchical clustering, and density-based clustering.\nExample: A retailer could use clustering to identify customer segments—like bargain hunters, premium buyers, or occasional shoppers—without knowing those groups beforehand.\n\n\n\n\n\n\nNote\n\n\n\nThink about Spotify or Apple Music. How do you think clustering helps them group songs or users?\n\n\n\n\n1.5.3 Anomaly Detection\nAnomaly detection is about identifying outliers—cases that deviate significantly from normal patterns. This is incredibly important in practice: - Fraud detection: spotting unusual credit card transactions. - Cybersecurity: detecting intrusions or abnormal network activity. - Healthcare: identifying abnormal patient vitals that signal a problem.\nMethods include statistical tests, clustering-based detection (points far from any cluster), and machine learning methods like one-class support vector machines and neural networks.\nExample: If a customer normally spends $50–$100 per purchase, but suddenly makes a $10,000 purchase in another country, that’s flagged as an anomaly.\n\n\n\n\n\n\nNote\n\n\n\n‘If your Netflix account suddenly started watching cartoons all night long—would that be an anomaly worth investigating?’”\n\n\n\n\n1.5.4 Association Rule Analysis\nAssociation rule analysis is about discovering dependencies between items.\nThe classic example is market-basket analysis: - Rule: {Diapers, Milk} → {Beer}. - This means customers who buy diapers and milk are likely to also buy beer.\nMethods include Apriori algorithm and pattern discovery approaches. Applications extend beyond retail: Telecom: Diagnosing alarms based on combinations of error codes.; Healthcare: Identifying co-occurrence of symptoms or treatments.; E-commerce: Powering recommendation engines (‘People who bought this also bought…’).\nExample: On Amazon, when you see ‘Frequently Bought Together,’ that’s association rule mining in action.\n\n\n\n\n\n\nNote\n\n\n\n‘Think about your own online shopping. Has an item ever been recommended to you that felt spot-on? That’s association rules at work.’”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Courses</span>"
    ]
  },
  {
    "objectID": "intro.html#users-of-dm",
    "href": "intro.html#users-of-dm",
    "title": "1  Introduction to Courses",
    "section": "1.6 Users of DM?",
    "text": "1.6 Users of DM?\nWe have one last question to ask: who actually uses data mining? The short answer is: almost everyone. For example,\n\nAnalysts in various fields: Business analysts, data analysts, and market researchers use mining to guide decisions.\nScientists and engineers: They apply it in areas like bioinformatics, astronomy, materials science, and engineering simulations.\nBusinesses: Companies mine customer data to personalize services, detect fraud, and optimize operations.\nGovernments: For public policy, fraud detection, and even security monitoring.\nComputers and AI systems: Increasingly, automated pipelines powered by machine learning algorithms mine data in real time—think recommendation systems or self-driving cars.\n\nIn practice, much of this happens behind the scenes, through automation. For example, Google’s search engine mines billions of webpages continuously, and Netflix constantly mines viewing data to refine recommendations.\nAnd let’s not forget—you! As students in this class, you’re training to become the next generation of professionals who will design, interpret, and apply these data mining techniques.\n\n\n\n\n\n\nNote\n\n\n\n‘Quick brainstorm: Can you name one profession or industry that you think will rely even more on data mining in the next 5–10 years?’”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Courses</span>"
    ]
  },
  {
    "objectID": "DV/DV-CommonPlots.html",
    "href": "DV/DV-CommonPlots.html",
    "title": "8  Visualization Examples",
    "section": "",
    "text": "8.1 Overview\nThis section introduces to various types of plots using popular visualization packages:",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization Examples</span>"
    ]
  },
  {
    "objectID": "DV/DV-CommonPlots.html#overview",
    "href": "DV/DV-CommonPlots.html#overview",
    "title": "8  Visualization Examples",
    "section": "",
    "text": "Matplotlib (https://matplotlib.org/): This is the “grandparent” of Python visualization libraries. It’s incredibly powerful and provides a high degree of control over every aspect of a plot, making it suitable for creating publication-quality figures. While it can be verbose for simple plots, many other libraries are built on top of Matplotlib, leveraging its capabilities.\nSeaborn (https://seaborn.pydata.org/): Built on Matplotlib, Seaborn provides a higher-level interface for drawing attractive and informative statistical graphics. It simplifies the creation of complex plots like heatmaps, violin plots, pair plots, and categorical plots, making it a favorite for data exploration and analysis, especially when working with Pandas DataFrames.\nPlotly (https://plotly.com/python/): Plotly is a versatile, open-source graphing library that enables the creation of interactive, web-based visualizations. It supports a wide range of chart types (2D, 3D, statistical, scientific, financial, geographic) and integrates seamlessly with Jupyter notebooks, Dash applications, and other web frameworks. Plotly Express, a high-level API, makes it very easy to generate common interactive plots with minimal code.\nBokeh (https://bokeh.org/): Bokeh is another powerful library for creating interactive, web-based visualizations. It excels at delivering high-performance plots that can be embedded into web applications, supporting real-time streaming data for applications like monitoring systems and dashboards.\nAltair (https://altair-viz.github.io/): Altair is a declarative statistical visualization library based on Vega-Lite. It focuses on providing a simple API where users define the structure of the visualization through data and chart specifications. Altair’s concise syntax makes it ideal for quickly prototyping and exploring data interactively, especially with smaller to medium-sized datasets.\n\n\n\n8.1.1 Preliminaries\n\n8.1.1.1 Packages\nFirst, let’s import the libraries for visualizations.\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\nimport altair as alt\nimport plotly.express as px\n\nSecond, let’s import some auxiliary functions/libraries.\n\n# To prevent unnecessary message display\nimport warnings\nwarnings.filterwarnings(\"ignore\")       # Turn off warnings\n\n# For ease of data handling\nimport datetime\nimport pandas_datareader.data as web\n\nfrom pathlib import Path\nfrom itertools import cycle\n\n\n\n8.1.1.2 Data\nFinally, let’s import a vega_datasets package for datasets to be used in the following examples.\n\nfrom vega_datasets import data\ndata.list_datasets()\n\n['7zip',\n 'airports',\n 'annual-precip',\n 'anscombe',\n 'barley',\n 'birdstrikes',\n 'budget',\n 'budgets',\n 'burtin',\n 'cars',\n 'climate',\n 'co2-concentration',\n 'countries',\n 'crimea',\n 'disasters',\n 'driving',\n 'earthquakes',\n 'ffox',\n 'flare',\n 'flare-dependencies',\n 'flights-10k',\n 'flights-200k',\n 'flights-20k',\n 'flights-2k',\n 'flights-3m',\n 'flights-5k',\n 'flights-airport',\n 'gapminder',\n 'gapminder-health-income',\n 'gimp',\n 'github',\n 'graticule',\n 'income',\n 'iowa-electricity',\n 'iris',\n 'jobs',\n 'la-riots',\n 'londonBoroughs',\n 'londonCentroids',\n 'londonTubeLines',\n 'lookup_groups',\n 'lookup_people',\n 'miserables',\n 'monarchs',\n 'movies',\n 'normal-2d',\n 'obesity',\n 'ohlc',\n 'points',\n 'population',\n 'population_engineers_hurricanes',\n 'seattle-temps',\n 'seattle-weather',\n 'sf-temps',\n 'sp500',\n 'stocks',\n 'udistrict',\n 'unemployment',\n 'unemployment-across-industries',\n 'uniform-2d',\n 'us-10m',\n 'us-employment',\n 'us-state-capitals',\n 'volcano',\n 'weather',\n 'weball26',\n 'wheat',\n 'windvectors',\n 'world-110m',\n 'zipcodes']",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization Examples</span>"
    ]
  },
  {
    "objectID": "DV/DV-CommonPlots.html#bar-charts",
    "href": "DV/DV-CommonPlots.html#bar-charts",
    "title": "8  Visualization Examples",
    "section": "8.2 Bar Charts",
    "text": "8.2 Bar Charts\nBar chart\n\n8.2.1 Simple Bar chart\nLet’s see a bar chart, using the ‘barley’ dataset.\n\nbarley = pd.DataFrame(\n  data.barley().groupby([\"site\"])[\"yield\"].sum()\n)\nbarley.head()\n\n\n\n\n\n\n\n\nyield\n\n\nsite\n\n\n\n\n\nCrookston\n748.39997\n\n\nDuluth\n559.93334\n\n\nGrand Rapids\n498.63334\n\n\nMorris\n708.00001\n\n\nUniversity Farm\n653.33335\n\n\n\n\n\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\nJust remove the ‘h’ in ax.barh() to get a vertical plot.\n\nfig, ax = plt.subplots()\nax.barh(barley[\"yield\"].index, barley[\"yield\"], 0.35)\nax.set_xlabel(\"Yield\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nJust switch x and y variables to get a vertical plot.\n\n(\n    so.Plot(barley.reset_index(), x=\"yield\", y=\"site\", color=\"site\").add(so.Bar(), so.Agg())\n)\n\n\n\n\n\n\n\n\n\n\nJust switch x and y to get a vertical plot.\n\nalt.Chart(barley.reset_index()).mark_bar().encode(\n    y=\"site\",\n    x=\"yield\",\n).properties(\n    width=alt.Step(40)  # controls width of bar.\n)\n\n\n\n\n\n\n\n\n\n\nfig = px.bar(barley.reset_index(), y=\"site\", x=\"yield\")\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n8.2.2 Grouped bar chart\n\nbarley = data.barley()\nbarley = pd.DataFrame(barley.groupby([\"site\", \"year\"])[\"yield\"].sum()).reset_index()\nbarley.head()\n\n\n\n\n\n\n\n\nsite\nyear\nyield\n\n\n\n\n0\nCrookston\n1931\n436.59999\n\n\n1\nCrookston\n1932\n311.79998\n\n\n2\nDuluth\n1931\n302.93333\n\n\n3\nDuluth\n1932\n257.00001\n\n\n4\nGrand Rapids\n1931\n290.53335\n\n\n\n\n\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\n\nlabels = barley[\"site\"].unique()\ny = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots()\nax.barh(y - width / 2, barley.loc[barley[\"year\"] == 1931, \"yield\"], width, label=\"1931\")\nax.barh(y + width / 2, barley.loc[barley[\"year\"] == 1932, \"yield\"], width, label=\"1932\")\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_xlabel(\"Yield\")\nax.set_yticks(y)\nax.set_yticklabels(labels)\nax.legend(frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nbarley[\"year\"] = barley[\"year\"].astype(\"category\")  # to force category\n\n(\n    so.Plot(barley.reset_index(), x=\"yield\", y=\"site\", color=\"year\").add(\n        so.Bar(), so.Dodge()\n    )\n)\n\n\n\n\n\n\n\n\n\n\n\nalt.Chart(barley.reset_index()).mark_bar().encode(\n    y=\"year:O\", x=\"yield\", color=\"year:N\", row=\"site:N\"\n).properties(\n    width=alt.Step(40)  # controls width of bar.\n)\n\n\n\n\n\n\n\n\n\n\npx_barley = barley.reset_index()\n# This prevents plotly from using a continuous scale for year\npx_barley[\"year\"] = px_barley[\"year\"].astype(\"category\")\nfig = px.bar(px_barley, y=\"site\", x=\"yield\", barmode=\"group\", color=\"year\")\nfig.show()\n\n                            \n                                            \n\n\n\n\n\n\n\n\n8.2.3 Stacked bar chart\npreprocessing a bit\n\nlabels = barley[\"site\"].unique()\ny = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width (or height) of the bars\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\n\nfig, ax = plt.subplots()\nax.barh(y, barley.loc[barley[\"year\"] == 1931, \"yield\"], width, label=\"1931\")\nax.barh(\n    y,\n    barley.loc[barley[\"year\"] == 1932, \"yield\"],\n    width,\n    label=\"1932\",\n    left=barley.loc[barley[\"year\"] == 1931, \"yield\"],\n)\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_xlabel(\"Yield\")\nax.set_yticks(y)\nax.set_yticklabels(labels)\nax.legend(frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nbarley[\"year\"] = barley[\"year\"].astype(\"category\")  # to force category\n(\n    so.Plot(barley.reset_index(), x=\"yield\", y=\"site\", color=\"year\").add(\n        so.Bar(), so.Stack()\n    )\n)\n\n\n\n\n\n\n\n\n\n\n\nalt.Chart(barley.reset_index()).mark_bar().encode(\n    y=\"site\",\n    x=\"yield\",\n    color=\"year:N\",\n).properties(\n    width=alt.Step(40)  # controls width of bar.\n)\n\n\n\n\n\n\n\n\n\n\nfig = px.bar(px_barley, y=\"site\", x=\"yield\", barmode=\"relative\", color=\"year\")\nfig.show()\n\n                            \n                                            \n\n\n\n\n\n\n\n\n8.2.4 Diverging stacked bar chart\nFirst, let’s create some data to use in our examples.\n\ncategory_names = [\n    \"Strongly disagree\",\n    \"Disagree\",\n    \"Neither agree nor disagree\",\n    \"Agree\",\n    \"Strongly agree\",\n]\nresults = [\n    [10, 15, 17, 32, 26],\n    [26, 22, 29, 10, 13],\n    [35, 37, 7, 2, 19],\n    [32, 11, 9, 15, 33],\n    [21, 29, 5, 5, 40],\n    [8, 19, 5, 30, 38],\n]\n\nlikert_df = pd.DataFrame(\n    results, columns=category_names, index=[f\"Question {i}\" for i in range(1, 7)]\n)\nlikert_df\n\n\n\n\n\n\n\n\nStrongly disagree\nDisagree\nNeither agree nor disagree\nAgree\nStrongly agree\n\n\n\n\nQuestion 1\n10\n15\n17\n32\n26\n\n\nQuestion 2\n26\n22\n29\n10\n13\n\n\nQuestion 3\n35\n37\n7\n2\n19\n\n\nQuestion 4\n32\n11\n9\n15\n33\n\n\nQuestion 5\n21\n29\n5\n5\n40\n\n\nQuestion 6\n8\n19\n5\n30\n38\n\n\n\n\n\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\n\nmiddle_index = likert_df.shape[1] // 2\noffsets = (\n    likert_df.iloc[:, range(middle_index)].sum(axis=1)\n    + likert_df.iloc[:, middle_index] / 2\n)\ncategory_colors = plt.get_cmap(\"coolwarm_r\")(\n    np.linspace(0.15, 0.85, likert_df.shape[1])\n)\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Plot Bars\nfor i, (colname, color) in enumerate(zip(likert_df.columns, category_colors)):\n    widths = likert_df.iloc[:, i]\n    starts = likert_df.cumsum(axis=1).iloc[:, i] - widths - offsets\n    rects = ax.barh(\n        likert_df.index, widths, left=starts, height=0.5, label=colname, color=color\n    )\n\n# Add Zero Reference Line\nax.axvline(0, linestyle=\"--\", color=\"black\", alpha=1, zorder=0, lw=0.3)\n\n# X Axis\nax.set_xlim(-90, 90)\nax.set_xticks(np.arange(-90, 91, 10))\nax.xaxis.set_major_formatter(lambda x, pos: str(abs(int(x))))\n\n# Y Axis\nax.invert_yaxis()\n\n# Remove spines\nax.spines[\"right\"].set_visible(False)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"left\"].set_visible(False)\n\n# Legend\nax.legend(\n    ncol=len(category_names),\n    bbox_to_anchor=(0, 1),\n    loc=\"lower left\",\n    fontsize=\"small\",\n    frameon=False,\n)\n\n# Set Background Color\nfig.set_facecolor(\"#FFFFFF\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n#nah\n\n\n\n\n#nah\n\n\n\n\n#nah\n\n\n\n\n\n\n\n8.2.5 Pyramid\nData\n\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/selva86/datasets/master/email_campaign_funnel.csv\"\n)\ndf.head()\n\n\n\n\n\n\n\n\nStage\nGender\nUsers\n\n\n\n\n0\nStage 01: Browsers\nMale\n-1.492762e+07\n\n\n1\nStage 02: Unbounced Users\nMale\n-1.286266e+07\n\n\n2\nStage 03: Email Signups\nMale\n-1.136190e+07\n\n\n3\nStage 04: Email Confirmed\nMale\n-9.411708e+06\n\n\n4\nStage 05: Campaign-Email Opens\nMale\n-8.074317e+06\n\n\n\n\n\n\n\n\n\nMatplotlib/SeabornSeabornAltairPlotly\n\n\n\nfig, ax = plt.subplots()\ngroup_col = \"Gender\"\norder_of_bars = df.Stage.unique()[::-1]\ncolors = [\n    plt.cm.Spectral(i / float(len(df[group_col].unique()) - 1))\n    for i in range(len(df[group_col].unique()))\n]\n\nfor c, group in zip(colors, df[group_col].unique()):\n    sns.barplot(\n        x=\"Users\",\n        y=\"Stage\",\n        data=df.loc[df[group_col] == group, :],\n        order=order_of_bars,\n        color=c,\n        label=group,\n        ax=ax,\n        lw=0,\n    )\n\ndivisor = 1e6\nax.set_xticklabels([str(abs(x) / divisor) for x in ax.get_xticks()])\nplt.xlabel(\"Users (millions)\")\nplt.ylabel(\"Stage of Purchase\")\nplt.yticks(fontsize=12)\nplt.title(\"Population Pyramid of the Marketing Funnel\", fontsize=22)\nplt.legend(frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nnah\n\n\nnah\n\n\n\nfig = px.funnel(df, y=\"Stage\", x=\"Users\")\nfig.show()\n\n                            \n                                            \n\n\n\n\n\n\n\n\n8.2.6 Waterfall chart\nWaterfall charts are good for showing how different contributions combine to net out at a certain value. There’s a package dedicated to them called waterfallcharts. It builds on matplotlib. First, let’s create some data:\n\na = [\"sales\", \"returns\", \"credit fees\", \"rebates\", \"late charges\", \"shipping\"]\nb = [10, -30, -7.5, -25, 95, -7]\n\nNow let’s plot this data. Because the defaults of waterfallcharts don’t play that nicely with the plot style used for this book, we’ll temporarily switch back to the matplotlib default plot style using a context and with statement:\n\n\nMatplotlibSeabornAltairPlotly\n\n\n\nimport waterfall_chart\n\nwith plt.style.context(\"default\"):\n    plot = waterfall_chart.plot(a, b, sorted_value=True, rotation_value=0)\n\n\n\n\n\n\n\n\n\n\nnah\n\n\nnah\n\n\n\nimport plotly.graph_objects as go\n\npx_b = b + [sum(b)]\n\nfig = go.Figure(\n    go.Waterfall(\n        name=\"20\",\n        orientation=\"v\",\n        measure=[\"relative\"] * len(a) + [\"total\"],\n        x=a + [\"net\"],\n        textposition=\"outside\",\n        text=[str(x) for x in b] + [\"net\"],\n        y=px_b,\n        connector={\"line\": {\"color\": \"rgb(63, 63, 63)\"}},\n    )\n)\n\nfig.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization Examples</span>"
    ]
  },
  {
    "objectID": "DV/DV-CommonPlots.html#line-area-charts",
    "href": "DV/DV-CommonPlots.html#line-area-charts",
    "title": "8  Visualization Examples",
    "section": "8.3 Line & Area Charts",
    "text": "8.3 Line & Area Charts\n\n8.3.1 Line plot\nFirst, let’s get some data on GDP growth:\n\ntodays_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\nfred_df = web.DataReader([\"GDPC1\", \"NGDPRSAXDCGBQ\"], \"fred\", \"1999-01-01\", \"2021-12-31\")\nfred_df.columns = [\"US\", \"UK\"]\nfred_df.index.name = \"Date\"\nfred_df = 100 * fred_df.pct_change(4)\ndf = pd.melt(\n    fred_df.reset_index(),\n    id_vars=[\"Date\"],\n    value_vars=fred_df.columns,\n    value_name=\"Real GDP growth, %\",\n    var_name=\"Country\",\n)\ndf = df.set_index(\"Date\")\ndf.head()\n\n\n\n\n\n\n\n\nCountry\nReal GDP growth, %\n\n\nDate\n\n\n\n\n\n\n1999-01-01\nUS\nNaN\n\n\n1999-04-01\nUS\nNaN\n\n\n1999-07-01\nUS\nNaN\n\n\n1999-10-01\nUS\nNaN\n\n\n2000-01-01\nUS\n4.224745\n\n\n\n\n\n\n\n\n\nMatplotlibSeaborn\n\n\nNote that Matplotlib prefers data to be one variable per column, in which case we could have just run\n\nfig, ax = plt.subplots()\ndf.plot(ax=ax)\nax.set_title('Real GDP growth, %', loc='right')\nax.yaxis.tick_right()\n\n\n\n\n\n\n\n\nbut we are working with tidy data here, so we’ll do the plotting slightly differently.\n\nfig, ax = plt.subplots()\nfor i, country in enumerate(df[\"Country\"].unique()):\n    df_sub = df[df[\"Country\"] == country]\n    ax.plot(df_sub.index, df_sub[\"Real GDP growth, %\"], label=country, lw=2)\nax.set_title(\"Real GDP growth per capita, %\", loc=\"right\")\nax.yaxis.tick_right()\nax.spines[\"right\"].set_visible(True)\nax.spines[\"left\"].set_visible(False)\nax.legend(loc=\"lower left\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote that only some seaborn commands currently support the use of named indexes, so we use df.reset_index() to make the ‘Date’ index into a regular column in the snippet below (although in recent versions of seaborn, lineplot() would actually work fine with data=df):\n\nfig, ax = plt.subplots()\ny_var = \"Real GDP growth, %\"\nsns.lineplot(x=\"Date\", y=y_var, hue=\"Country\", data=df.reset_index(), ax=ax)\nax.yaxis.tick_right()\nax.spines[\"right\"].set_visible(True)\nax.spines[\"left\"].set_visible(False)\nax.set_ylabel(\"\")\nax.set_title(y_var)\nplt.show()\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(df.reset_index(), x=\"Date\", y=\"Real GDP growth, %\", color=\"Country\").add(so.Line())\n)\n\n\n\n\n\n\n\n\n\n8.3.1.1 Altair\n\nalt.Chart(df.reset_index()).mark_line().encode(\n    x=\"Date:T\",\n    y=\"Real GDP growth, %\",\n    color=\"Country\",\n    strokeDash=\"Country\",\n)\n\n\n\n\n\n\n\n\n\n8.3.1.2 Plotly\n\nfig = px.line(\n    df.reset_index(),\n    x=\"Date\",\n    y=\"Real GDP growth, %\",\n    color=\"Country\",\n    line_dash=\"Country\",\n)\nfig.show()\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\n8.3.2 Overlapping Area plot\nFor this, let’s look at the dominance of the three most used methods for detecting exoplanets.\n\nplanets = sns.load_dataset(\"planets\")\nmost_pop_methods = (\n    planets.groupby([\"method\"])[\"number\"]\n    .sum()\n    .sort_values(ascending=False)\n    .index[:3]\n    .values\n)\nplanets = planets[planets[\"method\"].isin(most_pop_methods)]\nplanets.head()\n\n\n\n\n\n\n\n\nmethod\nnumber\norbital_period\nmass\ndistance\nyear\n\n\n\n\n0\nRadial Velocity\n1\n269.300\n7.10\n77.40\n2006\n\n\n1\nRadial Velocity\n1\n874.774\n2.21\n56.95\n2008\n\n\n2\nRadial Velocity\n1\n763.000\n2.60\n19.84\n2011\n\n\n3\nRadial Velocity\n1\n326.030\n19.40\n110.62\n2007\n\n\n4\nRadial Velocity\n1\n516.220\n10.50\n119.47\n2009\n\n\n\n\n\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\nThe easiest way to do this in matplotlib is to adjust the data a bit first and then use the built-in pandas plot function. (This is true in other cases too, but in this case it’s much more complex otherwise).\n\n(\n    planets.groupby([\"year\", \"method\"])[\"number\"]\n    .sum()\n    .unstack()\n    .plot.area(alpha=0.6, ylim=(0, None))\n    .set_title(\"Planets dicovered by top 3 methods\", loc=\"left\")\n);\n\n\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(\n        planets.groupby([\"year\", \"method\"])[\"number\"].sum().reset_index(),\n        x=\"year\",\n        y=\"number\",\n        color=\"method\",\n    ).add(so.Area(alpha=0.3), so.Agg(), so.Stack())\n)\n\n\n\n\n\n\n\n\n\n\n\nalt.Chart(\n    planets.groupby([\"year\", \"method\"])[\"number\"]\n    .sum()\n    .reset_index()\n    .assign(\n        year=lambda x: pd.to_datetime(x[\"year\"], format=\"%Y\")\n        + pd.tseries.offsets.YearEnd()\n    )\n).mark_area().encode(x=\"year:T\", y=\"number:Q\", color=\"method:N\")\n\n\n\n\n\n\n\n\n\nnah",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization Examples</span>"
    ]
  },
  {
    "objectID": "DV/DV-CommonPlots.html#circular-charts",
    "href": "DV/DV-CommonPlots.html#circular-charts",
    "title": "8  Visualization Examples",
    "section": "8.4 Circular Charts",
    "text": "8.4 Circular Charts\n\n8.4.1 Polar\nI’m not sure I’ve ever seen a polar plots in economics, but you never know.\nLet’s generate some polar data first:\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\npolar_data = pd.DataFrame({\"r\": r, \"theta\": theta})\npolar_data.head()\n\n\n\n\n\n\n\n\nr\ntheta\n\n\n\n\n0\n0.00\n0.000000\n\n\n1\n0.01\n0.062832\n\n\n2\n0.02\n0.125664\n\n\n3\n0.03\n0.188496\n\n\n4\n0.04\n0.251327\n\n\n\n\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\n\nax = plt.subplot(111, projection=\"polar\")\nax.plot(polar_data[\"theta\"], polar_data[\"r\"])\nax.set_rmax(2)\nax.set_rticks([0.5, 1, 1.5, 2])  # Fewer radial ticks\nax.set_rlabel_position(-22.5)  # Move radial labels away from plotted line\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nnah\n\n\nnah\n\n\n\nfig = go.Figure(\n    data=go.Scatterpolar(\n        r=polar_data[\"r\"].values,\n        theta=polar_data[\"theta\"].values * 180 / (np.pi),\n        mode=\"lines\",\n    )\n)\n\nfig.update_layout(showlegend=False)\nfig.show()\n\n                            \n                                            \n\n\n\n\n\n\n\n\n8.4.2 Radar (or spider) chart\nLet’s generate some synthetic data for this one. Assumes that result to be shown is the sum of observations.\n\n# Set seed for reproducibility\nseed_for_prng = 78557\nprng = np.random.default_rng(seed_for_prng)  # prng=probabilistic random number generator\n\ndf = pd.DataFrame(\n    dict(\n        zip(\n            [\"var\" + str(i) for i in range(1, 6)],\n            [np.random.randint(30, size=(4)) for i in range(1, 6)],\n        )\n    )\n)\ndf.head()\n\n\n\n\n\n\n\n\nvar1\nvar2\nvar3\nvar4\nvar5\n\n\n\n\n0\n22\n14\n21\n0\n3\n\n\n1\n14\n1\n19\n12\n21\n\n\n2\n25\n16\n22\n22\n15\n\n\n3\n22\n6\n17\n8\n25\n\n\n\n\n\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\n\nfrom math import pi\n\ndef radar_plot(data, variables):\n    n_vars = len(variables)\n    # Plot the first line of the data frame.\n    # Repeat the first value to close the circular graph:\n    values = data.loc[data.index[0], variables].values.flatten().tolist()\n    values += values[:1]\n    # What will be the angle of each axis in the plot? (we divide / number of variable)\n    angles = [n / float(n_vars) * 2 * pi for n in range(n_vars)]\n    angles += angles[:1]\n    # Initialise the spider plot\n    ax = plt.subplot(111, polar=True)\n    # Draw one axe per variable + add labels\n    plt.xticks(angles[:-1], variables)\n    # Draw ylabels\n    ax.set_rlabel_position(0)\n    # Plot data\n    ax.plot(angles, values, linewidth=1, linestyle=\"solid\")\n    # Fill area\n    ax.fill(angles, values, \"b\", alpha=0.1)\n    return ax\n\n\nradar_plot(df, df.columns);\n\n\n\n\n\n\n\n\n\n\nnah\n\n\nnah\n\n\n\ndf = px.data.wind()\nprint(df.head())\nfig = px.line_polar(\n    df,\n    r=\"frequency\",\n    theta=\"direction\",\n    color=\"strength\",\n    line_close=True,\n    color_discrete_sequence=px.colors.sequential.Plasma_r,\n    template=\"plotly_dark\",\n)\nfig.show()\n\n  direction strength  frequency\n0         N      0-1        0.5\n1       NNE      0-1        0.6\n2        NE      0-1        0.5\n3       ENE      0-1        0.4\n4         E      0-1        0.4\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n8.4.3 Contour Plot\nContour plots can help you show how a third variable, Z, varies with both X and Y (ie Z is a surface). The way that Z is depicted could be via the density of lines drawn in the X-Y plane (use ax.contour() for this) or via colour, as in the example below (using ax.contourf()).\nThe heatmap (or contour plot) below, which has a colour bar legend and a title that’s rendered with latex, uses a perceptually uniform distribution that makes equal changes look equal; matplotlib has a few of these. If you need more colours, check out the packages colorcet and palettable.\n\n\nMatplotlibSeabornAltairPlotly\n\n\nNote that, in the below, Z is returned by a function that accepts a grid of X and Y values.\n\ndef f(x, y):\n    return np.sin(x) ** 10 + np.cos(10 + y * x) * np.cos(x)\n\n\nx = np.linspace(0, 5, 100)\ny = np.linspace(0, 5, 100)\n\nX, Y = np.meshgrid(x, y)\nZ = f(X, Y)\n\nfig, ax = plt.subplots()\ncf = ax.contourf(X, Y, Z, cmap=\"plasma\")\nax.set_title(r\"$f(x,y) = \\sin^{10}(x) + \\cos(x)\\cos\\left(10 + y\\cdot x\\right)$\")\ncbar = fig.colorbar(cf);\n\n\n\n\n\n\n\n\n\n\nnah\n\n\nnah\n\n\n\nimport plotly.graph_objects as go\n\ngrid_fig = go.Figure(data=go.Contour(z=Z, x=x, y=y))\n\ngrid_fig.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization Examples</span>"
    ]
  },
  {
    "objectID": "DV/DV-CommonPlots.html#scatter-plot",
    "href": "DV/DV-CommonPlots.html#scatter-plot",
    "title": "8  Visualization Examples",
    "section": "8.5 Scatter Plot",
    "text": "8.5 Scatter Plot\n\n8.5.1 Scatter plot\nIn this example, we will see a simple scatter plot with several categories using the “cars” data:\n\ncars = data.cars()\ncars.head()\n\n\n\n\n\n\n\n\nName\nMiles_per_Gallon\nCylinders\nDisplacement\nHorsepower\nWeight_in_lbs\nAcceleration\nYear\nOrigin\n\n\n\n\n0\nchevrolet chevelle malibu\n18.0\n8\n307.0\n130.0\n3504\n12.0\n1970-01-01\nUSA\n\n\n1\nbuick skylark 320\n15.0\n8\n350.0\n165.0\n3693\n11.5\n1970-01-01\nUSA\n\n\n2\nplymouth satellite\n18.0\n8\n318.0\n150.0\n3436\n11.0\n1970-01-01\nUSA\n\n\n3\namc rebel sst\n16.0\n8\n304.0\n150.0\n3433\n12.0\n1970-01-01\nUSA\n\n\n4\nford torino\n17.0\n8\n302.0\n140.0\n3449\n10.5\n1970-01-01\nUSA\n\n\n\n\n\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\n\nfig, ax = plt.subplots()\nfor origin in cars[\"Origin\"].unique():\n    cars_sub = cars[cars[\"Origin\"] == origin]\n    ax.scatter(cars_sub[\"Horsepower\"], cars_sub[\"Miles_per_Gallon\"], label=origin)\nax.set_ylabel(\"Miles per Gallon\")\nax.set_xlabel(\"Horsepower\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote that this uses the seaborn objects API.\n\n(\n    so.Plot(cars, x=\"Horsepower\", y=\"Miles_per_Gallon\", color=\"Origin\").add(so.Dot())\n)\n\n\n\n\n\n\n\n\n\n\nFor this first example, we’ll also show how to make the altair plot interactive with movable axes and a tooltip that reveals more info when you hover your mouse over points.\n\nalt.Chart(cars).mark_circle(size=60).encode(\n    x=\"Horsepower\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    tooltip=[\"Name\", \"Origin\", \"Horsepower\", \"Miles_per_Gallon\"],\n).interactive()\n\n\n\n\n\n\n\n\n\nPlotly is another declarative plotting library, at least sometimes (!), but one that is interactive by default.\n\nfig = px.scatter(\n    cars,\n    x=\"Horsepower\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    hover_data=[\"Name\", \"Origin\", \"Horsepower\", \"Miles_per_Gallon\"],\n)\nfig.show()\n\n                            \n                                            \n\n\n\n\n\n\n\n\n8.5.2 Facets\nThis applies to all plots, so in some sense is common! Facets, aka panels or small multiples, are ways of showing the same chart multiple times. Let’s see how to achieve them in a few of the most popular plotting libraries.\nWe’ll use the “tips” dataset for this.\n\ndf = sns.load_dataset(\"tips\")\ndf.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\nThere are many ways to create facets using Matplotlib, and you can get facets in any shape or sizes you like.\nThe easiest way, though, is to specify the number of rows and columns. This is achieved by specifying nrows and ncols when calling plt.subplots(). It returns an array of shape (nrows, ncols) of Axes objects. For most purposes, you’ll want to flatten these to a vector before iterating over them.\n\nfig, axes = plt.subplots(nrows=1, ncols=4, sharex=True, sharey=True)\nflat_axes = axes.flatten()  # Not needed with 1 row or 1 col, but good to be aware of\n\nfacet_grp = list(df[\"day\"].unique())\n# This part just to get some colours from the default color cycle\ncolour_list = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n\n\niter_cycle = cycle(colour_list)\n\nfor i, ax in enumerate(flat_axes):\n    sub_df = df.loc[df[\"day\"] == facet_grp[i]]\n    ax.scatter(\n        sub_df[\"tip\"],\n        sub_df[\"total_bill\"],\n        s=30,\n        edgecolor=\"k\",\n        color=next(iter_cycle),\n    )\n    ax.set_title(facet_grp[i])\nfig.text(0.5, 0.01, \"Tip\", ha=\"center\")\nfig.text(0.0, 0.5, \"Total bill\", va=\"center\", rotation=\"vertical\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nDifferent facet sizes are possible in numerous ways. In practice, it’s often better to have evenly sized facets laid out in a grid–especially each facet is of the same x and y axes. But, just to show it’s possible, here’s an example that gives more space to the weekend than to weekdays using the tips dataset:\n\n# This part just to get some colours\ncolormap = plt.cm.Dark2\n\nfig = plt.figure(constrained_layout=True)\nax_dict = fig.subplot_mosaic([[\"Thur\", \"Fri\", \"Sat\", \"Sat\", \"Sun\", \"Sun\"]])\nfacet_grp = list(ax_dict.keys())\ncolorst = [colormap(i) for i in np.linspace(0, 0.9, len(facet_grp))]\nfor i, grp in enumerate(facet_grp):\n    sub_df = df.loc[df[\"day\"] == facet_grp[i]]\n    ax_dict[grp].scatter(\n        sub_df[\"tip\"],\n        sub_df[\"total_bill\"],\n        s=30,\n        edgecolor=\"k\",\n        color=colorst[i],\n    )\n    ax_dict[grp].set_title(facet_grp[i])\n    if grp != \"Thurs\":\n        ax_dict[grp].set_yticklabels([])\nplt.tight_layout()\nfig.text(0.5, 0, \"Tip\", ha=\"center\")\nfig.text(0, 0.5, \"Total bill\", va=\"center\", rotation=\"vertical\")\nplt.show()\n\n\n\n\n\n\n\n\nAs well as using lists, you can also specify the layout using an array or using text, eg\n\naxd = plt.figure(constrained_layout=True).subplot_mosaic(\n    \"\"\"\n    ABD\n    CCD\n    CC.\n    \"\"\"\n)\nkw = dict(ha=\"center\", va=\"center\", fontsize=60, color=\"darkgrey\")\nfor k, ax in axd.items():\n    ax.text(0.5, 0.5, k, transform=ax.transAxes, **kw)\n\n\n\n\n\n\n\n\n\n\nSeaborn makes it easy to quickly create facet plots. Note the use of col_wrap.\n\n(\n    so.Plot(df, x=\"tip\", y=\"total_bill\", color=\"day\")\n    .facet(col=\"day\", wrap=2)\n    .add(so.Dot())\n)\n\n\n\n\n\n\n\n\nA nice feature of seaborn that is much more fiddly in (base) matplotlib is the ability to specify rows and columns separately: (smoker)\n\n(\n    so.Plot(df, x=\"tip\", y=\"total_bill\", color=\"day\")\n    .facet(col=\"day\", row=\"smoker\")\n    .add(so.Dot())\n)\n\n\n\n\n\n\n\n\n\n\n\nalt.Chart(df).mark_point().encode(\n    x=\"tip:Q\",\n    y=\"total_bill:Q\",\n    color=\"smoker:N\",\n    facet=alt.Facet(\"day:N\", columns=2),\n).properties(\n    width=200,\n    height=100,\n)\n\n\n\n\n\n\n\n\n\n\nfig = px.scatter(\n    df, x=\"tip\", y=\"total_bill\", color=\"smoker\", facet_row=\"smoker\", facet_col=\"day\"\n)\nfig.show()\n\n                            \n                                            \n\n\n\n\n\n\n\n\n8.5.3 Connected scatter plot\nA simple variation on the scatter plot designed to show an ordering, usually of time. We’ll trace out a Beveridge curve based on US data.\n\nstart = datetime.datetime(2000, 1, 1)\nend = datetime.datetime(datetime.datetime.now().year, 1, 1)\ncode_dict = {\n    \"Vacancies\": \"LMJVTTUVUSA647N\",\n    \"Unemployment\": \"UNRATE\",\n    \"LabourForce\": \"CLF16OV\",\n}\nlist_dfs = [\n    web.DataReader(value, \"fred\", start, end)\n    .rename(columns={value: key})\n    .groupby(pd.Grouper(freq=\"AS\"))\n    .mean()\n    for key, value in code_dict.items()\n]\ndf = pd.concat(list_dfs, axis=1)\ndf = df.assign(Vacancies=100 * df[\"Vacancies\"] / (df[\"LabourForce\"] * 1e3)).dropna()\ndf[\"Year\"] = df.index.year\ndf.head()\n\n\n\n\n\n\n\n\nVacancies\nUnemployment\nLabourForce\nYear\n\n\nDATE\n\n\n\n\n\n\n\n\n2001-01-01\n3.028239\n4.741667\n143768.916667\n2001\n\n\n2002-01-01\n2.387254\n5.783333\n144856.083333\n2002\n\n\n2003-01-01\n2.212238\n5.991667\n146499.500000\n2003\n\n\n2004-01-01\n2.470209\n5.541667\n147379.583333\n2004\n\n\n2005-01-01\n2.753326\n5.083333\n149289.166667\n2005\n\n\n\n\n\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\n\nplt.close(\"all\")\nfig, ax = plt.subplots()\nquivx = -df[\"Unemployment\"].diff(-1)\nquivy = -df[\"Vacancies\"].diff(-1)\n# This connects the points\nax.quiver(\n    df[\"Unemployment\"],\n    df[\"Vacancies\"],\n    quivx,\n    quivy,\n    scale_units=\"xy\",\n    angles=\"xy\",\n    scale=1,\n    width=0.006,\n    alpha=0.3,\n)\nax.scatter(\n    df[\"Unemployment\"],\n    df[\"Vacancies\"],\n    marker=\"o\",\n    s=35,\n    edgecolor=\"black\",\n    linewidth=0.2,\n    alpha=0.9,\n)\nfor j in [0, -1]:\n    ax.annotate(\n        df[\"Year\"].iloc[j],\n        xy=(df[[\"Unemployment\", \"Vacancies\"]].iloc[j].tolist()),\n        xycoords=\"data\",\n        xytext=(-20, -40),\n        textcoords=\"offset points\",\n        arrowprops=dict(arrowstyle=\"-&gt;\", connectionstyle=\"angle3,angleA=0,angleB=-90\"),\n    )\nax.set_xlabel(\"Unemployment rate, %\")\nax.set_ylabel(\"Vacancy rate, %\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(df, x=\"Unemployment\", y=\"Vacancies\")\n    .add(so.Dots())\n    .add(so.Path(marker=\"o\"))\n    .label(\n        x=\"Unemployment rate, %\",\n        y=\"Vacancy rate, %\",\n    )\n)\n\n\n\n\n\n\n\n\n\n\nNah\n\n\nNah\n\n\n\n\n\n\n8.5.4 Bubble plot\nThis is a scatter plot where the size of the point carries an extra dimension of information.\n\n\nMatplotlibSeabornAltairPlotly\n\n\n\nfig, ax = plt.subplots()\nscat = ax.scatter(\n    cars[\"Horsepower\"], cars[\"Miles_per_Gallon\"], s=cars[\"Displacement\"], alpha=0.4\n)\nax.set_ylabel(\"Miles per Gallon\")\nax.set_xlabel(\"Horsepower\")\nax.legend(\n    *scat.legend_elements(prop=\"sizes\", num=4),\n    loc=\"upper right\",\n    title=\"Displacement\",\n    frameon=False,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(cars, x=\"Horsepower\", y=\"Miles_per_Gallon\", pointsize=\"Displacement\").add(\n        so.Dot()\n    )\n)\n\n\n\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_circle().encode(\n    x=\"Horsepower\", y=\"Miles_per_Gallon\", size=\"Displacement\"\n)\n\n\n\n\n\n\n\n\n\n\n# Adding a new col is easiest way to get displacement into legend with plotly:\ncars[\"Displacement_Size\"] = pd.cut(cars[\"Displacement\"], bins=4)\nfig = px.scatter(\n    cars,\n    x=\"Horsepower\",\n    y=\"Miles_per_Gallon\",\n    size=\"Displacement\",\n    color=\"Displacement_Size\",\n)\nfig.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization Examples</span>"
    ]
  },
  {
    "objectID": "DV/DV-CommonPlots.html#tables",
    "href": "DV/DV-CommonPlots.html#tables",
    "title": "8  Visualization Examples",
    "section": "8.6 Tables",
    "text": "8.6 Tables\n\n8.6.1 Heatmap\nHeatmaps, or sometimes known as correlation maps, represent data in 3 dimensions by having two axes that forms a grid showing colour that corresponds to (usually) continuous values.\nWe’ll use the flights data to show the number of passengers by month-year:\n\nflights = sns.load_dataset(\"flights\")\nflights = flights.pivot(index=\"month\", columns=\"year\", values=\"passengers\").T\nflights.head()\n\n\n\n\n\n\n\nmonth\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1949\n112\n118\n132\n129\n121\n135\n148\n148\n136\n119\n104\n118\n\n\n1950\n115\n126\n141\n135\n125\n149\n170\n170\n158\n133\n114\n140\n\n\n1951\n145\n150\n178\n163\n172\n178\n199\n199\n184\n162\n146\n166\n\n\n1952\n171\n180\n193\n181\n183\n218\n230\n242\n209\n191\n172\n194\n\n\n1953\n196\n196\n236\n235\n229\n243\n264\n272\n237\n211\n180\n201\n\n\n\n\n\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\n\nfig, ax = plt.subplots()\nim = ax.imshow(flights.values, cmap=\"inferno\")\ncbar = ax.figure.colorbar(im, ax=ax)\nax.set_xticks(np.arange(len(flights.columns)))\nax.set_yticks(np.arange(len(flights.index)))\n# Labels\nax.set_xticklabels(flights.columns, rotation=90)\nax.set_yticklabels(flights.index)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nsns.heatmap(flights);\n\n\n\n\n\n\n\n\n\n\n\nalt.Chart(flights).mark_rect().encode(\n    x=alt.X(\"month\", type=\"nominal\", sort=None), y=\"year:O\", color=\"passengers:Q\"\n)\n\n\n\n\n\n\n\n\n\nnah\n\n\n\n\n\n\n8.6.2 Calendar heatmap\nOkay the previous heatmap was technically a calendar heatmap. But there are some nifty tools for making day-of-week by month heatmaps.\n\nMatplotlibSeabornAltairPlotly\n\n\n\nimport dayplot as dp\n\ndf = dp.load_dataset()\n\nfig, ax = plt.subplots(figsize=(15, 6))\ndp.calendar(\n    dates=df[\"dates\"],\n    values=df[\"values\"],\n    cmap=\"inferno\",  # any matplotlib colormap\n    start_date=\"2024-01-01\",\n    end_date=\"2024-12-31\",\n    ax=ax,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nnah\n\n\nnah\n\n\nnah\n\n\n\n\n\n\n8.6.3 Violin plot\nLet’s use the tips dataset:\n\ntips = sns.load_dataset(\"tips\")\ntips.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\n\ncolormap = plt.cm.Set1\ncolorst = [colormap(i) for i in np.linspace(0, 0.9, len(tips[\"time\"].unique()))]\n\nfig, ax = plt.subplots()\nfor i, grp in enumerate(tips[\"time\"].unique()):\n    vplot = ax.violinplot(\n        tips.loc[tips[\"time\"] == grp, \"tip\"], positions=[i], vert=True\n    )\nlabels = list(tips[\"time\"].unique())\nax.set_xticks(np.arange(len(labels)))\nax.set_xticklabels(labels)\nax.set_ylabel(\"Tip\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nsns.violinplot(data=tips, x=\"time\", y=\"tip\");\n\n\n\n\n\n\n\n\n\n\n\nalt.Chart(tips).transform_density(\n    \"tip\", as_=[\"tip\", \"density\"], groupby=[\"time\"]\n).mark_area(orient=\"horizontal\").encode(\n    y=\"tip:Q\",\n    color=\"time:N\",\n    x=alt.X(\n        \"density:Q\",\n        stack=\"center\",\n        impute=None,\n        title=None,\n        axis=alt.Axis(labels=False, values=[0], grid=False, ticks=True),\n    ),\n    column=alt.Column(\n        \"time:N\",\n        header=alt.Header(\n            titleOrient=\"bottom\",\n            labelOrient=\"bottom\",\n            labelPadding=0,\n        ),\n    ),\n).properties(width=100).configure_facet(spacing=0).configure_view(stroke=None)\n\n\n\n\n\n\n\n\n\n\nfig = px.violin(\n    tips,\n    y=\"tip\",\n    x=\"time\",\n    color=\"time\",\n    box=True,\n    points=\"all\",\n    hover_data=tips.columns,\n)\nfig.show()\n\n                            \n                                            \n\n\n\n\n\n\n\n\n8.6.4 Lollipop\n\nplanets = sns.load_dataset(\"planets\").groupby(\"year\")[\"number\"].count()\nplanets.head()\n\nyear\n1989    1\n1992    2\n1994    1\n1995    1\n1996    6\nName: number, dtype: int64\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\n\nfig, ax = plt.subplots()\nax.stem(planets.index, planets, basefmt=\"\")\nax.yaxis.tick_right()\nax.spines[\"left\"].set_visible(False)\nax.set_ylim(0, 200)\nax.set_title(\"Number of exoplanets discovered per year\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(planets.reset_index(), x=\"year\", y=\"number\")\n    .add(so.Dot(), so.Agg(\"sum\"))\n    .add(so.Bar(width=0.1), so.Agg(\"sum\"))\n)\n\n\n\n\n\n\n\n\n\n\nnah\n\n\n\nimport plotly.graph_objects as go\n\npx_df = planets.reset_index()\n\nfig1 = go.Figure()\n# Draw points\nfig1.add_trace(\n    go.Scatter(\n        x=px_df[\"year\"],\n        y=px_df[\"number\"],\n        mode=\"markers\",\n        marker_color=\"darkblue\",\n        marker_size=10,\n    )\n)\n# Draw lines\nfor index, row in px_df.iterrows():\n    fig1.add_shape(type=\"line\", x0=row[\"year\"], y0=0, x1=row[\"year\"], y1=row[\"number\"])\nfig1.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization Examples</span>"
    ]
  },
  {
    "objectID": "DV/DV-CommonPlots.html#tree-diagrams",
    "href": "DV/DV-CommonPlots.html#tree-diagrams",
    "title": "8  Visualization Examples",
    "section": "8.7 Tree Diagrams",
    "text": "8.7 Tree Diagrams\nTree\n\n8.7.1 Dendrogram or hierarchical clustering\n\n\nMatplotlibSeabornAltairPlotly\n\n\nnah\n\n\n\n# Data\ndf = (\n    pd.read_csv(\n        \"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\"\n    )\n    .rename(columns={\"rownames\": \"Model\"})\n    .set_index(\"Model\")\n)\n# Plot\nsns.clustermap(\n    df, metric=\"correlation\", method=\"single\", standard_scale=1, cmap=\"vlag\"\n);\n\n\n\n\n\n\n\n\n\n\nnah\n\n\nnah\n\n\n\n\n\n\n8.7.2 Treemap\n\n\nAltairPlotly\n\n\nnah\n\n\n\nimport numpy as np\nimport plotly.express as px\n\ndf = px.data.gapminder().query(\"year == 2007\")\nfig = px.treemap(\n    df,\n    path=[px.Constant(\"world\"), \"continent\", \"country\"],\n    values=\"pop\",\n    color=\"lifeExp\",\n    hover_data=[\"iso_alpha\"],\n    color_continuous_scale=\"RdBu\",\n    color_continuous_midpoint=np.average(df[\"lifeExp\"], weights=df[\"pop\"]),\n)\nfig.update_layout(margin=dict(t=50, l=25, r=25, b=25))\nfig.show()\n\n                            \n                                            \n\n\n\n\n\n\n\n\n8.7.3 Waffle, isotype, or pictogram charts\nThese are great for showing easily-understandable magnitudes.\n\n\nMatplotlibAltairPlotly\n\n\nThere is a package called pywaffle that provides a convenient way of doing this. It expects a dictionary of values. Note that the icon can be changed and, because it builds on matplotlib, you can tweak to your heart’s content.\n\nfrom pywaffle import Waffle\n\ndata = {\"Democratic\": 48, \"Republican\": 46, \"Libertarian\": 3}\nfig = plt.figure(\n    FigureClass=Waffle,\n    rows=5,\n    values=data,\n    colors=[\"#232066\", \"#983D3D\", \"#DCB732\"],\n    legend={\"loc\": \"upper left\", \"bbox_to_anchor\": (1, 1)},\n    icons=\"child\",\n    font_size=12,\n    icon_legend=True,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nnah\n\n\nnah",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization Examples</span>"
    ]
  },
  {
    "objectID": "DV/DV-CommonPlots.html#other-chart-types",
    "href": "DV/DV-CommonPlots.html#other-chart-types",
    "title": "8  Visualization Examples",
    "section": "8.8 Other Chart Types",
    "text": "8.8 Other Chart Types\n\n8.8.1 Slope chart\nA slope chart has two points connected by a line and is good for indicating how relationships between variables have changed over time.\n\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/selva86/datasets/master/gdppercap.csv\"\n)\ndf = pd.melt(\n    df,\n    id_vars=[\"continent\"],\n    value_vars=df.columns[1:],\n    value_name=\"GDP per capita\",\n    var_name=\"Year\",\n).rename(columns={\"continent\": \"Continent\"})\ndf.head()\n\n\n\n\n\n\n\n\nContinent\nYear\nGDP per capita\n\n\n\n\n0\nAfrica\n1952\n1252.572466\n\n\n1\nAmericas\n1952\n4079.062552\n\n\n2\nAsia\n1952\n5195.484004\n\n\n3\nEurope\n1952\n5661.057435\n\n\n4\nOceania\n1952\n10298.085650\n\n\n\n\n\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\nThere isn’t an off-the-shelf way to do this in matplotlib but the example below shows that, with matplotlib, where there’s a will there’s a way! It’s where the ‘build-what-you-want’ comes into its own. Note that the functino that’s defined returns an Axes object so that you can do further processing and tweaking as you like.\n\nfrom matplotlib import lines as mlines\n\n\ndef slope_plot(data, x, y, group, before_txt=\"Before\", after_txt=\"After\"):\n    if len(data[x].unique()) != 2:\n        raise ValueError(\"Slope plot must have two unique periods.\")\n    wide_data = data[[x, y, group]].pivot(index=group, columns=x, values=y)\n    x_names = list(wide_data.columns)\n\n    fig, ax = plt.subplots()\n\n    def newline(p1, p2, color=\"black\"):\n        ax = plt.gca()\n        line = mlines.Line2D(\n            [p1[0], p2[0]],\n            [p1[1], p2[1]],\n            color=\"red\" if p1[1] - p2[1] &gt; 0 else \"green\",\n            marker=\"o\",\n            markersize=6,\n        )\n        ax.add_line(line)\n        return line\n\n    # Vertical Lines\n    y_min = data[y].min()\n    y_max = data[y].max()\n    ax.vlines(\n        x=1,\n        ymin=y_min,\n        ymax=y_max,\n        color=\"black\",\n        alpha=0.7,\n        linewidth=1,\n        linestyles=\"dotted\",\n    )\n    ax.vlines(\n        x=3,\n        ymin=y_min,\n        ymax=y_max,\n        color=\"black\",\n        alpha=0.7,\n        linewidth=1,\n        linestyles=\"dotted\",\n    )\n    # Points\n    ax.scatter(\n        y=wide_data[x_names[0]],\n        x=np.repeat(1, wide_data.shape[0]),\n        s=15,\n        color=\"black\",\n        alpha=0.7,\n    )\n    ax.scatter(\n        y=wide_data[x_names[1]],\n        x=np.repeat(3, wide_data.shape[0]),\n        s=15,\n        color=\"black\",\n        alpha=0.7,\n    )\n    # Line Segmentsand Annotation\n    for p1, p2, c in zip(wide_data[x_names[0]], wide_data[x_names[1]], wide_data.index):\n        newline([1, p1], [3, p2])\n        ax.text(\n            1 - 0.05,\n            p1,\n            c,\n            horizontalalignment=\"right\",\n            verticalalignment=\"center\",\n            fontdict={\"size\": 14},\n        )\n        ax.text(\n            3 + 0.05,\n            p2,\n            c,\n            horizontalalignment=\"left\",\n            verticalalignment=\"center\",\n            fontdict={\"size\": 14},\n        )\n    # 'Before' and 'After' Annotations\n    ax.text(\n        1 - 0.05,\n        y_max + abs(y_max) * 0.1,\n        before_txt,\n        horizontalalignment=\"right\",\n        verticalalignment=\"center\",\n        fontdict={\"size\": 16, \"weight\": 700},\n    )\n    ax.text(\n        3 + 0.05,\n        y_max + abs(y_max) * 0.1,\n        after_txt,\n        horizontalalignment=\"left\",\n        verticalalignment=\"center\",\n        fontdict={\"size\": 16, \"weight\": 700},\n    )\n    # Decoration\n    ax.set(\n        xlim=(0, 4), ylabel=y, ylim=(y_min - 0.1 * abs(y_min), y_max + abs(y_max) * 0.1)\n    )\n    ax.set_xticks([1, 3])\n    ax.set_xticklabels(x_names)\n    # Lighten borders\n    for ax_pos in [\"top\", \"bottom\", \"right\", \"left\"]:\n        ax.spines[ax_pos].set_visible(False)\n    return ax\n\n\nslope_plot(df, x=\"Year\", y=\"GDP per capita\", group=\"Continent\");\n\n\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(df, x=\"Year\", y=\"GDP per capita\", color=\"Continent\")\n    .add(so.Line(marker=\"o\"), so.Agg())\n    .add(so.Range())\n)\n\n\n\n\n\n\n\n\n\n\n\nalt.Chart(df).mark_line().encode(x=\"Year:O\", y=\"GDP per capita\", color=\"Continent\")\n\n\n\n\n\n\n\n\n\n\nimport plotly.graph_objects as go\n\nyr_names = [int(x) for x in df[\"Year\"].unique()]\npx_df = (\n    df.pivot(index=\"Continent\", columns=\"Year\", values=\"GDP per capita\")\n    .reset_index()\n    .rename(columns=dict(zip(df[\"Year\"].unique(), range(len(df[\"Year\"].unique())))))\n)\n\nx_offset = 5\n\nfig1 = go.Figure()\n# Draw lines\nfor index, row in px_df.iterrows():\n    fig1.add_shape(\n        type=\"line\",\n        x0=yr_names[0],\n        y0=row[0],\n        x1=yr_names[1],\n        y1=row[1],\n        name=row[\"Continent\"],\n        line=dict(color=px.colors.qualitative.Plotly[index]),\n    )\n    fig1.add_trace(\n        go.Scatter(\n            x=[yr_names[0]],\n            y=[row[0]],\n            text=row[\"Continent\"],\n            mode=\"text\",\n            name=None,\n        )\n    )\n\n\nfig1.update_xaxes(range=[yr_names[0] - x_offset, yr_names[1] + x_offset])\nfig1.update_yaxes(\n    range=[px_df[[0, 1]].min().min() * 0.8, px_df[[0, 1]].max().max() * 1.2]\n)\nfig1.update_layout(showlegend=False)\nfig1.show()\n\n                            \n                                            \n\n\n\n\n\n\n\n\n8.8.2 Dumbbell Plot\nThese are excellent for showing a change in time with a large number of categories, as we will do here with continents and mean GDP per capita.\n\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/selva86/datasets/master/gdppercap.csv\"\n)\ndf = pd.melt(\n    df,\n    id_vars=[\"continent\"],\n    value_vars=df.columns[1:],\n    value_name=\"GDP per capita\",\n    var_name=\"Year\",\n).rename(columns={\"continent\": \"Continent\"})\ndf.head()\n\n\n\n\n\n\n\n\nContinent\nYear\nGDP per capita\n\n\n\n\n0\nAfrica\n1952\n1252.572466\n\n\n1\nAmericas\n1952\n4079.062552\n\n\n2\nAsia\n1952\n5195.484004\n\n\n3\nEurope\n1952\n5661.057435\n\n\n4\nOceania\n1952\n10298.085650\n\n\n\n\n\n\n\n\n\nMatplotlibSeabornAltairPlotly\n\n\nAgain, no off-the-shelf method–but that’s no problem when you can build it yourself.\n\ndef dumbbell_plot(data, x, y, change):\n    if len(data[x].unique()) != 2:\n        raise ValueError(\"Dumbbell plot must have two unique periods.\")\n    if not isinstance(data[y].iloc[0], str):\n        raise ValueError(\"Dumbbell plot y variable only works with category values.\")\n    wide_data = data[[x, y, change]].pivot(index=y, columns=x, values=change)\n    x_names = list(wide_data.columns)\n    y_names = list(wide_data.index)\n\n    def newline(p1, p2, color=\"black\"):\n        ax = plt.gca()\n        line = mlines.Line2D([p1[0], p2[0]], [p1[1], p2[1]], color=\"skyblue\", zorder=0)\n        ax.add_line(line)\n        return line\n\n    fig, ax = plt.subplots()\n    # Points\n    ax.scatter(\n        y=range(len(y_names)),\n        x=wide_data[x_names[1]],\n        s=50,\n        color=\"#0e668b\",\n        alpha=0.9,\n        zorder=2,\n        label=x_names[1],\n    )\n    ax.scatter(\n        y=range(len(y_names)),\n        x=wide_data[x_names[0]],\n        s=50,\n        color=\"#a3c4dc\",\n        alpha=0.9,\n        zorder=1,\n        label=x_names[0],\n    )\n    # Line segments\n    for i, p1, p2 in zip(\n        range(len(y_names)), wide_data[x_names[0]], wide_data[x_names[1]]\n    ):\n        newline([p1, i], [p2, i])\n    ax.set_yticks(range(len(y_names)))\n    ax.set_yticklabels(y_names)\n    # Decoration\n    # Lighten borders\n    for ax_pos in [\"top\", \"right\", \"left\"]:\n        ax.spines[ax_pos].set_visible(False)\n    ax.set_xlabel(change)\n    ax.legend(frameon=False, loc=\"lower right\")\n    plt.show()\n\n\ndumbbell_plot(df, x=\"Year\", y=\"Continent\", change=\"GDP per capita\")\n\n\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(df, y=\"Continent\", x=\"GDP per capita\", color=\"Year\").add(\n        so.Dots(pointsize=10, fillalpha=1)\n    )\n)\n\n\n\n\n\n\n\n\n\n\nnah\n\n\n\nimport plotly.graph_objects as go\n\nfig1 = go.Figure()\n\nyr_names = df[\"Year\"].unique()\n\n\n# Draw lines\nfor i, cont in enumerate(df[\"Continent\"].unique()):\n    cdf = df[df[\"Continent\"] == cont]\n    fig1.add_shape(\n        type=\"line\",\n        x0=cdf.loc[cdf[\"Year\"] == yr_names[0], \"GDP per capita\"].values[0],\n        y0=cont,\n        x1=cdf.loc[cdf[\"Year\"] == yr_names[1], \"GDP per capita\"].values[0],\n        y1=cont,\n        line=dict(color=px.colors.qualitative.Plotly[0], width=2),\n    )\n# Draw points\nfor i, year in enumerate(yr_names):\n    yrdf = df[df[\"Year\"] == year]\n    fig1.add_trace(\n        go.Scatter(\n            y=yrdf[\"Continent\"],\n            x=yrdf[\"GDP per capita\"],\n            mode=\"markers\",\n            name=year,\n            marker_color=px.colors.qualitative.Plotly[i],\n            marker_size=10,\n        ),\n    )\n\nfig1.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization Examples</span>"
    ]
  },
  {
    "objectID": "DV/DV-CommonPlots.html#related-readingreference",
    "href": "DV/DV-CommonPlots.html#related-readingreference",
    "title": "8  Visualization Examples",
    "section": "8.9 Related Reading/Reference",
    "text": "8.9 Related Reading/Reference\nThe plots and codes for them have benefited from\n\nthe phenomenal matplotlib documentation,\nthe seaborn documentation,\nthe altair documentation,\nthe plotly documentation,\nthe lets-plot documentation,\nviztech (a repository that aimed to recreate the entire Financial Times Visual Vocabulary using plotnine),\nexamples posted around the web on forums and in blog posts\n\nYou may be wondering why plotnine isn’t featured here: its functions have almost exactly the same names as those in lets-plot, and we have opted to include the latter as it is currently the more mature plotting package. However, most of the code below for lets-plot also works in plotnine, and you can read more about plotnine in {ref}vis-plotnine.\nBear in mind that for many of the matplotlib examples, using the df.plot.* syntax can get the plot you want more quickly! To be more comprehensive, the solution for any kind of data is shown in the examples below.\n\nChapters in the course textbook\n\nchapter 2 of Business Analytics - Communicating with numbers\n\nWebsites for R textbook\n\nSection 3.3, 3.4, 3.5 of An Introduction to R\nSections 7 of Beginning Computer Science with R",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization Examples</span>"
    ]
  },
  {
    "objectID": "BP/BP-Function.html",
    "href": "BP/BP-Function.html",
    "title": "4  Function",
    "section": "",
    "text": "4.1 Overview\nFunctions in programming are essential for structuring and abstracting code. They allow you to define a block of code once and reuse it wherever needed—improving readability, modularity, and reducing repetition. This section covers how to use, create, and manage functions in Python, including built-in functions, user-defined functions, object methods, and scope.\nBy leveraging functions, you can write cleaner, more efficient, and more maintainable code.",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Function</span>"
    ]
  },
  {
    "objectID": "BP/BP-Function.html#what-are-functions",
    "href": "BP/BP-Function.html#what-are-functions",
    "title": "4  Function",
    "section": "4.2 What are Functions?",
    "text": "4.2 What are Functions?\nA function is a named block of reusable code that performs a specific task. Like mathematical functions, Python functions take inputs (called arguments) and produce outputs (called return values) by executing a defined operation.\nFor example, a built-in function print() outputs text to the console (or to a specified file):\n\nprint(\"BUDA 450\")\n\nBUDA 450\n\n\nFunctions help improve code clarity, reduce redundancy, and support modular design. Python includes many built-in functions, and you can also access additional ones from external libraries—or define your own.\n\n\n\n\n\n\nExercise\n\n\n\nThe built-in function sum() takes a list of numbers and returns their total. Use sum() to calculate the sum of values in the list even_numbers = [2, 4, 6, 8, 10].\n\n\nShow the code\neven_numbers = [2, 4, 6, 8, 10]\nsum(even_numbers)",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Function</span>"
    ]
  },
  {
    "objectID": "BP/BP-Function.html#calling-functions",
    "href": "BP/BP-Function.html#calling-functions",
    "title": "4  Function",
    "section": "4.3 Calling Functions",
    "text": "4.3 Calling Functions\nIn Python, functions are called using their name, followed by parentheses (). Inside the parentheses, you may include arguments as the inputs, separated by commas , if there are multiple.\nFor example, the sum() function accepts arguments for a list of numbers and a value to specify a starting value for the summation:\n\nodd_numbers = [1,3,5]\nsum(odd_numbers, start=10)\n\n19\n\n\nAs shown in this example, Python functions can accept two main types of arguments:\n\nPositional arguments\n\nArguments are assigned based on their position in the function call.\nTheir order matters as the position of each argument play a different role.\n\n\n\n# print() with two arguments\nprint(\"Hello\", \"World\")\n\nHello World\n\n\n\nKeyword arguments\n\nArguments are explicitly assigned using name = value syntax, like value assignments to objects.\nThis improves clarity and allows reordering or omitting optional parameters:\n\n\n\n# Use the `sep` keyword argument to specify the separator is '+++'\nprint(\"Hello\", \"World\", sep='__space__')  # \"Hello__space__World\"\n\nHello__space__World\n\n\nKeyword arguments often have default values, making them optional. For instance, the print() function uses a default space ' ' for sep unless overridden1.\nKeyword arguments are always optional as they have default values predefined in the function code. For example, the separator in print()1 has a default value ' '.\n\n\n\n\n\n\nTip\n\n\n\nEven if a function takes no arguments, always use parentheses () to call it. This distinguishes functions from variables.\n\nprint()   # calling it for its operation (display none)\n\n\n\n\n\nprint     # refers to the function object itself\n\n&lt;function print(*args, sep=' ', end='\\n', file=None, flush=False)&gt;\n\n\nParentheses () are what we can consider for a clear distinction between functions and variables. Even when no argument is needed for a function’s operation, empty parentheses2 are used, referring to a function name.\n\n\n\n\n4.3.1 Built-in Functions\nPython includes a rich set of built-in functions that are always available without extra external modules. These functions handle many basic tasks, from displaying output to performing calculations or type conversions.\nYou can explore the full list of built-in functions in the Python’s official library documentation3.\nHere are some other built-in function examples:\n\n# Convert a number to a string\nstr(598)                          # Output: \"598\"\n\n# Get the length of a string\nlen(\"python\")                     # Output: 6\n\n# Round a number to a specified precision\nround(3.1415, 2)                  # Output: 3.14\n\n# Find the smallest of several values\nmin(1, 6/8, 4/3)                  # Output: 0.75\n\n0.75\n\n\n\n\n\n\n\n\nWarning\n\n\n\nLearning to program in Python partly means becoming familiar with its built-in functions (and many other functions in other modules related to your tasks). You don’t need to memorize them—just be aware they exist and know how to look them up when needed, like you use a dictionary for your writing.\n\n\n\n\n4.3.2 Object Methods\nIn Python, all values are objects. Objects can store data (called attributes) and have associated functions (called methods) that act on that data. A method is a function that is called on an object and is specific to the object’s type.\nFor example, my_message is an object that is defined with string-type values \"Hello World\", and a method lower() is a function that can be applied to the object to make all value (i.e., letters in the string) lowercase.\nTo call a method, use dot notation by writing the name of the target object to call the method on (i.e., apply the function to), followed by a period (dot) . , followed by the method name (and arguments) like object_name.method_name(arguments).\nFor example, a string object like \"Hello World\" has methods such as .lower() that returns a lowercase version of the string:\n\nmy_message = \"Hello World\"   # a string object\nmy_message.lower()           # a lowercase version of the string object\n\n'hello world'\n\n\n\n\n\n\n\n\nExercise\n\n\n\nA method replace(first, second) is for replacing the values that are equal to the first argument in the object with the values in the second argument. Using this method, make my_message return \"Hi World\".\n\n\nShow the code\nmy_message.replace(\"Hello\", \"Hi\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDot notation is also used to access attributes of an object.\nFor instance, if the_person is a Person object with a name attribute and a say_name() method, you would use:\n\nthe_person.name for the person’s name\nthe_person.say_name() to invoke the behavior method()\n\nThink of the dot . like a possessive: - the_person.name means “the person’s name”; - the_person.say_name() means “the person’s say_name() action”.\n\n\n\n\n4.3.3 Modules and Libraries\nWhile Python includes many built-in functions, additional functionality is organized into modules as collections of related functions and variables.\nTo use a module in your program, you must first import it using the import keyword. This keeps the interpreter efficient by loading only the necessary tools into memory.\nFor example, math is a module that contain various useful mathematical functions. To make the functions available to your program. For this, run the import keyword with a space and module name.\n\n# Import the built-in math module\nimport math\n\n# Call functions or access constants using dot notation\nmath.sqrt(25)      # 5.0\nprint(math.pi)     # 3.141592653589793\n\n3.141592653589793\n\n\nAs you may notice, Dot notation works here just as it does for object methods. That is, math.sqrt() means “the sqrt() function in the math module.”\nThis only needs to be done once per script execution, and so is normally done at the “top” of the script (in a Jupyter notebook, you can include an “importing” code cell, or import the module at the top of the cell in which you first need it):\n\n4.3.3.1 Selective Import\nYou can import specific functions or constants from a module directly into the global namespace using:\n\nfrom math import sqrt, pi\n\nsqrt(36)           # 6.0\nprint(pi)          # 3.141592653589793\n\n3.141592653589793\n\n\nOr you can import everything (not recommended for large or unfamiliar modules):\n\nfrom math import *\n\n\n\n\n4.3.4 Standard Library vs. External Packages\nModules like math, random, and datetime are part of Python’s Standard Library. However, Python’s real power comes from thousands of additional libraries (or packages) developed by the community, such as pandas, numpy, and matplotlib.\nTo use these external libraries, you must first install them. If you’re using Anaconda, you can install packages with the conda command:\nconda install numpy\nYou can also use Python’s package manager pip:\npip install pandas\nFor details, check XXXX.",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Function</span>"
    ]
  },
  {
    "objectID": "BP/BP-Function.html#writing-functions",
    "href": "BP/BP-Function.html#writing-functions",
    "title": "4  Function",
    "section": "4.4 Writing Functions",
    "text": "4.4 Writing Functions\nWhile Python offers many built-in and third-party functions, one of the most powerful features of the language is the ability to write your own functions. Functions allow you to encapsulate reusable logic, helping you organize code into meaningful, manageable chunks.\nAs Downey puts it:\n\n“Their primary purpose is to help us organize programs into chunks that match how we think about the problem.”\n\nWhenever you find yourself repeating the same logic, or want to improve the clarity of your program, writing a function is the right move. It reduces repetition, lowers the risk of errors, and makes your code more modular and easier to debug.\nHere’s a simple example:\n\n# Define a function that takes two arguments and returns a full name\ndef make_full_name(first_name, last_name):\n    full_name = first_name + \" \" + last_name\n    return full_name\n\n# Call the function with two arguments\nmy_name = make_full_name(\"Alice\", \"Kim\")  # Output: \"Alice Kim\"\n\n\n4.4.1 Components in a Function\nWriting a function in Python involves several key components. Each part plays a role in defining, organizing, and executing reusable code logic.\n\ndef keyword\n\n\nDefine a new function with the def keyword that signals to Python that what follows is a function definition.\n\n\nFunction name\n\n\nGive your function a meaningful name that reflects its purpose.\n\nOne good practice is naming functions using verbs (as they do something), and naming variables using nouns (as they represent data).\n\nNaming rules for general objects/variables also apply to functions (e.g., lowercase letters, underscores for separation).\n\n\nParentheses () and Parameters\n\n\nAttach parentheses () after the function name.\nInside the parentheses, you may include:\n\nPositional arguments: Required values, passed in order.\nKeyword arguments: Optional values, assigned with a default.\n\n\ndef greet(name, greeting=\"Hello\"):\n    print(f\"{greeting}, {name}!\")\n\nYou can call this function with or without the keyword argument:\n\ngreet(\"Ada\")                    # Output: Hello, Ada!\ngreet(\"Alan\", greeting=\"Hi\")   # Output: Hi, Alan!\n\nHello, Ada!\nHi, Alan!\n\n\n\nFunctions can also take no arguments—just use empty parentheses:\n\n\ndef say_hello():\n    print(\"Hello world!\")\n\n\n\nColon :\n\n\nEnd the function header line with a colon :, indicating that the body (block) of the function follows.\n\n\nFunction Body (Indented Block)\n\n\nKeep indentation (typically 4 spaces) for all the body of the function.\nIt contains the operations to execute when the function is called, which can include:\n\nVariable declarations\nOther function calls\nControl flow (if, for, etc.)\n\n\n\n(Optional) return statement\n\n\nUse the return keyword to specify the output of your function.\nWhen return is executed, the function ends and passes the value back to where it was called.\nThe return statement can be omitted if a function does not need to return a value.\n\ndef show_message(message):\n    print(message)\n\n\n\n\n4.4.2 Doc Strings\nFunctions are a way of abstracting behavior by organizing your code into meaningful, reusable parts. While good function and argument names help (def calc_rectangle_area(width, height) is better than def my_func(a, b, c)), naming alone isn’t always enough. To make your functions easy to understand and use (by yourself or others), it’s important to document them clearly.\nA doc string (documentation string) is a special multi-line string placed right below a function definition. typically describing:\n\nWhat the function does (at a high level), as a short abstraction (1~2 sentences);\nWhat inputs (arguments) it expects;\nWhat it returns (if anything).\n\nDoc strings are enclosed in triple quotes \"\"\" and are not assigned to any variable.\n\ndef to_celsius(degrees_fahrenheit):\n    \"\"\"Converts Fahrenheit to Celsius and returns the result.\"\"\"\n    return (degrees_fahrenheit - 32) * (5/9)\n\nIf a function contains doc strings, you can access this documentation using the built-in help() function with the function’s name:\n\nhelp(to_celsius)\n\nHelp on function to_celsius in module __main__:\n\nto_celsius(degrees_fahrenheit)\n    Converts Fahrenheit to Celsius and returns the result.\n\n\n\nIt’s good to think of doc strings as defining a “contract”: you are specifying that “if you give the function this set of inputs, it will perform this behavior and give you this output.”.\nFor simple methods you can build this information into a single sentence as in the above example. But for more complex functions, you may need to use a more complex format for your doc string. See the Google Style Guide for an example.\n\ndef score_message(score, name=\"student\"):\n    \"\"\"This is a doc-string, a string describing a function.\n    Args:\n        score (float): Raw score\n        name (str): Name of student\n    Returns:\n        str: A hello message.\n        float: A normalised score.\n    \"\"\"\n    norm_score = (score - 50) / 10\n    return f\"Hello {name}\", norm_score\n\n\n# Without indentation, this code is not part of function\nname = \"Ada\"\nscore = 98\n# No name entered\nprint(score_message(score))\n# Name entered\nprint(score_message(score, name=name))\n\n('Hello student', 4.8)\n('Hello Ada', 4.8)\n\n\nIn that last example, you’ll notice that we added some text to the function. This is a doc-string, or documentation string. It’s there to help users (and, most likely, future you) to understand what the function does. Let’s see how this works in action by calling help() on the score_message function:\n\nhelp(score_message)\n\nHelp on function score_message in module __main__:\n\nscore_message(score, name='student')\n    This is a doc-string, a string describing a function.\n    Args:\n        score (float): Raw score\n        name (str): Name of student\n    Returns:\n        str: A hello message.\n        float: A normalised score.\n\n\n\n::: {callout-caution title=“Exercise”}\nCheck the doc string of a built-in function by calling help() with the function you choose from the official website.\n\n\nShow the code\nhelp(round)\n\n\n:::",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Function</span>"
    ]
  },
  {
    "objectID": "BP/BP-Function.html#reference-and-resources",
    "href": "BP/BP-Function.html#reference-and-resources",
    "title": "4  Function",
    "section": "4.5 Reference and Resources",
    "text": "4.5 Reference and Resources\nmain - Functions (Ross)\naux - Functions (Sweigart) - Functions (Severance) - Fruitful Functions (Downey)",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Function</span>"
    ]
  },
  {
    "objectID": "BP/BP-Function.html#footnotes",
    "href": "BP/BP-Function.html#footnotes",
    "title": "4  Function",
    "section": "",
    "text": "See the description in the official documentation.↩︎\nThis does not mean that the function takes no arguments; it is just a useful shorthand for indicating that something is a function rather than a variable. Predetermined parameters will be used when no specific values are given to keyword arguments.↩︎\nThis course and materials are based on Python 3.13 (stable).↩︎",
    "crumbs": [
      "Basics in Python Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Function</span>"
    ]
  }
]